<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ApproxEstimation Error</title>
      <link href="2021/05/09/cs229%209/"/>
      <url>2021/05/09/cs229%209/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-9-ApproxEstimation-Error"><a href="#Lecture-9-ApproxEstimation-Error" class="headerlink" title="Lecture 9 ApproxEstimation Error"></a>Lecture 9 ApproxEstimation Error</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=12">视频链接</a></p><p>这节课大部分内容是基于两个假设</p><ul><li>there exists a data distribution D from which x，y pairs are sampled; training set and test set come from the same distribution</li><li><p>all the samples are sampled independently</p><p>[5:28]的图 m个训练样本$x^{(1)}, y^{(1)}, \cdots, x^{(m)}, y^{(m)}$都是随机变量，经过Learning Algorithm 这个Deterministic Function 得到了$\hat{h}$或者$\hat{\theta}$，这个也是random variable</p></li></ul><p>Distribution of $\theta$ 我们称之为Sampling Distribution 存在$\theta^<em>$ or $h^</em>$ 我们称之为“True” parameter，我们希望这个parameter 是learning algorithm的输出，然而我们永远不会知道$\theta^*$是什么</p><p>注意$\theta^<em>$ 和$h^</em>$ 不是随机的， 意味着没有与之相关的概率分布，只是一个我们不知道的常数</p><h3 id="bias-和-variance的概念"><a href="#bias-和-variance的概念" class="headerlink" title="bias 和 variance的概念"></a>bias 和 variance的概念</h3><p>[13: 55] 左上角是high bias low variance， 右上角是high bias high variance</p><p>左下角是low bias low variance， 右下角是low bias high variance</p><p>bias is basically  checking whether the sampling distribution kind of centered around the true parameter.</p><p><strong>variance is  measuring basically how dispersed the sampling distribution is</strong></p><p>偏差和方差基本上是采样分布的first and second moments of your sampling distribution</p><p>如果收集无数个samples 那么其variance将变成0</p><p>如果当$m \to \infty$时 $\hat{\theta} \to \theta^*$  我们称这样的算法时consistent</p><p>high bias algorithm means no matter how much data or evidence you provided, it kind of always keep away from $\theta^*$ </p><p>high variance algorithm can easily get swayed by noise in the data.</p><p>bias 和 variance 是相互独立的</p><p>fighting variance的方法</p><ul><li>more data</li><li>regularization</li></ul><h3 id="hypothesis的分析"><a href="#hypothesis的分析" class="headerlink" title="hypothesis的分析"></a>hypothesis的分析</h3><p>在the space of hypothesis中，</p><p>假设存在一个最好的hypothesis g，即take this hypothesis and take the expected value of the loss with respect to the data generating distribution across an infinite amount of data. you kind of have the lowest error with this. [26: 00]的图</p><p>$g$ - best possible hypothesis ;  $h^*$ best in class $\mathcal{H}$ ; $\hat{h}$ learnt from finite data</p><p>$\epsilon(h)$: Risk / Generalization Error $E_{(x, y) \sim D}[1\{h(x) \ne y\}]$</p><p>$\hat{\epsilon}(h)$: Empirical Risk $= \frac{1}{m} \sum_{i = 1}^m 1\{ h(x^{(i)} \ne y^{(i)}\}$ 相当于有限版的$\epsilon(h)$</p><p>$\epsilon(g)$: Bayes Error / Irreducible Error   $\epsilon(h^*) - \epsilon(g) $ 是 Approximation Error</p><p>$\epsilon(\hat{h}) - \epsilon(h^*)$ 是 Estimation Error</p><p>容易看出 $\epsilon(\hat{h}) $ 是 estimation error， approximation error， Irreducible error的和</p><p>第一种error 是由于limited data造成的， 第二种error 是由模型选择造成的， 第三种error是不可避免的 其中第一种error可以表示为Estimation Error + Estimation Variance</p><p>我们经常提到的bias 和 variance 是</p><p>Bias 为 Estimation Error + approximation error</p><p>Variance 为 Estimation Variance </p><p>Bias is basically trying to capture why is $\hat{h}$ far from a $g$</p><p>根据[38:40]的图，如果$\mathcal{H}$减小 那么圈子会缩小，variance会下降，但是有可能偏离g 即bias上升</p><p>正则化就是会使$\mathcal{H}$ 减小，bias可能增大，不一定</p><p>Fight high bias</p><ul><li>make $\mathcal{H}$ bigger</li></ul><h3 id="Empirical-risk-minimization-ERM"><a href="#Empirical-risk-minimization-ERM" class="headerlink" title="Empirical risk minimization ERM"></a>Empirical risk minimization ERM</h3><p>empirical risk minimizer是一个a very specific type of learning algorithms</p><script type="math/tex; mode=display">\hat{h}_{ERM} = argmin_{h \in \mathcal{H}} \frac{1}{m}\sum_{i = 1}^m 1\{h(x^{(i)}) \ne y^{(i)} \}</script><p>Uniform Convergence</p><p>有两个核心问题：</p><p>if we do empirical risk minimization what about the generalization of an effect ?</p><p>第一个问题 $\hat{\epsilon}(h)$ versus $\epsilon(h)$   第二个问题 $\hat{\epsilon}(h)$ versus $\epsilon(h^*)$   </p><p>Tools: 1 Union Bound</p><p>如果我们有k个事件$A_1$, $A_2$, $\cdots$, $A_k$ 不需要相互独立，则</p><script type="math/tex; mode=display">P(A_1 \cup A_2 \cup \cdots \cup A_k) \le P(A_1) + P(A_2) + \cdots+ P(A_k)</script><p>2 Hoeffding’s Inequality</p><p>Let $Z_1$, $Z_2$, $\cdots$,$Z_m \sim Bernoulli(\phi)$  $\hat{\phi} = \frac{1}{m}\sum_{i = 1}^mZ_i$ Let $\gamma &gt; 0$ [margin]</p><script type="math/tex; mode=display">Pr(|\hat{\phi} - \phi)| > \gamma) \le 2 exp(-2\gamma^2 m)</script><p>[62:00] $\epsilon(h)$ 和 $\hat{\epsilon}_S(h)$的图  $E[\hat{\epsilon}_S(h_i)] = \epsilon(h_i)$</p><script type="math/tex; mode=display">Pr(|\hat{\epsilon}(h_i) - \epsilon(h_i)| > \gamma) \le2exp(-2\gamma^2m) \tag{1}</script><p>对于$\hat{\epsilon}_S(h_i)$ 的图像是针对一个特定的size，当size变化了，$\hat{\epsilon}_S(h_i)$就会不同</p><p>我们对式子$(1)$扩展到所有的h，就是固定size 求关于h的期望 这个称为uniform convergence</p><p>我们试图查看经验风险曲线 converges uniformly to the 泛化风险曲线</p><p>[71:11]开始 证明了 当hypothesis class $\mathcal{H}$ 是有限的情况，思路是对所有k个假设使用union bound</p><p>感觉这部分有点硬核，建议对着讲义看</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Splits, Models &amp; Cross-Validation</title>
      <link href="2021/05/09/cs229%208/"/>
      <url>2021/05/09/cs229%208/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-8-Data-Splits-Models-amp-Cross-Validation"><a href="#Lecture-8-Data-Splits-Models-amp-Cross-Validation" class="headerlink" title="Lecture 8 Data Splits, Models &amp; Cross-Validation"></a>Lecture 8 Data Splits, Models &amp; Cross-Validation</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=11">课程视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes5.pdf">课程讲义链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/addendum_bias_variance.pdf">课程讲义链接2</a></p><p>听第七课的时候官网上的讲师还是Andrew Ng， 今天上官网的时候 发现讲师变成Moses Charikar</p><h3 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h3><p><strong>easy to learn but hard to master</strong></p><p>[06:26]画了三张图，左边是underfit the data，high bias；右边是overfit the data， high variance</p><p>bias在机器学习中的定义是 this learning algorithm had very strong preconceptions that the data could be fit by linear functions and this bias turns out not to be true.</p><p>Regularization 用来减少overfitting的情况</p><script type="math/tex; mode=display">min_\theta \frac{1}{2}\sum_{i = 1}^m ||y^{(i)} - \theta^Tx^{(i)}||^2 + \frac{\lambda}{2}||\theta||^2</script><p>其中$\frac{\lambda}{2}||\theta||^2$是正则项</p><p>对于SVM，通过核函数可以映射到无限维，为啥SVM没有明显的过拟合呢，老师说理由比较复杂，简单来说就是目标函数$||\omega||^2$有着和正则化相似的效果 </p><p>至于使用$\frac{\lambda}{2}||\theta||^2$ 而非 $\sum_j \lambda_j\theta_j$ 是因为会使训练的参数变得很多</p><p>PS： 对于不同的features 每个features的范围会不同，我们需要把features normalization到同一个范围下 比如$[0, 1]$ 这个范围 通过$(x - min) / (max - min)$</p><h3 id="Train-Dev-Test-sets"><a href="#Train-Dev-Test-sets" class="headerlink" title="Train/Dev/Test sets"></a>Train/Dev/Test sets</h3><p>对于10000个训练数据，我们要进行超参数的选择的话</p><p>$S_{train}, S_{dev}, S_{test}$ 其中dev 表示development，train each model（option for degree of polynomial 或者不同的正则系数等等) on $S_{train}$，get some hypothesis $h$ and measure erroe on $S_{dev}$ and pick model with lowest error on $S_{dev}$ </p><p>传统的划分方法是60% train, 20% dev, 20% test, 对于数据集很大的情况 这个比例会变化，可能是90% train， 5% dev，5% test</p><p>对于少量的训练数据，比如100个训练数据，用K-fold  k为10是typical choice</p><p>假如对于k为5的情况，就是把100个数据集分为五个不同的子集，接着要做的是</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">For i &#x3D; 1, ... ,kTrain parameters on k - 1 pieces;Test on remaining one piece;Average k on test errorsOptional: Refit model on 100% of data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果对于更小的数据集 比如m = 20</p><p>Leave-one-out 即设K = m 缺点是计算量很大 老师用这个方法 只在m小于等于100的情况下</p><p>老师推荐了 mlyearning.org这个网站 这里有进一步讨论 如果训练集和测试集是不同的分布</p><h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><p>a special case of model selection</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">start with F &#x3D; 空Repeat:1) Try to add each feature i to F, and see which single-feature addition most improves dev set performance2) Add that feature to F<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Neural Networks</title>
      <link href="2021/05/09/cs229%2011/"/>
      <url>2021/05/09/cs229%2011/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-11-amp-12-Neural-Networks"><a href="#Lecture-11-amp-12-Neural-Networks" class="headerlink" title="Lecture 11&amp;12 Neural Networks"></a>Lecture 11&amp;12 Neural Networks</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=14">视频链接</a></p><h3 id="将逻辑回归解释为一种特殊的神经网络"><a href="#将逻辑回归解释为一种特殊的神经网络" class="headerlink" title="将逻辑回归解释为一种特殊的神经网络"></a>将逻辑回归解释为一种特殊的神经网络</h3><p>视频前40分钟 是讲这个部分的</p><p><strong>将logistics regression作为神经网络的intuitive 我觉得挺好的</strong></p><p>目标1是对一张图来进行分类，判断是不是猫，假设图片是64*64大小 把图片展成向量 变成12288维向量，用logistics regression进行训练$\hat{y} = \sigma(\theta^Tx) = \sigma(wx + b)$</p><p>训练的步骤是:</p><ul><li>初始化w和b</li><li>找到w和b的最优值</li><li>用$\hat{y} = \sigma(wx + b)$来进行预测</li></ul><p>对于神经网络里面的神经元 我们定义是linear + activation</p><p>对于模型 我们的定义是 architecture + parameters</p><p>目标2 对于图片进行三分类</p><p>把一个神经元 扩展成3个神经元 分别计算$\hat{y_i} = a^{[1]}_i = \sigma(w_i^{[1]}x + b_i^{[1]}) \ \ i = 1,2,3$</p><p>上标1 代表layer 1，对于第一类的表示为$(1, 0, 0)$ 第二类的表示为$(0, 1, 0)$ 第三类的表示为$(0, 0, 1)$ </p><p>训练方法同上，三个神经元之间没有联系，可以独立的训练这三个神经元；这个模型robust</p><p>目标3 加上约束 图片上只有一个动物 softmax multi-class regression</p><p>对于softmax 其损失函数为cross-entropy loss $\mathcal{L} = -\sum_{k = 1}^3y_klog\hat{y_k}$</p><h3 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h3><p>这个部分对应视频的后40分钟</p><p>对于之前分辨一个图片是否是猫的图片 用了一个3 layers的网络</p><p>layer denotes neurons that are not connected to each other 一层神经元</p><p>在这个三层的网络单中[45:44]的图，第一层are going to understand the fundamental concepts of the image which is the edges. Then what’s gonna happen is that these neurons are going to communicate what they found on the image to the next layer’s neuron.</p><p><strong>end to end learning</strong>: we have an input, a ground truth and we don’t constrain the network in the middle i.e. we are just training based on the input and the output</p><p>视频中介绍的向量化的式子：</p><script type="math/tex; mode=display">Z^{[1]} = W^{[1]}X + b^{[1]}</script><p>m个训练数据 输入feature的维度是n 其中Z的shape是$(3, m)$ W的shape是$(n, m)$，b的维度是$(3, 1)$</p><h3 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h3><p>下面的公式 是根据一个三层全连接神经网络 其中第一层3个神经元 第二层2个神经元 最后一层1个神经元</p><p>用batch of examples，而非single example，原因是我们想要vectorization</p><script type="math/tex; mode=display">cost \ function: \mathcal{J}(\hat{y}, y) = \frac{1}{m}\sum_{i = 1}^m\mathcal{L}^{(i)}(\hat{y}, y) \\with: \ \mathcal{L}^{(i)} = -[y^{(i)}log\hat{y}^{(i)} + (1 - y^{(i)})log(1 - \hat{y}^{(i)}))] \\Update: \ \omega^{[l]} = \omega^{[l]} - \alpha\frac{\partial y}{\partial \omega^{[l]}}</script><p>反向来计算导数：</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial \omega^{[3]}} = -[y^{(i)}\frac{\partial}{\partial\omega^{[3]}}(log\sigma(\omega^{[3]}a^{[2]} + b^{[3]})) + (1 - y^{(i)})\frac{\partial}{\partial\omega^{[3]}}(log(1 - \sigma(\omega^{[3]}a^{[2]} + b^{[3]})))] \\= -[y^{(i)}\frac{1}{a^{[3]}}a^{[3]}(1 - a^{[3]})(a^{[2]})^T +(1-y^{(i)})\frac{1}{1 - a^{[2]}}(-1)a^{[3]}(1 - a^{[3]})(a^{[2]})^T \\= -[y^{(i)}(1 - a^{[3]})(a^{[2]})^T - (1 - y^{(i)})a^{[3]}(a^{[2]})^T] \\= -[y^{(i)}(a^{[2]})^T - a^{[3]}(a^{[2]})^T] = -(y^{(i)} - a^{[3]})(a^{[2]})^T \\\therefore \frac{\partial \mathcal{J}}{\partial \omega^{[3]}} = -\frac{1}{m}\sum_{i = 1}^m(y^{(i)} - a^{[3]})(a^{[2]})^T</script><p>反向传播到$\omega^{[2]}$</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial \omega^{[2]}} = \frac{\partial \mathcal{L}}{\partial a^{[3]}}\frac{\partial a^{[3]}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial a^{[2]}}\frac{\partial a^{[2]}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial \omega^{[2]}}</script><p>用之前算的结果来加速这个式子的算法</p><script type="math/tex; mode=display">\because \frac{\partial \mathcal{L}}{\partial \omega^{[1]}} = \frac{\partial \mathcal{L}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial \omega^{[3]}}=-(y^{(i)} - a^{[3]})(a^{[2]})^T \ \ \frac{\partial z^{[3]}}{\partial \omega^{[3]}} = (a^{[2]})^T\\ \therefore \frac{\partial \mathcal{L}}{\partial a^{[3]}}\frac{\partial a^{[3]}}{\partial z^{[3]}} = a^{[3]} - y^{(i)} \\\frac{\partial z^{[3]}}{\partial a^{[2]}} = (\omega^{[3]})^T \ \ \frac{\partial a^{[2]}}{\partial z^{[2]}} = a^{[2]}(1 - a^{[2]}) \ \ \frac{\partial z^{[2]}}{\partial \omega^{[2]}} = (a^{[1]})^T \\\therefore \frac{\partial \mathcal{L}}{\partial\omega^{[2]}} = (a^{[3]} - y^{(i)})(\omega^{[3]})^Ta^{[2]}(1 - a^{[2]})(a^{[1]})^T</script><p>注意 $\frac{\partial z^{[3]}}{\partial a^{[2]}}\frac{\partial a^{[2]}}{\partial z^{[2]}}$ 因为$a^{[2]}$和$z^{[2]}$两个都是$(2, 1)$向量  所以导数也是$(2, 1)$向量 它们之间的乘法应该是点乘</p><h3 id="Improving-NN"><a href="#Improving-NN" class="headerlink" title="Improving NN"></a>Improving NN</h3><ul><li><p>Activation function</p><p>sigmoid， ReLU， tanh</p><p>Q: Why we need activation function ?  A: to get nonlinearity</p></li><li><p>Initialization method 优势在于方便optimization</p><p>对于输入数据为$(x_1, x_2)^T$  令新的数据$x_i^{(i)} = x_i^{(i)} - \mu_i$ 其中$\mu = \frac{1}{m}\sum_{i = 1}^m x^{(i)}$</p><p>然后令$\sigma^2 = \frac{1}{m}\sum_{i = 1}^m(x^{(i)})^2$  取$\tilde{x_i} = \frac{x^{(i)}}{\sigma}$</p><p>注意$\mu$和$\sigma$是根据训练集里面计算的，对于测试集也用这两个参数</p></li><li><p>Vanishing， Exploding gradients</p><p>[55:00]举了一个例子，说明随着网络的层数增加，矩阵相乘的error也会增加，有可能gradients是一个超级大的值 或者是非常小的值 权值初始化在1左右是非常好的</p><p>一个神经元的输入为n， n越大$w_i$越小，$\frac{1}{n}$的初始化时非常好的，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token operator">*</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> n<span class="token punctuation">)</span> <span class="token comment">#对于sigmoid函数 这个初始化效果很好</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token operator">*</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">/</span> n<span class="token punctuation">)</span> <span class="token comment">#对于ReLU函数 这个初始化效果很好</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>上面的n 时 $n^{[l - 1]}$</p></li><li><p>其它Initialization</p><p>Xavier Initialization $W^{[l]} \sim \sqrt{\frac{1}{n^{[l - 1]}}}$   for tanh     He Initialization $W^{[l]} \sim \sqrt{\frac{2}{n^{[l]} + n^{[l - 1]}}}$</p></li><li><p>Optimization</p><p>mini-batch algorithm</p></li><li><p>Momentum algorithm</p><p>Assume you have the loss that is very extended in one direction. [72:00]有示例</p><p>What momentum is going to say is look at the past updates that you did and try to consider these past updates in order to find the right way to go</p><script type="math/tex; mode=display">v = \beta v + (1 - \beta)\frac{\partial \mathcal{L}}{\partial \omega} \\\omega = \omega - \alpha v</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS基础</title>
      <link href="2021/05/09/css1/"/>
      <url>2021/05/09/css1/</url>
      
        <content type="html"><![CDATA[<p>这是学堂在线上面的前端课程<a href="https://www.xuetangx.com/search?query=%E5%89%8D%E7%AB%AF&amp;page=1">课程链接</a></p><h2 id="CSS基础"><a href="#CSS基础" class="headerlink" title="CSS基础"></a>CSS基础</h2><h3 id="CSS简介"><a href="#CSS简介" class="headerlink" title="CSS简介"></a>CSS简介</h3><p>css Cascading Style Sheets 可设置颜色大小，动画效果</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">h1</span><span class="token punctuation">&#123;</span>    <span class="token property">color</span><span class="token punctuation">:</span>white<span class="token punctuation">;</span>    <span class="token property">font-size</span><span class="token punctuation">:</span>14px<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>h1是selector选择器  color是属性， white是属性值</p><p>在页面中使用CSS</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">&lt;!-- 外链 -->&lt;link rel = "stylesheet" href=" ">&lt;!-- 嵌入 -->&lt;style>li</span> <span class="token punctuation">&#123;</span><span class="token property">margin</span><span class="token punctuation">:</span> 0<span class="token punctuation">;</span><span class="token property">list-style</span><span class="token punctuation">:</span> none<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span><span class="token selector">p</span>  <span class="token punctuation">&#123;</span><span class="token property">margin</span><span class="token punctuation">:</span> lem 0<span class="token punctuation">;</span><span class="token punctuation">&#125;</span>&lt;/style>&lt;!-- 内联  不推荐使用-->&lt;p style=<span class="token string">"margin:lem 0"</span>>Example Content&lt;/p><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>css工作原理</p><p>加载HTML 之后会解析HTML 然后会创建DOM树， 解析HTML时也会加载CSS 解析CSS 然后会添加样式到DOM节点</p><h3 id="基础选择器"><a href="#基础选择器" class="headerlink" title="基础选择器"></a>基础选择器</h3><p>可使用多种方式选择元素 按照标签名，类名或id； 按照属性； 按照DOM树中的位置</p><p>通配选择器*， 标签选择器</p><p>id选择器 #logo 表示id为logo的标签 一般只有一个； 类选择器 .done class为done的所有标签 可多个</p><p>属性选择器 input[type=”password”]； a[href^=”#”] 表示href的值以#开头， a[href$=”.jpg”] 表示href的值以.jpg结尾</p><p><strong>伪类 666</strong></p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">&lt;!-- 动态伪类 根据元素所处的状态来选择-->a : link</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token selector">选中一个正常连接a : visited</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token selector">访问过的链接a : hover</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token selector">鼠标放在链接上a : active</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token property">链接被鼠标按下去了</span><span class="token punctuation">:</span>focus  对于输入框 点下去后 &lt;!-- 结构性伪类 --><span class="token property">li</span><span class="token punctuation">:</span>first-child 表示li标签的第一个孩子<span class="token property">li</span><span class="token punctuation">:</span>last-child 表示li标签的最后一个孩子<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>组合器</strong></p><p>直接组合 AB 满足A同时满足B input:focus</p><p>后代组合 A B 选中B 如果它时A的子孙  nav a</p><p>亲子组合 A &gt; B 选中B， 如果它时A的子元素   section &gt; p</p><p>选择器组，多个选择器放一起， 用逗号隔开</p><h3 id="设置字体"><a href="#设置字体" class="headerlink" title="设置字体"></a>设置字体</h3><p>font-family 设置字体 </p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">h1</span><span class="token punctuation">&#123;</span>    <span class="token property">font-family</span><span class="token punctuation">:</span> Optima<span class="token punctuation">,</span> Georgia<span class="token punctuation">,</span> serif<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>设置多个字体就是为了防止浏览器有些字体没有</p><p>通用字体族 Serif 衬线体 Sans-Serif 无衬线体 Cursive 手写体 Monospace 等宽字体</p><p>使用font-family的建议 字体列表最后写上通用字体簇 英文字体放在中文字体前面</p><p>使用web fonts @font-face</p><p><strong>font-size 关键字</strong>， 长度， 百分数， 百分数时相对于父元素字体大小来的</p><p>长度px 像素， 2em 表示是父元素字体长度的两倍</p><p><strong>font-weight 设置粗细</strong></p><p><strong>font:</strong> style weight size/height family</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">h1</span><span class="token punctuation">&#123;</span>    <span class="token property">font</span><span class="token punctuation">:</span> bold 14px/1.7 Helvetica<span class="token punctuation">,</span> sans-serif<span class="token punctuation">;</span><span class="token punctuation">&#125;</span>行高是14px * 1.7<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="设置文字样式"><a href="#设置文字样式" class="headerlink" title="设置文字样式"></a>设置文字样式</h3><p>text-align 设置对齐模式， justify 是左右两边对齐</p><p>letter-spacing 是字母与字母之间的距离， word-spacing 是词与词之间的距离</p><p>text-shadow 设置文字阴影</p><h3 id="盒模型"><a href="#盒模型" class="headerlink" title="盒模型"></a>盒模型</h3><p>在浏览器进行渲染的时候 它会把页面中的元素格式化成一个一个盒子，每一个盒子都有padding, border, margin, content； content表示盒子里面内容的大小，padding是内容和边框的内边距，border是边宽，<strong>margin是盒子和其它盒子之间的距离</strong></p><ul><li><p>width 指定content box的宽度， 取值为长度，百分数-相对于容器的宽度来计算的，auto由浏览器根据其它属性确定，<strong>容器有指定高度时，百分数才生效</strong></p></li><li><p>padding-top, padding-left, padding-right, padding-bottom 默认值为0，百分数相对于容器宽度</p></li><li><p>border none表示没有边框，solid实线，dashed虚线，dotted 点状线； 指定容器边框样式、粗细和颜色</p><ul><li>border-width, border-style, border-style</li><li>或者 border: 1px solid #ccc</li><li>四个方向 border-top, border-right, border-bottom, border-left</li><li>border-left-width: 3px</li><li>可以利用border来模拟三角形</li></ul></li><li><p>margin</p><ul><li><p>使用margin: auto  水平居中</p></li><li><p>```css<br>div{</p><pre><code>width: 200px;height: 200px;backgroud: coral;margin-left: auto;margin-right: auto;</code></pre><p>}</p><pre class="line-numbers language-none"><code class="language-none">+ margin collapse  &#96;&#96;&#96;css  .a&#123;      background: lightblue;      height: 100px;      margin-bottom: 100px;  &#125;  .b&#123;      background: coral;      height: 100px;      margin-top: 100px;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">'</span>a<span class="token punctuation">'</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">'</span>b<span class="token punctuation">'</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>按照margin的定义两个div的垂直距离应该是200px，然而实际上是100px，因为在垂直方向上相邻的margin会被合并，取两者最大的值，这个现象只会在垂直方向上发生</p></li><li><p>margin可以为负值</p></li></ul></li><li><p>默认的height和width是关于content的，如果设置box-sizing为border-box 那么height和width就是关于content + border的</p></li><li><p>当容器的内容比较多的时候 就会溢出容器，称为overflow，overflow hidden表示超出的内容被隐藏，overflow scroll 表示内容超过的时候会有一个滚动条出现</p></li><li><p>当宽度是百分比或者auto的时候，其宽度我们并不能预先确定好，因为要根据浏览器宽度的大小和内容的多少，我们可以通过min-width和max-width 来限制实际的宽度</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">article</span><span class="token punctuation">&#123;</span>    <span class="token property">margin</span><span class="token punctuation">:</span> auto<span class="token punctuation">;</span>    <span class="token property">line-height</span><span class="token punctuation">:</span> 1.7<span class="token punctuation">;</span>    <span class="token property">max-width</span><span class="token punctuation">:</span> 20em<span class="token punctuation">;</span>    <span class="token property">min-width</span><span class="token punctuation">:</span> 10em<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面意味着一行最多20个字，最少10个字</p></li><li><p>min-height 和 max-height</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">article img</span><span class="token punctuation">&#123;</span>    <span class="token property">max-width</span> <span class="token punctuation">:</span> 100%<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="CSS中的盒子"><a href="#CSS中的盒子" class="headerlink" title="CSS中的盒子"></a>CSS中的盒子</h3><p>DOM树与盒子</p><p>根据DOM树生成一系列的盒子，然后摆放盒子 称作Layout</p><p>摆放盒子</p><ul><li>盒子尺寸和类型</li><li>定位模式</li><li>节点但在DOM树中的位置</li><li>其它信息 窗口大小、图片大小等</li></ul><p>块级元素会生成块级的盒子，行级元素会生成行级元素，<strong>块级的不能和其它盒子并列摆放</strong></p><p>行级盒子可以和其它的行级盒子摆放在一行或拆开成多行，<strong>盒模型中width、height不适用</strong></p><p>行级盒子的宽度是由内容和容器来决定的</p><p>行级元素有span、em、strong、cite、code等</p><p>可以通过display属性来把任意标签设置成行级或者块级，display: block, display:inline, </p><p>display:inline-block 本身是行级，可以放在行盒中；可以设置宽高；作为一个整体不会被拆散成多行</p><p>display:none 会被忽略不会被展示出来</p><h3 id="盒子的效果"><a href="#盒子的效果" class="headerlink" title="盒子的效果"></a>盒子的效果</h3><p>圆角border-radius —- 让我想到了小米的logo 可以通过设置border-radius来设置完成</p><p>圆角可以设置成圆形，也可以设置成椭圆 20px / 50px； 也可以设置成百分比</p><p>10px 20px 30px 40px / 50px 60px 70px 80px 前面是水平方向上的，后面是垂直方向上的</p><p>background</p><ul><li>background-color 背景颜色</li><li>background-image 设置背景图片</li><li>background-repeat 背景图片是否重复</li><li>background-position 背景图片所在位置</li><li>background-size 背景图片的大小</li></ul><p>background position的一个常用的使用场景</p><p>CSS Sprites 为了让页面加载的更快，我们会把小的一些背景图合并到一张大图中，然后通过background position来使用每一张小的图片</p><p>background size默认是图片的大小</p><p>background-size: 100px auto， 宽度是100px， 高度是根据宽度来调整的</p><p>background-size: auto 100% , 高度是100%， 宽度按照比例缩放</p><p>background-size: contain 表示图片能够完全的展示出来</p><p>background-size: cover 表示图片能够完全把容器覆盖住 这个时候图片可能会被裁剪</p><p>background-clip border-box， padding-box， content-box</p><p><strong>box-shadow</strong> : 1px 1px 8px 0 gray 前面两个值 是水平和垂直方向上一个偏移量，第三个值表示阴影模糊的程度，第四个值表示阴影扩张的大小，最后是阴影的颜色， <strong>不占用空间</strong></p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token property">box-shadow</span><span class="token punctuation">:</span> 0 0 0 2px red<span class="token punctuation">,</span>0 0 0 4px orange<span class="token punctuation">,</span>0 0 0 6px yellow<span class="token punctuation">,</span>            0 0 0 8px green<span class="token punctuation">,</span>            0 0 0 10px blue<span class="token punctuation">,</span>            0 0 0 12px cyan<span class="token punctuation">,</span>            0 0 0 14px purple<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个会形成一个彩虹边框</p><h3 id="行高和垂直对齐"><a href="#行高和垂直对齐" class="headerlink" title="行高和垂直对齐"></a>行高和垂直对齐</h3><p>Font Metrics： baseline-最重要， meanline， mean line减去baseline 是x-height 是小写字母的高度，有些字母比如h 会上超过mean line 或者比如p向下超过descender line 上界时ascender line 下界是descender line，盒子的摆放需要依赖这几条线来定位，一个包含文字的行盒的高度，line-height，ascender line到顶部和descender line到底部的距离是一样的，行盒的对齐默认是根据baseline</p><p>视频[5:00] 有详细的关于垂直对齐的介绍</p><p>vertical-align: 0px  是基于baseline来对齐</p><h3 id="CSS继承"><a href="#CSS继承" class="headerlink" title="CSS继承"></a>CSS继承</h3><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>article</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>title<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        abc    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>article</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">.title</span><span class="token punctuation">&#123;</span>        <span class="token property">color</span> <span class="token punctuation">:</span> blue<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">article h1</span><span class="token punctuation">&#123;</span>        <span class="token property">color</span> <span class="token punctuation">:</span> red<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>哪条规则生效？ 涉及到选择器的优先级 是由特殊程度来决定的</p><h1 id="nav-list-li-a-link-id-：1-伪-类-2-伪-元素-2"><a href="#nav-list-li-a-link-id-：1-伪-类-2-伪-元素-2" class="headerlink" title="nav .list li a:link       id ：1  $($伪$)$类: 2   $($ 伪$)$ 元素 : 2"></a>nav .list li a:link       id ：1  $($伪$)$类: 2   $($ 伪$)$ 元素 : 2</h1><p>.hd ul.links a              id ：0  $($伪$)$类: 2   $($ 伪$)$ 元素 : 2   上面是122 下面是22 上面优先级高</p><p>对于前面那个例子 blue 是10  red 是 2  所以是blue</p><p>高优先级的选择器的属性会覆盖低优先级的选择器中相同的属性的值</p><p>应用， 写一个按钮的基础样式， 然后再写特有的，利用属性覆盖实现代码复用</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>button</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>btn<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    普通按钮<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>button</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>button</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>btn primary<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    主要按钮<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>button</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">.btn</span><span class="token punctuation">&#123;</span>        一些基本属性    <span class="token punctuation">&#125;</span>    <span class="token selector">.btn.primary</span><span class="token punctuation">&#123;</span>        特有属性    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>某些属性会自动继承其父元素的计算值，除非显示指定一个值</strong></p><p>有些属性是不能继承的 比如box-sizing，但是我们可以显示继承，比如使用*</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">*</span><span class="token punctuation">&#123;</span>    <span class="token property">box-sizing</span> <span class="token punctuation">:</span> inherit<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>可以使用initial 关键字显示重制为初始值</p><p>[8:45]的图介绍了 CSS求值的过程</p><h3 id="CSS中的值和单位"><a href="#CSS中的值和单位" class="headerlink" title="CSS中的值和单位"></a>CSS中的值和单位</h3><p>常见的CSS的值有9种</p><ul><li>关键字 比如 initial、inherit</li><li>字符串 “Microsoft Yahei”</li><li>URL </li><li>长度 100px、5em</li><li>百分数</li><li>整数 z-index : 5</li><li>浮点数 line-height : 1.8</li><li>颜色 #ff0000</li><li>时间 300ms</li></ul><p>长度单位</p><p>px  像素点 ； in 英寸 ； cm 厘米 ； mm 毫米； pt 1/72英寸 ; pc 1/6 英寸</p><p>em 元素字体大小 ; rem html字体大小；vh 1%窗口高 ; vw 1%窗口宽 ; vmax vw vh较大者</p><p>[6:26]介绍了 HSL的颜色表示方式</p>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Continuous-Time Fourier Transform</title>
      <link href="2021/03/29/signal%20and%20system%208/"/>
      <url>2021/03/29/signal%20and%20system%208/</url>
      
        <content type="html"><![CDATA[<h2 id="Continuous-Time-Fourier-Transform"><a href="#Continuous-Time-Fourier-Transform" class="headerlink" title="Continuous-Time Fourier Transform"></a>Continuous-Time Fourier Transform</h2><p><a href="https://www.bilibili.com/video/BV1xy4y167DD?p=8">视频地址</a></p><h3 id="用复指数信号的线性组合来表示非周期信号"><a href="#用复指数信号的线性组合来表示非周期信号" class="headerlink" title="用复指数信号的线性组合来表示非周期信号"></a>用复指数信号的线性组合来表示非周期信号</h3><p>对于一个非周期信号，基于这个信号来建立周期信号，就是把原来的信号重复一下，</p><p>形成周期为$T_0$的周期信号，图片参考视频[03:22]的图片</p><p>这个周期信号在一个周期内是和非周期信号是相同的，<strong>当周期$T_0$趋近于无穷时，周期信号变成了非周期信号</strong> </p><p><strong>核心思想</strong>：用傅里叶级数来表示这个周期信号，然后让周期趋近于无穷，得到的就是非周期信号的傅里叶变换</p><script type="math/tex; mode=display">\widetilde{x}(t) = x(t) \quad for \ |t| <\frac{T_0}{2} \\\widetilde{x}(t) = \sum_{k = -\infty}^{\infty}a_ke^{jk\omega_0t} \quad \quad \omega_0 = \frac{2\pi}{T_0} \\a_k = \frac{1}{T_0}\int_{-\frac{T_0}{2}}^{\frac{T_0}{2}}\widetilde{x}(t)e^{-jk\omega_0t}dt \\a_k = \frac{1}{T_0}\int_{-\infty}^{+\infty}x(t)e^{-jk\omega_0t}dt \\</script><p>Define: $X(\omega) \triangleq \int_{-\infty}^{+\infty}x(t)e^{-j\omega t}dt \tag{1}$       Then: $T_0a_k = X(\omega)\bigg|_{\omega = k\omega_0} \tag{*}$</p><p>$X(\omega)$ is the envelope$($ 包络函数 $)$ of $T_0a_k$ ； 这告诉了我们如何通过对包络函数的sample来建立周期信号的傅里叶级数系数</p><script type="math/tex; mode=display">\widetilde{x}(t) = \sum_{k = -\infty}^{+\infty}a_ke^{jk\omega_0t} = \sum_{k = -\infty}^{+\infty}\frac{1}{T_0}X(k\omega_0)e^{jk\omega_0t} \\\widetilde{x}(t) = \frac{1}{2\pi}\sum_{k = -\infty}^{+\infty}X(k\omega_0)e^{jk\omega_0t}\omega_0 \\As \ T_0 \rightarrow \infty, \\\omega_0 \rightarrow0,\widetilde{x}(t) \rightarrow x(t), \omega_0 \rightarrow d\omega, \Sigma \rightarrow \int \\</script><p>则得到 $x(t) = \frac{1}{2\pi}\int_{-\infty}^{+\infty}X(\omega)e^{j\omega t}d\omega \tag{2}$</p><p>方程$(1)$ 称为analysis equation   也成为傅里叶变换                       方程$(2)$称为synthesis equation</p><p>[11:00]开始的图示说明非常棒！！</p><p>$x(t)$和$X(\omega)$的关系是一一对应，即使$x(t)$是实函数，$X(\omega)$也是复函数</p><p>$X(\omega) = Re\{ X(\omega) \} + j Im \{  X(\omega) \} = |X(\omega)|e^{j\angle X(\omega)}$</p><h3 id="傅里叶变换的例子"><a href="#傅里叶变换的例子" class="headerlink" title="傅里叶变换的例子"></a>傅里叶变换的例子</h3><p>对于函数$x(t) = e^{-at}u(t)$ 其傅里叶变换是</p><p>$X(\omega) = \int_{-\infty}^{+\infty}x(t)e^{-j\omega t}dt = \int_{0}^{+\infty}e^{-at}e^{-j\omega t}dt = \int_{0}^{+\infty}e^{-t(a + j\omega)}dt = \frac{1}{a + j\omega}e^{-(a + j\omega)t}\bigg|^\infty_0$</p><p>当$a &gt; 0$ 积分求$t = +\infty$时 式子为0，当$t = 0$时 式子为$\frac{1}{a + j\omega}$   可得$e^{-at}u(t)$对应的傅里叶变换是$\frac{1}{a + j\omega}$</p><p>[18:30] 展示了 该函数与其傅里叶变换形成的函数的图像(有振幅的图，有幅度的图)</p><p>[21:30] 展示了该图像的波特图版本 振幅是$20log_{10}X(\omega)\triangleq decibels \ (dB)$ 横坐标是频率的对数，对于横坐标上面等距离移动，每次移动频率变为原来的10倍</p><p>由于横坐标是对数，所以对应的频率是大于0的频率，但是由于傅里叶变换出的函数的性质 我们可以推测出频率为负时的图像的样子</p><p><strong>Fourier Series coefficients equal $\frac{1}{T_0}$ times samples of Fourier transform of one period</strong></p><p>也就是说 对于之前求周期信号的傅里叶级数 我们可以考虑求一个周期的傅里叶变换[25:30]</p><p>可以可以！</p><h3 id="将傅里叶变换和傅里叶级数用一个通用框架来表示"><a href="#将傅里叶变换和傅里叶级数用一个通用框架来表示" class="headerlink" title="将傅里叶变换和傅里叶级数用一个通用框架来表示"></a>将傅里叶变换和傅里叶级数用一个通用框架来表示</h3><p>观察傅里叶级数和傅里叶变换的synthesis equation 我们可以发现</p><p>$\widetilde{x}(t)$的傅里叶级数是$a_k$  $\widetilde{x}(t)$的傅里叶变换是$\widetilde{X}(\omega)$ </p><p>则$\widetilde{X}(\omega) \triangleq \sum_{-\infty}^{\infty}2\pi a_k \delta(\omega - k\omega_0) \tag{3}$</p><script type="math/tex; mode=display">\widetilde{x}(t) = \frac{1}{2\pi}\int_{-\infty}^{+\infty}\widetilde{X}(\omega)e^{j\omega t}d\omega \\= \frac{1}{2\pi}\sum_{k = -\infty}^{+\infty}2\pi a_k \int_{-\infty}^{+\infty}\delta(\omega - k\omega_0)e^{-j\omega t}d\omega</script><h2 id="通用框架的例子"><a href="#通用框架的例子" class="headerlink" title="通用框架的例子"></a>通用框架的例子</h2><p>symmetric square wave     666哇</p><p>还讲了一个$x(t) = \delta(t)$的例子 $x(t)$的傅里叶变换是常数，</p><p>对于周期函数$\widetilde{x}(t) = \sum_{k = -\infty}^{+\infty}\delta(t - kT_0)$ 其对应的傅里叶变换$\widetilde{X}(\omega)$  这函数是怎么得到的 没有弄明白</p>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fourier Transform Properties</title>
      <link href="2021/03/29/signal%20and%20system%209/"/>
      <url>2021/03/29/signal%20and%20system%209/</url>
      
        <content type="html"><![CDATA[<h2 id="Fourier-Transform-Properties"><a href="#Fourier-Transform-Properties" class="headerlink" title="Fourier Transform Properties"></a>Fourier Transform Properties</h2><p><a href="https://www.bilibili.com/video/BV1xy4y167DD?p=9">视频链接</a></p><p><strong>Continuous —- Time Fourier Transform</strong></p><script type="math/tex; mode=display">x(t) = \frac{1}{2\pi}\int_{-\infty}^{+\infty}X(\omega)e^{j\omega t}d\omega \quad \quad  sysnthesis \\X(\omega) = \int_{-\infty}^{+\infty}x(t)e^{-j\omega t}dt \quad \quad analysis</script><h3 id="Symmetry"><a href="#Symmetry" class="headerlink" title="Symmetry"></a>Symmetry</h3><script type="math/tex; mode=display">x(t) \ real \ \ \Rightarrow \ X(-\omega) = X^*(\omega) \\ReX(\omega) = ReX(-\omega) \quad |X(\omega)| = |X(-\omega)| \quad</script><p>the real part is an even function of frequency, and the magnitude is an even function of frequency</p><script type="math/tex; mode=display">Im X(\omega) = -ImX(-\omega) \quad \angle X(\omega) = -\angle X(-\omega)</script><p>the imaginary part is an odd function of frequency, and the phase angle is an odd function of frequency</p><h3 id="Time-and-frequency-scaling"><a href="#Time-and-frequency-scaling" class="headerlink" title="Time and frequency scaling"></a>Time and frequency scaling</h3><script type="math/tex; mode=display">if \ x(t) \iff X(\omega) \quad then \quad x(at) \iff \frac{1}{|a|}X(\frac{\omega}{a})</script><p><strong>如果以两倍的速度播放磁带，意味着2倍线性压缩时间轴，然而实际发生的是频率变成2倍</strong></p><p>[09:07]实验展示非常好</p><h3 id="duality-relationship"><a href="#duality-relationship" class="headerlink" title="duality relationship"></a>duality relationship</h3><p>看最开始的synthesis equation 和 analysis equation</p><p>如果$X(\omega)$是时间信号$x(t)$的傅里叶变换，那么$x(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}X(\omega)e^{j\omega t} d\omega$  非常像傅里叶变换 变换的是$X(-\omega)$，再加上一个额外的因子$\frac{1}{2\pi}$  即我们可得对偶性</p><script type="math/tex; mode=display">x(t) \iff X(\omega) \quad \quad X(t) \iff 2\pi x(-\omega)</script><p><strong>这个性质非常重要！</strong> 对于离散傅里叶变换 这个性质是没有的 [15:00]显示了例子</p><h3 id="Parseval’s-relation"><a href="#Parseval’s-relation" class="headerlink" title="Parseval’s relation:"></a>Parseval’s relation:</h3><script type="math/tex; mode=display">\int_{-\infty}^{+\infty}|x(t)|^2dt = \frac{1}{2\pi}\int_{-\infty}^{+\infty}|X(\omega)|^2d\omega \\\frac{1}{T_0}\int_{T_0}|\widetilde{x}(t)|^2dt = \sum_{-\infty}^{+\infty}|a_k|^2</script><p>下面这个式子 是对于周期信号，由于周期信号的能量是无穷大，所以对于周期信号 只考虑一个周期的能力，$\widetilde{x}(t)$是一个周期的信号</p><h3 id="Time-shifting"><a href="#Time-shifting" class="headerlink" title="Time shifting:"></a>Time shifting:</h3><script type="math/tex; mode=display">x(t - t_0) \iff e^{-j\omega t_0}X(\omega)</script><p>右边的$e^{j\omega t_0}$的幅度为1，并且有一个与频率成线性关系的相位</p><p>即 a time shift corresponds to a linear change in phase and frequency</p><h3 id="Differentiation"><a href="#Differentiation" class="headerlink" title="Differentiation:"></a>Differentiation:</h3><script type="math/tex; mode=display">\frac{dx(t)}{dt} \iff j\omega X(\omega)</script><h3 id="Integration"><a href="#Integration" class="headerlink" title="Integration:"></a>Integration:</h3><script type="math/tex; mode=display">\int_{-\infty}^tx(\tau)d\tau \iff \frac{1}{j\omega}X(\omega) + \pi X(0)\delta(\omega)</script><p>因为对函数求导的时候会把常数项给消掉，所以在积分性质中 就尝试把常数项带回来 所以后面加上$\pi X(0)\delta(\omega)$</p><h3 id="Linearity"><a href="#Linearity" class="headerlink" title="Linearity:"></a>Linearity:</h3><script type="math/tex; mode=display">ax_1(t) + bx_2(t)  \iff aX_1(\omega) +bX_2(\omega)</script><p>老师说 这四个性质 for the most part, apply both to Fourier series and Fourier transforms, because what we’ve done is to incorporate the Fourier series within the framework of the Fourier transform.</p><h3 id="Two-addtional-major-properties-convolution-property-and-modulation-property"><a href="#Two-addtional-major-properties-convolution-property-and-modulation-property" class="headerlink" title="Two addtional major properties: convolution property and modulation property"></a>Two addtional major properties: convolution property and modulation property</h3><p><strong>the convolution property forms the mathematical and conceptual basis for the whole notion of filtering</strong></p><script type="math/tex; mode=display">h(t) * x(t) \iff H(\omega)X(\omega)</script><p>LTI中，对于输入信号是$\delta(t)$ 冲激响应是$h(t)$ 由于$\delta(t)$的傅里叶变换是常数1，所以输出信号的傅里叶变换是$H(\omega)$</p><p>对于卷积性质的更直观的解释是：（这个解释来源于冲激响应的傅里叶变换和频率响应之间的关系）</p><p>LTI中 如果输入信号是$e^{j\omega_0 t}$ 那么输出信号是$e^{j\omega_0 t}H(\omega_0)$ 这里的$H(\omega_0)$被称为频率响应 这个频率响应就是冲激响应的傅里叶变换</p><p>老师说 “the synthesis equation tells us how to decompose x of t as a linear combination of complex exponentials. What are the complex amplitudes of those complex exponentials? The complex amplitude of those complex exponentials is $X(\omega)$ [参考26:53的图] or proportional to x of omega $(2\pi X(\omega)d\omega)$,  as this signal goes through this linear time invariant system, what happens to each of those exponential is each one get multiplied by the frequency response at associated frequency. What comes out is the amplitude of the complex exponentials that are used to build the output</p><p>So in fact, the convolutional property simply is telling us that in terms of decomposition of the signal in terms of complex exponentials as we push that signal through a linear time invariant system, we’re separately multiplying by the frequency response the amplitudes of the exponential components used to build the input. And that sum in turn is the decomposition of the output in terms of complex exponentials.”  要尝试在概念性的程度上理解卷积特性 </p><p><strong>modulation property</strong></p><p>因为time domain和frequency domain是interchangeable 因为duality 这意味着</p><p><strong>if we multiply in the time domain, that would correspond to convolution in the frequency domain</strong>   666</p><script type="math/tex; mode=display">Modulation: \quad s(t)p(t) \iff \frac{1}{2\pi}[S(\omega)*P(\omega)] \\Convolution: \quad s(t)*p(t) \iff S(\omega)P(\omega)</script><p>对于modulation property 它是the entire basis for amplitude modulation systems as used almost universally in communications</p><p>如果we have a signal with a certain spectrum, and we multiply by a sinusoidal signal whose Fourier transform is a set of impulses, then in a frequency domain we convolve. And that corresponds to taking the original spectrum and translating it shifting it in frequency up to the frequency of the carrier.</p><h3 id="filtering的简单介绍"><a href="#filtering的简单介绍" class="headerlink" title="filtering的简单介绍"></a>filtering的简单介绍</h3><p>filtering 意思是modify separately the individual frequency components in a signal</p><p>卷积特性告诉我们 如果我们看各个频率分量，它们被乘以频率响应 那么意味着 我们可以放大或减小任何一个components separately using a linear time invariant system</p><p><strong>Ideal lowpass filter</strong> [29:54]的图  </p><p><strong>differentiator</strong> $y(t) = \frac{dx(t)}{dt} \Rightarrow H(\omega) = j\omega $ [31:10] 图，它的作用是放大高频率 衰减低频</p><p>举个例子：对于空间信号，它有高频部分和低频部分，高频对应things that are varying rapidly spatially, and low frequencies varying slowly spatially. 对[32:42]的图 进行低通滤波 就是问问视频摄像组是否可以稍微defocus it. 图像散焦[32:53] 也就是失去边缘 lost edges，edges指的是rapid variation， 我们保留的是broader slow variation.</p><p>如果想要图像的边缘增强，那么我们就differentiated the image 这个会attenuate slowly varying background and amplify the rapidly varying edges.[35:15]是经过了differentiator的图</p><h3 id="用傅里叶变换解线性常系数微分方程"><a href="#用傅里叶变换解线性常系数微分方程" class="headerlink" title="用傅里叶变换解线性常系数微分方程"></a>用傅里叶变换解线性常系数微分方程</h3><script type="math/tex; mode=display">\frac{dy(t)}{dt} + ay(t) = x(t) \\\Rightarrow j\omega Y(\omega) + aY(\omega) = X(\omega) \\\Rightarrow Y(\omega) = \frac{1}{j\omega + a}X(\omega) \\\Rightarrow h(t) \iff H(\omega) \\\Rightarrow e^{-at}u(t) \iff \frac{1}{j\omega + a}</script><p>这里用到了 differential property 和 linearity property</p><p>看到$\frac{1}{j\omega + a}$的图像 是减弱高频 保持低频，之前的将图像defocus 就相当于将图像通过这个线性微分方程</p><p>另一个例子：</p><script type="math/tex; mode=display">\frac{dy(t)}{dt} + 2y(t) = e^{-t}u(t) \\j\omega Y(\omega) + 2Y(\omega) = \frac{1}{j\omega + 1} \\Y(\omega) = \frac{1}{j\omega + 2}\frac{1}{j\omega + 1} = \frac{-1}{j\omega + 2} + \frac{1}{j\omega + 1} \\\iff -e^{-2t}u(t) + e^{-t}u(t) = y(t)</script>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kernels</title>
      <link href="2021/03/22/cs229%207/"/>
      <url>2021/03/22/cs229%207/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-7-Kernels"><a href="#Lecture-7-Kernels" class="headerlink" title="Lecture 7 Kernels"></a>Lecture 7 Kernels</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=10">视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes3.pdf">讲义链接</a></p><p><a href="https://link.springer.com/article/10.1007/BF00994018">SVM论文链接</a></p><p>感觉关于SVM的数学对于我来说还是挺硬核的</p><h3 id="视频笔记"><a href="#视频笔记" class="headerlink" title="视频笔记"></a>视频笔记</h3><p>这节课的主要内容是 <strong>optimization problem, representer theorem, kernels, examples of kernel</strong></p><p>optimal margin classifier plus kernels —-&gt; Support Vector Machine</p><p>关于上次课所讲的optimal margin classifer的优化式子为</p><script type="math/tex; mode=display">max_{\gamma, \omega, b} \gamma \\s.t. \frac{y^{(i)}(\omega^Tx^{(i)} + b)}{||\omega||} \ge \gamma \quad i = 1, \cdots, m</script><p><strong>下面的限制 表示 每个训练数据的geometric margin 都要比$\gamma$大</strong></p><p>因为同时对$\omega$和$b$进行常数倍乘，直线$\omega^Tx^{(i)} + b$是不会变的，我们可以设置$||\omega|| = \frac{1}{\gamma}$  六六六，得到</p><script type="math/tex; mode=display">\max \frac{1}{||\omega||} \\s.t. y^{(i)}(\omega^Tx^{(i)} + b)\gamma \ge \gamma</script><p>等价于</p><script type="math/tex; mode=display">min_{\omega,b} \frac{1}{2} ||\omega||^2 \\s.t. y^{(i)}(\omega^Tx^{(i)} + b) \ge 1 \quad i = 1, \cdots, m \tag{1}</script><p><strong>关于Kernels的部分 讲义写的非常好了</strong> $($这个思想贼棒$)$</p><p>假设 $\omega = \sum_{i  = 1}^m \alpha_i y^{(i)}x^{(i)}$ !!! 这个假设能让我们设计出可以有效解决高维特征的算法</p><p><strong>有一个叫做representer的定理 表示做出这个假设 我们不会损失任何性能</strong></p><p>假设成立时，通过$($batch $)$gradient descent来更新$\omega$，无论更新多少次，$\omega$永远都可以被$x^{(i)}$线性表示</p><p>这个可以用数学归纳法证明出来， 具体参考讲义里面的内容</p><p>根据这个假设来改写方程$(1)$</p><script type="math/tex; mode=display">\min_\alpha \frac{1}{2}(\sum_{i = 1}^m\alpha_iy^{(i)}x^{(i)})^T(\sum_{j = 1}^m\alpha_jy^{(j)}x^{(j)}) \\= \min_\alpha \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy^{(i)}y^{(j)}(x^{(i)})^Tx^{(j)} \\denote \quad (x^{(i)})^Tx^{(j)} \ as \ \langle x^{(i)},x^{(j)}\rangle \\s.t. \quad y^{(i)}((\sum_j\alpha_jy^{(j)}x^{(j)})^Tx^{(i)} + b) \ge 1 \\y^{(i)}(\sum_j\alpha_jy^{(j)}\langle x^{(j)}, x^{(i)}\rangle + b) \ge 1 \tag2</script><p>如果能计算$\langle x^{(j)}, x^{(i)} \rangle$  那么就能处理高维特征向量的情况</p><p>$(2)$可以继续改写，通过dual optimization，得到 讲义中的方程$(24)$ </p><p>进行预测 就是</p><script type="math/tex; mode=display">h_{\omega, b}(x) = g(\omega^Tx + b) = g((\sum_i\alpha_iy^{(i)}x^{(i)})^Tx + b) \\ = g(\sum_i\alpha_iy^{(i)} \langle x^{(i)}, x^{(j)}\rangle + b)</script><h4 id="Kernel-trick："><a href="#Kernel-trick：" class="headerlink" title="Kernel trick："></a>Kernel trick：</h4><p>1$)$  Write algorithm in terms of $\langle x^{(i)}, x^{(j)} \rangle$ or $\langle x, z \rangle$ x和z表示不同的训练数据</p><p>2$)$ Let there be mapping from $x \rightarrow \phi(x)$</p><p>3$)$ Find way to compute the kernel function $K(x, z) = \phi(x)^T\phi(z)$</p><p>4$)$ Replace $\langle x, z \rangle$ in algorithm with $K(x, z)$</p><p>总结kernel的作用：</p><p>如果训练集的特征是n维向量$x \in \mathcal{R}^n$ 将$x \rightarrow \phi(x)$ 之后维数增大，比如变成了$n^2$ 那么计算</p><p>$\phi(x)^T\phi(z)$的时间复杂度就是$O(n^2)$ <strong>如果用kernel 即用$\langle x, z \rangle$来表示出$\phi(x)^T\phi(z)$ </strong>那么时间复杂度就变成了$O(n)$   用实际的例子来说明 $x = (x_1, x_2, x_3)^T , \phi(x) = (x_1x_1,x_1x_2,x_1x_3,x_2x_1,x_2x_2,x_2x_3,x_3x_1,x_3x_2,x_3x_3)^T$ 那么kernal $\mathcal{K}( x, z ) = \phi(x)^T\phi(z) = (x^Tz)^2$  [41:30] 介绍了另一个kernel $\mathcal{K}(x,z) = (x^Tz + c)^2$</p><p>对于kernel $\mathcal{K}(x, z) = (x^Tz + c)^d$  计算这个kernel的时间复杂度还是$O(n)$，$\phi(x)$ has all  ${n+d\choose d}$ features of monomial up to order d</p><p>[47:30] 开始是SVM with a polynomial Kernel visualization (没看懂 为什么映射到二维平面映射出的是非线性边界线) </p><p>用kernel的作用是 希望能将feature映射到高维空间中，希望在低维线性不可分的数据 在高维中变得线性可分</p><p>常见的几个核： Linear Kernel $\mathcal{K}(x, z) = x^Tz$   Gaussian Kernel</p><p>[64:46] 如果数据有点noisy，想要找到一个复杂的非线性边界线，不希望try so hard to separate every little example that’s defined a really complicated decision boundary. So sometimes either the low-dimensional space or in the high dimensional space $\phi$， you don’t actually want the algorithms to separate out your data perfectly and then sometimes even in hight dimensional space your data may not be linearly separable.</p><p>$L_1$范数soft margin SVM  式子就变成了讲义25页上面的那个式子</p><p>[68:30] 举了soft margin SVM的作用 作用之一是为了解决outliner的情况 和 解决距离边界线非常近的例子</p><h3 id="SVM补充"><a href="#SVM补充" class="headerlink" title="SVM补充"></a>SVM补充</h3><p><a href="https://blog.csdn.net/Gamer_gyt/article/details/51265347">scikit-learn之SVM算法</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine</title>
      <link href="2021/03/22/cs229%206/"/>
      <url>2021/03/22/cs229%206/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-6-Support-Vector-Machine"><a href="#Lecture-6-Support-Vector-Machine" class="headerlink" title="Lecture 6 Support Vector Machine"></a>Lecture 6 Support Vector Machine</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=8">视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes3.pdf">讲义链接</a></p><p>虽然这课叫support vector machine，但是只有最后半节课讲了support vector machine</p><h3 id="Laplace-Smoothing"><a href="#Laplace-Smoothing" class="headerlink" title="Laplace Smoothing"></a>Laplace Smoothing</h3><p>如果出现了一个单词，这个单词在之前的训练集里面没有出现过，那么按照朴素贝叶斯</p><script type="math/tex; mode=display">\mathcal{P}(x_{6017} = 1 | y = 1) = \frac{0}{\# \{ y = 1\}} = 0 \\\mathcal{P}(x_{6017} = 1 | y = 0) = \frac{0}{\# \{ y = 0\}} = 0</script><p>从统计学上说，说一个没有看见过的事情的出现概率是0 是 一个坏主意</p><p>使朴素贝叶斯崩溃的地方是 如果根据上面的式子得到的参数<script type="math/tex">\phi_{6017| y = 1} = 0</script> 和<script type="math/tex">\phi_{6017|y = 0} = 0</script></p><p>用这个参数进行估计的时候，由于式子是<script type="math/tex">\prod_{i = 1}^{10000} \mathcal{P}(x_i | y)</script> 有一项为0 结果就为0</p><p>进行预测<script type="math/tex">\mathcal{P}(y = 1 | x) = \frac{\mathcal{P(x|y = 1)}\mathcal{P}(y = 1)}{\mathcal{P}(x | y = 1)\mathcal{P}(y = 1) + \mathcal{P}(x | y = 0 )\mathcal{P}(y = 0)} = \frac{0}{0 + 0}</script></p><p><strong>拉普拉斯平滑就是用来解决这个问题</strong></p><p>老师用一个motivation来引入拉普拉斯平滑：</p><p>足球队在9.12 10.10 10.17 11.21 四场球赛都输掉了 预测12.31号的足球比赛</p><p>如果用MLE的方法<script type="math/tex">\mathcal{P}(x = 1) = \frac{\# '1's}{\#'1's + \#'0's} = \frac{0}{0 + 4} = 0</script></p><p>拉普拉斯 就是把<script type="math/tex">\#'1'</script>和<script type="math/tex">\#'0'</script>比原来多1 那么<script type="math/tex">\mathcal{P}(x = 1) = \frac{\# '1's}{\#'1's + \#'0's} = \frac{(0 + 1)}{(0+1) + (4+1)} = \frac{1}{6}</script></p><p><strong>对于more generally的情况</strong></p><p>Estimate <script type="math/tex">\mathcal{P}(x = j) = \frac{\sum_{j = 1}^m1\{x^{(i)} = j\} + 1}{M + k}</script></p><p>对于Naive Bayes：</p><script type="math/tex; mode=display">\phi_{j|y = 0} = \frac{\sum_{i = 1}^m1\{x_j^{(i)} = 1,y^{(i)} = 0\} + 1}{\sum_{i = 1}^m1\{y^{(i)} = 0 \} + 2}</script><p><strong>Tips: 将连续的特征转化为离散的特征:</strong> 将连续的数值划分称若干个区间，然后对每个区间进行标注</p><p>根据经验 一般将连续的feature 分成10个bucket 效果比较好</p><p>用naive bayes来进行text classfication的一个缺点是 naive bayes丢掉了一个单词出现次数的信息</p><p>因为每个特征<script type="math/tex">x_i \in \{0, 1\}</script> <script type="math/tex">(Multi-variate Bernoulli)</script></p><p>有一种另外的表现方式 是专门用于text data的 每个特征<script type="math/tex">x_i \in \{1, \cdots, \#classes \}（Multinomial）</script> </p><p>用第二种模型的出来的式子是<script type="math/tex">\mathcal{P}(x, y) = \mathcal{P}(x | y)\mathcal{P}(y) = \prod_{j = 1}^n \mathcal{P}(x_j|y)\mathcal{P}(y)</script></p><p> <strong>与第一种模型的区别是 这里的n 是邮件里面的字数 而非所有单词的数量</strong></p><p>参数<script type="math/tex">\phi_{k|y = 0} = \mathcal{P}(x_j = k | y = 0)</script> 其中<script type="math/tex">\mathcal{P}(x_j = k|y = 0)</script>的表示的是 如果<script type="math/tex">y = 0</script> 那么第j个单词为k的概率为<script type="math/tex">\mathcal{P}(x_j = k | y = 0)</script></p><p>值得注意的是参数<script type="math/tex">\phi_{k|y=0}</script>中j并没有出现，原因是对于邮件中每个位置 第一个单词出现drugs的机会和第二个单词出现drugs的机会相同 $($drugs是随便一个单词$)$</p><p>根据MLE 求得的结果为:</p><script type="math/tex; mode=display">\phi_{k|y = 0} =  \frac{\sum_{i = 1}^m1\{y^{(i)} = 0\}\sum_{j = 1}^n1\{x_j^{(i)} = k\} + 1}{\sum_{i = 1}^m1\{y^{(i)} = 0\}n_i + \# words}</script><p><strong>这个公式 分母是训练集中所有non-spam的单词和，<script type="math/tex">n_i</script>是第<script type="math/tex">i</script>个non-spam文档的单词数目, 分子是训练集中non-spam中k单词出现的个数</strong></p><p>如果遇见不在words里面的单词，一种方法是直接丢弃，另一种方法是take the rare words and map them to a special token which traditionally is denoted UNK for unknown words.</p><p><strong>Tips: 如果要写一个包含10000行代码的程序，一种方法是先写10000行代码 然后进行第一次编译，另一种方法是编写小模块，运行小模块然后进行测试，最后构建出程序，然后开始看看第一次遇到什么语法错误；对于机器学习也是一样 一开始建立简单模型 测试 然后看问题出在哪 然后进行改进</strong></p><h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><p>如果要找一个非线性的边界线，支持向量机可以帮助我们找到潜在的非线性的决策边界</p><p>如果用logistc regression来得到一个非线性的边界，一般用的方法是对feature进行扩展</p><p>比如<script type="math/tex">x_1,x_2</script>变为<script type="math/tex">x_1,x_2,x_1^2,x_2^2,x_1x_2</script>  即原来的向量<script type="math/tex">(x_1,x_2)^T</script>变成了<script type="math/tex">\phi(x) = (x_1,x_2,x_1^2,x_2^2,x_1x_2)^T</script></p><p>high dimensional features 选择这些featrues是很痛苦的一件事情</p><p>如果是support vector machine的话，就能够推导出输入是特征<script type="math/tex">x_1,x_2</script>的算法，把它们映射到更高维度的features</p><p>老师说 support vector machines are not as effective as neural networks for many problems, but one great property of support vector machine is turn key. You kind of just turn the key and it works and there isn’t as many parameters like the learning rate and other things that you had to fiddle with</p><h4 id="Optimal-margin-classfier-separable-case"><a href="#Optimal-margin-classfier-separable-case" class="headerlink" title="Optimal margin classfier(separable case)"></a>Optimal margin classfier(separable case)</h4><p>Defn: Functional margin 对于logistic function <script type="math/tex">h_\theta(x) = g(\theta^Tx)</script> 输出的是0和1 而不是概率</p><p>logistic regression的预测是</p><p>if <script type="math/tex">\theta^Tx \ge 0 ( h_\theta(x) = g(\theta^Tx) \ge 0.5)</script> predict 1 otherwise predict 0</p><p>If <script type="math/tex">y^{(i)} = 1</script>, hope that <script type="math/tex">\theta^Tx^{(i)} \gg 0</script>   If <script type="math/tex">y^{(i)} = 0</script>, hope that <script type="math/tex">\theta^Tx^{(i)} \ll 0</script>  这就是functional margin（没懂啊）</p><p>Defn: Geometric margin</p><p>就是边界线离训练数据的距离</p><p>[60:00]的图 告诉我们对于线性可分的数据集，我们可以得到多个分界线 比如红色的和绿色的，SVM就是帮助我们找到绿色的分界线</p><h5 id="下面是正式的定义"><a href="#下面是正式的定义" class="headerlink" title="下面是正式的定义"></a>下面是正式的定义</h5><p>Notation: labels $y\in\{-1, +1\}$ have h output value in $\{-1, +1\}$</p><script type="math/tex; mode=display">g(z) = \begin{cases}1& if \ z \ge 0 \\-1& otherwise\end{cases}</script><p>Previously $h_\theta(x) = g(\theta^Tx)$ 其中$x \in \mathcal{R}^{n + 1}, x_0 = 1$</p><p>在SVM中 $h_{\omega, b}(x) = g(\omega^Tx + b )$  其中$x \in \mathcal{R}^n, b \in \mathcal{R}$ drop convetions $x_0 = 1$</p><p>Functional margin of hyperplane defined by $(\omega, b)$ w.r.t. $(x^{(i)}, y^{(i)})$</p><script type="math/tex; mode=display">\hat{\gamma}^{(i)} = y^{(i)}(\omega^Tx^{(i)} + b) \tag{1}</script><p>为了要large functional margin 即 如果$y^{(i)} = $1 我们想要$\omega^Tx^{(i)} + b \gg 0 $ 如果$y^{(i)} = -1$ 我们想要 $\omega^Tx^{(i)} + b \ll 0$， 由式子$(1)$ 等价于要$\gamma$非常的大</p><p>functional margin w.r.t. training set $\hat{\gamma} = \min_i\hat{\gamma}^{(i)} \ i = 1,\cdots, m$</p><p><strong>观察到如果将$\omega$和b 加倍 那么function margin也会加倍</strong>  所以我们要规范化长度即$||\omega|| = 1$</p><p>那么$(\omega, b) \rightarrow (\frac{\omega}{||\omega||}, \frac{b}{||b||})$ 实际上 分母可以取其他的值</p><h5 id="Geometric-margin："><a href="#Geometric-margin：" class="headerlink" title="Geometric margin："></a>Geometric margin：</h5><p>[73:44]的图 那张图清晰的定义了geometric margin</p><p>Geometric margin of hyperplane $(\omega, b)$ w.r.t. $(x^{(i)}, y^{(i)})$ is </p><script type="math/tex; mode=display">\gamma^{(i)} = \frac{y^{(i)}(\omega^Tx^{(i)} + b)}{||\omega||} \\\gamma^{(i)} = \frac{\hat{\gamma}^{(i)}}{||\omega||}</script><p>geometric margin w.r.t. training set $\gamma = \min_i\gamma^{(i)}$</p><p><strong>那么 optimal margin classifier 就是choose $\omega, b$ to maximize $\gamma$</strong></p><p>解决这个问题的方法之一是 </p><script type="math/tex; mode=display">max_{\gamma, \omega, b} \gamma \\s.t. \frac{y^{(i)}(\omega^Tx^{(i)} + b)}{||\omega||} \ge \gamma \quad i = 1, \cdots, m</script><p>这不是一个凸优化问题 所以很难用梯度下降来解决</p><p>我们把这个式子 改变一下，改成一个凸优化问题</p><script type="math/tex; mode=display">\min_{\omega, b} ||\omega||^2 \\s.t. y^{(i)}(\omega^T x^{(i)} + b) \ge 1</script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Continuous-Time Fourier Series</title>
      <link href="2021/03/22/signal%20and%20system%207/"/>
      <url>2021/03/22/signal%20and%20system%207/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-7-Continuous-Time-Fourier-Series"><a href="#Lecture-7-Continuous-Time-Fourier-Series" class="headerlink" title="Lecture 7 Continuous-Time Fourier Series"></a>Lecture 7 Continuous-Time Fourier Series</h2><p>在一个线性系统中如果输入可被分解为a linear combination of basic input，with each of these basic inputs generating an associated output</p><script type="math/tex; mode=display">x(t) = a_1\phi_1(t) + a_2\phi_2(t) + \cdots \\\phi_k(t) \rightarrow \psi_k(t) \quad and \ system\ is \ linear \\Then:y(t) = a_1\psi_1(t) + a_2\psi_2(t) + \cdots</script><p><strong>对于基本信号$\phi_k(t)$的选择 首先是a broad class of signals could be represented in terms of these basic inputs 其次是 the response to these basic inputs is easy to compute.</strong></p><p><strong>核心的两个性质</strong></p><p>在LTI系统中我们的选择是</p><p>C-T: $\phi_k(t) = \delta(t - k\Delta)$  和 $\psi_k(t) = h(t - k\Delta) \Rightarrow$ Convolution Integral</p><p>D-T: $\phi_k[n] = \delta[n - k]$ 和$\psi_k[n] = h[n - k] \Rightarrow$ Convolution Sum</p><p>新的building blocks是complex exponentials$($ 拥有上述两个性质 $)$ </p><p>$\phi_k(t) = e^{s_kt}$  $s_k$ 是complex， $\phi_k[n]$ = $z_k^n$  $z_k$ 是complex</p><p><strong>Fourier Analysis:</strong></p><p>C-T: $s_k = j\omega_k$      $\phi_k(t) = e^{j\omega_kt}$  </p><p>D-T: $|z_k| = 1$        $\phi_k[n] = e^{j\Omega_kn}$</p><p>这里对于$s_k$ 和 $z_k$ 是复数的特殊情况</p><p><strong>一般性的化 $s_k$ complex $\Rightarrow$ Laplace transforms   $z_k$ complex $\Rightarrow$ z-transforms</strong></p><h3 id="Eigenfunction-property-of-this-particular-set-of-building-blocks"><a href="#Eigenfunction-property-of-this-particular-set-of-building-blocks" class="headerlink" title="Eigenfunction property of this particular set of building blocks"></a>Eigenfunction property of this particular set of building blocks</h3><script type="math/tex; mode=display">e^{j\omega_kt} \rightarrow H(\omega_k)e^{j\omega_kt} \\e^{j\omega_kt} \rightarrow \int_{-\infty}^{+\infty}h(\tau)e^{j\omega_k(t - \tau)}d\tau \\=e^{j\omega_kt}\int_{-\infty}^{+\infty}h(\tau)e^{-j\omega_k\tau}d\tau \\= e^{j\omega_kt}H(\omega_k)</script><p>we put in a complex exponential, we get out a complex exponential of the same frequency, multiplied by a complex constant. <strong>这就是所谓的eigenfunction property</strong></p><p>即输入和输出看起来除了振幅变化之外 其余完全一样，the change amplitude being the eigenvalue</p><p>称$e^{j\omega_kt}$为eigenfunction  称$H(w_k)$为eigenvalue</p><h3 id="Periodic-Signals-Fourier-Series"><a href="#Periodic-Signals-Fourier-Series" class="headerlink" title="Periodic Signals: Fourier Series"></a>Periodic Signals: Fourier Series</h3><script type="math/tex; mode=display">x(t) = x(t + T_0) \quad T_0 \ is \ period \\w_0 = \frac{2\pi}{T_0} = 2\pi f_0</script><p>对于$e^{j\omega_0t}$ 的基本周期就是$T_0$，谐波相关的复指数是(虽然$T_0$也是周期，但是它们的基本周期更短) 如$e^{jk\omega_0t}$ 其周期为$\frac{T_0}{k} = \frac{2\pi}{k\omega_0}$  随着k的变化 这些complex exponentials 称作为harmonically related </p><p>傅里叶级数（一个非常普通的周期函数可以表示成谐波相关的复指数信号的线性组合）</p><script type="math/tex; mode=display">x(t) = \sum_{k = -\infty}^{+\infty}a_ke^{jk\omega_0t} \tag{1}</script><p> 对于这个级数我们有两个问题</p><ul><li>我们如何确定傅里叶级数的系数$a_k$</li><li>how broad a class of signals can be represented this way</li></ul><p>傅里叶级数也有其他的表现形式 trigonometric form:</p><script type="math/tex; mode=display">a_k = A_ke^{j\theta_k} = B_k + jC_k \\e^{jk\omega_0t} = cosk\omega_0t + jsink\omega_0t \\</script><p>然后老师写了下面的式子，至于怎么从$(1)$得到下面的式子 我没弄明白</p><script type="math/tex; mode=display">x(t) = a_0 + 2\sum_{k = 1}^\infty A_kcos(k\omega_0t + \theta_k) \\= a_0 + 2\sum_{k = 1}^\infty [B_kcosk\omega_0t -C_ksink\omega_0t]</script><p>注意到：</p><script type="math/tex; mode=display">\int_{T_0}e^{jm\omega_0t}dt = \begin{cases}T_0& m = 0 \\0& m \ne 0\end{cases} \\\because \int_{T_0}e^{jm\omega_0t}dt = \int_{T_0}cosm\omega_0tdt + j\int_{T_0}sinm\omega_0tdt = 0 + 0j = 0 \ for \ m\ne0</script><p>根据这个性质：</p><script type="math/tex; mode=display">\int_{T_0}x(t)e^{jn\omega_0t}dt = \int_{T_0}e^{-jn\omega_0t}\sum_{k=-\infty}^{+\infty}a_ke^{jk\omega_0t}dt \\= \sum_{k = -\infty}^{+\infty}a_k\int_{T_0}e^{-j(k - n)\omega_0t}dt \\= \begin{cases}T_0& if \ k = n \\0& if \ k \ne n\end{cases} \\\therefore \frac{1}{T_0}\int_{T_0}x(t)e^{-jn\omega_0t} = a_n \quad analysis \ equation</script><p>我们称方程$(1)$ 为synthesis equation</p><p>例子[25:00] antisymmetric periodic square wave</p><p>得到的傅里叶系数$a_0 = 0, a_k = \frac{1}{j\pi k}(1 - (-1)^k)$ 是纯虚数 和 odd sequence 即$a_k = -a_{-k}$</p><p>表示成三角级数: **注意因为$a_k$是纯虚数，乘上$j$之后就变成实数</p><script type="math/tex; mode=display">x(t) = a_0 + \sum_{k = 1}^{+\infty} 2ja_ksink\omega_0t</script><p>例子 [30:40]  symmetric periodic square wave</p><p>[34:00] 展示了级数是如何还原出方波函数的 利用部分和$x_N(t) = \frac{1}{2} + \sum_{k = 1}^N2a_kcosk\omega_0t$ 不断增加N的大小 来看图像是如何的</p><p><strong>the low frequency terms that represent the broad time behavior, and it’s the high frequency terms that are used to build up the sharp transitions in the time domain.</strong></p><p>[43:00]讲的是傅里叶级数收敛的条件</p><p>[47:00] 显示了部分和随N增加，原函数与傅里叶级数部分和的误差的能量值 随N增大的变化，当N是奇数时误差能量不变，N是偶数时误差能量减小</p>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Systems Represented by Differential and Difference Equations</title>
      <link href="2021/03/14/signal%20and%20system%206/"/>
      <url>2021/03/14/signal%20and%20system%206/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-6-System-Represented-by-Differential-and-Difference-Equations"><a href="#Lecture-6-System-Represented-by-Differential-and-Difference-Equations" class="headerlink" title="Lecture 6 System Represented by Differential and Difference Equations"></a>Lecture 6 System Represented by Differential and Difference Equations</h2><p><a href="https://www.bilibili.com/video/BV1xy4y167DD?p=6">视频链接</a> 对应于书上的2.4的内容</p><p>A particularly important set of systems, which are linear and time-invariant, are those that are represented by linear constant-coefficient differential equations in continuous time or linear constant-coefficient difference equations in discrete time</p><h3 id="n阶线性常系数微分方程的定义："><a href="#n阶线性常系数微分方程的定义：" class="headerlink" title="n阶线性常系数微分方程的定义："></a>n阶线性常系数微分方程的定义：</h3><script type="math/tex; mode=display">\sum_{k = 0}^Na_k\frac{d^ky(t)}{dt^k} = \sum_{k = 0}^Mb_k\frac{d^kx(t)}{dt^k}  \tag{1}</script><p> 常系数表示系数都是常数（不随时间变化），被称为线性是因为对应于linear combination of these derivatives, not because it corresponds to a linear system</p><p><strong>这个方程可能会也可能不会对应于线性系统</strong></p><h3 id="n阶线性常系数差分方程的定义："><a href="#n阶线性常系数差分方程的定义：" class="headerlink" title="n阶线性常系数差分方程的定义："></a>n阶线性常系数差分方程的定义：</h3><script type="math/tex; mode=display">\sum_{k=0}^Na_ky[n-k] = \sum_{k=0}^Mb_kx[n-k] \tag{2}</script><h3 id="n阶线性常系数微分方程的解法："><a href="#n阶线性常系数微分方程的解法：" class="headerlink" title="n阶线性常系数微分方程的解法："></a>n阶线性常系数微分方程的解法：</h3><p><strong>方程相对应的Homogeneous Equation定义如下，其解为$y_h(t)$</strong></p><script type="math/tex; mode=display">\sum_{k = 0}^Na_k\frac{d^ky_h(t)}{dt^k} = 0 \tag{3}</script><p>给定输入$x(t)$，如果$y_p(t)$满足方程$(1)$ 那么$y_p(t) + y_h(t)$也会满足方程$(1)$ </p><p>通常称$y_p(t)$为Particular solution，$y_h(t)$为Homogeneous solution</p><p>所以方程$(1)$并非是a unique specification of the system.</p><h3 id="n阶线性齐次常系数微分方程的解法："><a href="#n阶线性齐次常系数微分方程的解法：" class="headerlink" title="n阶线性齐次常系数微分方程的解法："></a>n阶线性齐次常系数微分方程的解法：</h3><p>“guess” solution of the form $y_h(t) = Ae^{st}$ 把这个代入到$(3)$中得</p><script type="math/tex; mode=display">\sum_{k=0}^Na_kAs^ke^{st} = 0 \label{4}\tag{4} \\</script><script type="math/tex; mode=display">\because e^{st} \ne 0, A \ne 0 \quad \therefore \sum_{k=0}^Na_ks^k = 0 \quad N \ roots \ s_i \quad i = 1,\cdots,N \tag{5}</script><script type="math/tex; mode=display">y_h(t) = A_1e^{s_1t} + A_2e^{s_2t} + \cdots + A_Ne^{s_Nt} \tag{6}</script><p>通过方程$(5)$能得到得是N个s得值 但是常数$N$以及$A_1,\cdots,A_N$是不确定的</p><p>我们要解出这些常数 就需要N auxiliary conditions, 例如</p><script type="math/tex; mode=display">y(t), \frac{dy(t)}{dt},\cdots,\frac{d^{N - 1}y(t)}{dt^{N-1}} \ at \ t = t_0</script><p>取决于这些auxiliary conditions，the system may or may not correspond to a linear system</p><ul><li>Linear system $\iff$ auxiliary conditions = 0</li><li>Causal LTI $\iff$ initial rest： if $x(t) = 0$ for $t &lt; t_0$ then $y(t) = 0$ for $t &lt; t_0$</li></ul><p>例：$\frac{dy(t)}{dt} + ay(t) = x(t)$ 其对应的齐次方程为$\frac{dy_h(t)}{dt} + ay_h(t) = 0 \tag{7}$</p><p>猜测解为$y_h(t)  = Ae^{st}$ 代入得 $Ase^{st} + aAe^{st} = 0$ 化简得$s + a = 0$</p><p>得$y_h(t) = Ae^{-at}$</p><p>假设给定特定得输入为$x(t) = ku(t)$ 则$x(1) = k$ 代入微分方程得$\frac{dy(t)}{dt} + ay(t) = ku(t)$</p><p>解得相应的特解为$y_p = \frac{k}{a}[1 - e^{-at}]u(t)$</p><p>对于该微分方程的a family of solutions 为$y(t) = \frac{k}{a}[1 - e^{-at}]u(t) + Ae^{-at}$</p><p>求出A的值要auxiliary conditions，对于initial rest的条件 $\Rightarrow$ Causal, LTI $\Rightarrow$ $y(t) = \frac{k}{a}[1 - e^{-at}]u(t)$</p><p><strong>求冲激响应 这个知识点重要</strong></p><p>对于LTI系统 级联的顺序无关紧要</p><script type="math/tex; mode=display">u(t) \rightarrow \boxed{\frac{d}{dt}} \rightarrow \boxed{h(t)} \rightarrow h(t)</script><p>上面的系统与下面等效</p><script type="math/tex; mode=display">u(t) \rightarrow \boxed{h(t)}  \rightarrow \boxed{\frac{d}{dt}}  \rightarrow h(t)</script><p>上面的中间结果为$\delta(t)$，下面的中间结果为$s(t) = \frac{1}{a}[1 - e^{-at}]u(t)$ 所以求$h(t)$ 就可以直接对$s(t)$求导</p><p>解得$h(t) = \frac{s(t)}{dt} = u(t)\frac{d}{dt}\frac{1}{a}[1 - e^{-at}] + \frac{1}{a}[1 - e^{-at}]\frac{d}{dt}u(t) = e^{-at}u(t) + \frac{1}{a}[1 - e^{-at}]\delta(t)$</p><p>当$t \ne 0$时 $\delta(t)$为0， 得$h(t) = e^{-at}u(t)$  当$t = 0时$ $1 - e^{-at} = 0$ 所以无论$t$取什么值，右边的都为0</p><p>由之前的知识知 如果该系统是稳定的， 那么$a &gt; 0$</p><h3 id="n阶线性常系数差分方程的解法："><a href="#n阶线性常系数差分方程的解法：" class="headerlink" title="n阶线性常系数差分方程的解法："></a>n阶线性常系数差分方程的解法：</h3><p>与连续的情况类似 齐次方程为</p><script type="math/tex; mode=display">\sum_{k = 0}^N a_ky_h[n - k] = 0 \quad (Homogeneous \ Equation) \tag{8}</script><p>给定输入$x[n]$，如果$y_p[n]$满足方程$(8)$ 那么$y_p[n] + y_h[n]$也会满足方程$(1)$ </p><p>通常称$y_p[n]$为Particular solution，$y_h[n]$为Homogeneous solution</p><h3 id="n阶线性齐次常系数微分方程的解法：-1"><a href="#n阶线性齐次常系数微分方程的解法：-1" class="headerlink" title="n阶线性齐次常系数微分方程的解法："></a>n阶线性齐次常系数微分方程的解法：</h3><p>“guess” solution of the form $y[n] = Az^n$ 代入到方程$(8)$得$\sum_{k = 0}^Na_kAz^nz^{-k} = 0$</p><p>同样$\because A \ne 0，z \ne 0$ 我们得到$\sum_{k = 0}^Na_kz^{-k} = 0$ N roots $z_1, z_2, \cdots, z_N$</p><p>最终的解是$y_h[n] = A_1z_1^n + \cdots + A_Nz_N^n$</p><p>我们要解出这些常数 就需要N auxiliary conditions, 例如</p><script type="math/tex; mode=display">y[n_o], y[n_o - 1], y[n_o - 2],\cdots,y[n_o - N + 1]</script><p>取决于这些auxiliary conditions，the system may or may not correspond to a linear system</p><ul><li>Linear system $\iff$ auxiliary conditions = 0</li><li>Causal LTI $\iff$ initial rest： if $x[n] = 0$ for $n &lt; n_o$ then $y[t] = 0$ for $n &lt; n_o$</li></ul><p>微分方程和差分方程很类似， 某些情况下，差分方程更容易处理</p><p>假设系统是causal的：</p><p>对于方程$(2)$的解，$y[n] = \frac{1}{a_o} \{  \sum_{k = 0}^Mb_kx[n - k] - \sum_{k = 1}^Na_ky[n - k] \}$</p><p>如果我们知道$x[n]$和$y[n_o - 1], y[n_o - 2], \cdots, y[n_o - N]$ 那么我可以求得$y[n_o]$继而可以求$y[n_o + 1]$</p><p>例 $y[n] - ay[n - 1] = x[n]$ $\Rightarrow$ $y[n] = x[n] + ay[n - 1] \tag{9}$</p><p>causal LTI $\iff$ Initial Rest  假设输入$x[n] = \delta[n]$ 代入得$h[n] = \delta[n] + ah[n - 1]$</p><p>$h[n] = 0$ 对于$n &lt; 0$ 然后$n = 0$ 代入得$h[0] = 0$ 继续得$h[1] = a, h[2] = a^2$</p><p>如果是这个系统是稳定话 $\iff |a| &lt; 1$</p><p>求解齐次方程的解 $y_h[n]$  令$y_h[n] = Az^n$ 代入得$Az^n - aAz^{n - 1} \Rightarrow 1 - az^{-1} = 0 \Rightarrow a = z$</p><p>$\Rightarrow y_h[n] = Aa^n$  所以对与这个系统$\delta[n] \rightarrow a^nu[n] + Aa^n$</p><p>如果系统是causal的LTI 那么$A = 0$</p><p>[32:04] 将方程$(9)$画成图，这个图是一个反馈系统 [33:49] 画的是方程$(2)$对应的图</p><p>接下来的是将连续的情况画成图</p>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Perceptron Generalized Linear Model</title>
      <link href="2021/03/13/cs229%204/"/>
      <url>2021/03/13/cs229%204/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-4-Perceptron-Generalized-Linear-Model"><a href="#Lecture-4-Perceptron-Generalized-Linear-Model" class="headerlink" title="Lecture 4 Perceptron Generalized Linear Model"></a>Lecture 4 Perceptron Generalized Linear Model</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=5">视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes1.pdf">讲义链接</a></p><h3 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h3><p>感知机现实不怎么会用到，学这个主要是出于历史原因</p><p>logistic regression用的函数是sigmoid函数 $g(z) = \frac{1}{1 + e^{-z}}$</p><p>而percetron用的是$g(z) = \begin{cases} 1&amp; z \ge 0 \\0&amp;  z &lt; 0 \end{cases}$</p><p>两者的更新公式都是$\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}$</p><p>其中$y^{(i)} - h_\theta(x^{(i)})$的值只可能是1或者0或者-1</p><p>翻翻李航的书(第二版) P39  $y$的取值是$\{-1,1\}$，更新标准是 如果错误分类，则</p><script type="math/tex; mode=display">\omega \leftarrow \omega + \eta y_i x_i \\b \leftarrow b + \eta y_i</script><p>[9:00] 左右的视频讲的非常清楚 图画的很好</p><p>感知机一般不用于实际的原因之一是这个模型没有概率解释，而且感知机无法对XOR进行分类</p><h3 id="exponential-family"><a href="#exponential-family" class="headerlink" title="exponential family"></a>exponential family</h3><p>这是一类概率分布 probability density function为：</p><script type="math/tex; mode=display">p(y;\eta) = b(y)exp(\eta^TT(y) - a(\eta))</script><p>这个公式里面$y$是数据，$\eta$是natural parameter，$T(y)$是充分统计量，$b(y)$是base measure，$a(\eta)$是log-partition（为了让这个式子积分结果为1）</p><p>关于充分统计量的知识<a href="https://www.zhihu.com/question/41367707/answer/572628701">链接</a>  在数理统计书上的定义是 设$\theta$是总体分布中的参数，$X_1,X_2,\cdots,X_n$是来自此总体的样本，$T = T(X_1, X_2, \cdots,X_n)$是统计量，若在已知$T = t$的条件下，样本的条件分布于$\theta$无关，则称$T$为$\theta$的充分统计量</p><p>Bernoulli(Binary data)  $\phi = $ probability of event</p><p>其PDF为$p(y;\phi) = \phi^y(1-\phi)^{(1 - y)}$ 我们要把这个PDF massage 成exponential family的形式</p><script type="math/tex; mode=display">\begin{aligned}p(y;\phi) &= exp(log(\phi^y(1-\phi)^{(1-y)})) \\&= exp[log(\frac{\phi}{1-\phi})y + log(1 - \phi)]\end{aligned}</script><p>massage: $b(y) = 1, T(y) = y,\eta = log(\frac{\phi}{1 - \phi}) + log(1 - \phi),a(\eta) = -log(1 - \phi)$</p><p>由$\eta = log(\frac{\phi}{1 - \phi})$可得$\phi = \frac{1}{1 + e^{-\eta}}$代入道$a(\eta)$中可得$a(\eta) = log(1 + e^\eta)$</p><p>由上面的massage 可知Bernoulli分布是exponential 分布</p><p>Gaussian（假设方差为1）</p><p>其PDF为$p(y;\mu) = \frac{1}{\sqrt{2\pi}}exp(-\frac{(y - \mu)^2}{2})$把这个PDF massage成 exponential family的形式</p><script type="math/tex; mode=display">p(y;\mu) = \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}exp(\mu y - \frac{1}{2}\mu^2)</script><p>其中$b(y) = \frac{1}{\sqrt{2\pi}}exp{-\frac{y^2}{2}}, T(y) = y, \eta = \mu, a(\eta) = \frac{\mu^2}{2} = \frac{\eta^2}{2}$</p><p><strong>exponential family的性质</strong></p><ul><li>如果我们对exponential family进行MLE w.r.t. $\eta$，那么the optimization problem is concave</li><li>$E[y;\eta] = \frac{\partial}{\partial\eta}a(\eta)$</li><li>$Var[y;\eta] = \frac{\partial^2}{\partial \eta^2}a(\eta)$</li></ul><h3 id="Generalized-Linear-Models"><a href="#Generalized-Linear-Models" class="headerlink" title="Generalized Linear Models"></a>Generalized Linear Models</h3><p><strong>we can build many powerful models by choosing an appropriate family in the exponential family and kind of plugging it onto a linear model</strong></p><p>对于GLM的假设</p><ul><li><p>$y | x;\theta \sim$ Exponential Family$(\eta)$ 对于不同的数据类型可以用不同的模型Real - Gaussian，Binary - Bernoulli，Count - Poisson，$R^+$ - Gamma, exponential，Distribution - Beta，Dirichlet 后面两个一般用于贝叶斯统计学习</p></li><li><p>$\eta = \theta^Tx \ \theta \in \mathrm{R}^n \ x \in \mathrm{R}^n$</p></li><li><p>Test Time: Output为$E[y|x;\theta] \Rightarrow h_\theta(x) = E[y|x;\theta]$</p><p>Train Time: $\max_\theta log p(y^{(i)}, \theta^Tx^{(i)})$ 用gradient ascent</p></li></ul><p><strong>GLMs Training</strong></p><p>Learning Update Rule(所有GLMs都一样) $\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x{(i)}))x_j^{(i)}$</p><p>[52:12] 一些术语 $\eta$ - natural parameter </p><p>$\mu = E[y;\eta] = g(\eta)$ Canonical Response function</p><p>$\eta = g^{-1}(\mu)$ Canonical Link function</p><p>[53:50] 开始讲述了three different kinds of parameterizations we have</p><p>model parameters       natural parameters     canonical parameters</p><p>[58:40] 有人问到如何对于输出来选择分布</p><p>老师的Answer: the choice of what distribution you are going to choose is really dependent on the task that you have. So if  your task is regression where you want to output real valued numbers like price of the house then you choose a distribution over the real numbers like a Gaussian. If your task is classification, where your output is binary 0, or 1, you choose a distribution that models binary data</p><h3 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h3><p>用于多个类别的分类，输出是一个one-hot vector，就是一个只有0和1构成的向量，向量里面每个位置代表着一个类，一个向量里面只有一个1其余的都是0，1所在的位置就表示输出是相应的类</p><p>这部分讲义里面写的挺好</p><p>softmax 输出的是所有类的一个概率分布</p><h4 id="666-one-hot-vector-可以看作是一个概率分布"><a href="#666-one-hot-vector-可以看作是一个概率分布" class="headerlink" title="666 one-hot vector 可以看作是一个概率分布"></a>666 one-hot vector 可以看作是一个概率分布</h4><p><strong>真实标签 也可以看作是一个概率分布，对于标签的类概率是1，其余类概率是0</strong> </p><p>我们希望两个distribution变得相近，the term for that is to minimize the cross entropy between the two distributions</p><h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>这是官网上面关于高斯分布的材料的学习笔记，<a href="http://cs229.stanford.edu/section/gaussians.pdf">链接part1</a>  以及 <a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/more_on_gaussians.pdf">链接part2</a></p><p>对于univariate normal distribution，the coefficient $\frac{1}{\sqrt{2\pi}\sigma}$ is a constant that does not depend  on x; <strong>我们可以把它看作是为了让下面的积分为1 所存在的一个”normalization factor”</strong></p><script type="math/tex; mode=display">\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}exp(-\frac{1}{2\sigma^2}(x - \mu)^2) = 1</script><p>Proposition 1的证明在下面的Appendix A.1</p><script type="math/tex; mode=display">\Sigma = E[(X - \mu)(X - \mu)^T] = E[XX^T] - \mu\mu^T</script><p>Proposition 2证明$\Sigma$是半正定的（这里$\Sigma$的定义是the covariance matrix corresponding to some random vector X）证明中有个等式引起了我的注意：</p><script type="math/tex; mode=display">\sum_i\sum_jx_ix_jz_iz_j = (x^Tz)^2</script><p>x的维数要和z的维数相同</p><p>假如维数为1 则$\sum_i\sum_jx_ix_jz_iz_j = x_1^2z_1^2 = (x^Tz)^2$</p><p>假如维数为2 则$\sum_i\sum_jx_ix_jz_iz_j = x_1^2z_1^2 + 2x_1x_2z_1z_2 + x_2^2z_2^2 = (x1z_1 + x_2z_2)^2 = (x^Tz)^2$</p><p>假如维数为k成立，那么维数为k + 1时</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i = 1}^{k + 1}\sum_{j = 1}^{k + 1}x_ix_jz_iz_j  &= x_{k+1}z_{k+1}\sum_{j= 1}^{k + 1}x_jz_j +\sum_{i = 1}^k\sum_{j = 1}^{k + 1}x_ix_jz_iz_j \\&= x_{k+1}z_{k+1}\sum_{j= 1}^{k + 1}x_jz_j + x_{k + 1}z_{k + 1}\sum_{i = 1}^kx_iz_i + \sum_{i = 1}^k\sum_{j = 1}^kx_ix_jz_iz_j\\&= (x^Tz)_{k \ dimension}^2 + x_{k + 1}^2z_{k + 1}^2 + 2x_{k+1}z_{k+1}\sum_{i = 1}^kx_iz_i \\&= (x^Tz)_{k \ dimension}^2 + 2x_{k + 1}z_{k + 1}(x^Tz)_{k \ dimension} +  x_{k + 1}^2z_{k + 1}^2 \\&= ((x^Tz)_{k \ dimension}^2 + x_{k + 1}z_{k + 1})^2 \\&= (x^Tz)^2_{k+1 \ dimension} \quad \quad  \quad \quad \quad \quad q.e.d.\end{aligned}</script><p><strong>讲义里面的Shape of isocontours写的非常具有启发性</strong></p><p>对于$\Sigma$是对角阵的情况：</p><script type="math/tex; mode=display">c = \frac{1}{2\pi\sigma_1\sigma_2}exp(-\frac{1}{2\sigma_1^2(x_1 - \mu_1)^2} - \frac{1}{2\sigma_2^2}(x_2 - \mu_2)^2) \\log(\frac{1}{2\pi c\sigma_1\sigma_2}) = \frac{1}{2\sigma_1^2}(x_1 - \mu_1)^2 + \frac{1}{2\sigma_2^2}(x_2 - \mu_2)^2 \\1 = \frac{(x_1 - \mu_1)^2}{2\sigma_1^2log(\frac{1}{2\pi c\sigma_1\sigma_2})} + \frac{(x_2 - \mu_2)^2}{2\sigma_2^2log(\frac{1}{2\pi c\sigma_2\sigma_2})}</script><p><strong>等高线是一个axis-aligned ellipse</strong></p><p>对于$\Sigma$不是对角阵的情况：等高线是一个rotated ellipse</p><p>扩展到n维的情况 图形称作为ellipsoids</p><p><strong>Appendix A.2非常棒</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Locally Weighted Linear Regression</title>
      <link href="2021/03/08/cs229%203/"/>
      <url>2021/03/08/cs229%203/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-3-Locally-Weighted-Linear-Regression"><a href="#Lecture-3-Locally-Weighted-Linear-Regression" class="headerlink" title="Lecture 3 Locally Weighted Linear Regression"></a>Lecture 3 Locally Weighted Linear Regression</h3><p>这是根据<a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=2">吴恩达CS229课程视频</a>的笔记</p><p>官方有一个notes，<a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes1.pdf">链接</a></p><p>对于一个比较复杂形状的函数，自己定义函数的形式$y=f(x)$比较困难，这时可以用locally weighted linear regression</p><ul><li><p>”Parametric” learning algorithm:</p><p>Fit fixed set of parameters $\theta_i$ to data</p></li><li><p>“Non-Parameteric” learning algorithm</p><p>Amount of data/parameters you need to keep grows(linearly) with the size of data</p><p>LWR，KNN就是Non-Parameteric的</p></li></ul><p><strong>LWR通常不外推，意思是对于在训练集两端外的预测 效果可能会不好</strong></p><p><strong>老师更倾向对于相对低维的数据集使用LWR(比如n = 2或3，有很多数据的情况)</strong></p><h4 id="Probabilistic-interpretation"><a href="#Probabilistic-interpretation" class="headerlink" title="Probabilistic interpretation"></a>Probabilistic interpretation</h4><p>这个部分是说明cost function的由来，基于一个重要假设是$y^{(i)} = \theta^\mathrm{T}x^{(i)} + \epsilon^{(i)}$ 其中$\epsilon^{(i)}$是独立同分布且服从$\mathcal{N}(0, \sigma^2)$，$\epsilon^{(i)}$代表着随机误差</p><p><strong>注意，这个假设就表示训练集中的每个数据是IID的，如果现实中数据不是IID的，那么用这个方法就不合理，可以去构建一个更复杂的模型</strong></p><p>讲义当中的公式$\mathcal{P}(y^{(i)}|x^{(i)};\theta)$里面$x^{(i)}$和$\theta$中间的分号表示$\theta$是一个参数，如果是逗号，那么说明$\theta$是一个随机变量</p><p><strong>likelihood和probability的区别</strong></p><p>视频里面是这么说的：</p><p>the likelihood of the parameters is exactly the same thing as the probability of the data就是函数形式相同，if you view this thing as a function of the parameters holding the data fixed, then we call that likelihood</p><h4 id="Classification-Problem"><a href="#Classification-Problem" class="headerlink" title="Classification Problem"></a>Classification Problem</h4><p>视频44分钟里面画了一张图，用之前的线性回归来应用于这个数据集效果是不好的，线性回归的方法是回归出一条直线，然后设置一个阈值，通过阈值进行分类(大于阈值的一类，小于阈值的另一类)</p><p>这个方法的缺陷在于，45分钟的图，如果有一个稍微偏远的数据，可能会明显改变直线，因而改变决策边界，效果不好。</p><p>我们想要一个$h_\theta(x) \in [0, 1]$  sigmoid函数 $g(x) = \frac{1}{1 + e^{-z}}$ 代入得 $h_\theta(x) = g(\theta^{\mathrm{T}}x) = \frac{1}{1 + e^{-\theta^{\mathrm{T}}x}}$</p><p><em>这里有一个假设$\mathcal{P}(y = 1 | x ; \theta) = h_\theta(x)$</em> 由于是二分类的 所以$\mathcal{P}(y = 0 | x ; \theta) = 1 - h_\theta(x)$</p><p>将这两个式子写成一个式子 $\because y \in \{0 , 1\}$</p><script type="math/tex; mode=display">\mathcal{P}(y | x ; \theta) = h(x)^y(1-h(x))^{1-y}    \quad 666</script><p>对这个概率分布，we can write down the likelihood of the parameters as:</p><script type="math/tex; mode=display">\begin{aligned}L(\theta) &= p(\vec{y}|X,\theta) \\&= \prod_{i = 1}^n p(y^{(i)}|x^{(i)};\theta) \\&= \prod_{i = 1}^n (h_\theta(x^{(i)}))^{y^{(i)}}(1 - h_\theta(x^{(i)}))^{(1 - y^{(i)})}\end{aligned}</script><p>maximize the log likelihood:</p><script type="math/tex; mode=display">\begin{aligned}\ell(\theta) &= logL(\theta) \\&=  \sum_{i = 1}^n y^{(i)}logh(x^{(i)}) + (1 - y^{(i)})log(1 - h(x^{(i)}))\end{aligned}</script><p><strong>要注意的是</strong> 训练的时候是用梯度上升 而不是梯度下降！</p><p><strong>选择$h_\theta(x)$的原因之一是保证likelihood function只有一个全局最大值（concave function)</strong></p><p>对于logistics regression的常见问题</p><p><a href="https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c">为什么logistics regression不采用MSE作为Loss Function</a></p><p>总结一下就是<strong>MSE doesn’t strongly penalize misclassifications</strong>和<strong>MSE loss function for logistic regression is non-convex</strong></p><h4 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h4><p>这个方法和gradient descent的区别是 这个方法一次更新的跨度更大(视频66: 12)，视频中举例 如果用1000次gradient descent得到好的$\theta$，那么用Newton’s method 经过10次迭代就能得到好的$\theta$，不过每次迭代的代价会比较昂贵</p><p>牛顿方法旨在解决的问题是， 我们有一个函数$f$，我们想要找到一个$\theta$ 使得$f(\theta) = 0$</p><p>将牛顿方法应用到机器学习中就是用牛顿方法来找导数为0的点</p><p>视频69分钟开始 演示牛顿方法的过程，更新算法如下：</p><script type="math/tex; mode=display">\theta^{(t + 1)} := \theta^{(t)} - \frac{f(\theta^{(t)})}{f^\prime(\theta^{(t)})}</script><p>当$\theta$是一个向量的时候，$\theta \in \mathcal{R}^{n + 1}$</p><script type="math/tex; mode=display">\theta^{(t + 1)} := \theta^{(t)} - H^{-1}\nabla_\theta\mathcal{l}(\theta)</script><p>其中H是Hessian matrix $H_{ij} = \frac{\partial^2\mathcal{l}(\theta)}{\partial\theta_i\partial\theta_j}$， 这个算法的耗时地方在于对hessian matrix求逆耗时，如果拥有的参数数量10个或50个 老师推荐有这个方法</p><p>Newton’s method enjoys a property called quadratic convergence(意思是如果某一次迭代后$\theta$的误差是0.01error 再经过一次迭代之后 误差变为0.0001error) —&gt; 我没懂他说的0.01error是什么意思，emmm… 反正就是很快的</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convolution</title>
      <link href="2021/03/08/signal%20and%20system%204/"/>
      <url>2021/03/08/signal%20and%20system%204/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-4-Convolution"><a href="#Lecture-4-Convolution" class="headerlink" title="Lecture 4 Convolution"></a>Lecture 4 Convolution</h3><p><strong>这个笔记是根据</strong><a href="https://www.bilibili.com/video/BV1xy4y167DD?p=4">奥本海姆信号与系统的视频课来写的</a></p><p>如何利用time invariant 和linear属性—&gt;将一个信号分解为一组基本信号</p><p>对于基本信号的选择，目的是为了能够轻松地产生输出信号</p><ul><li>delayed impulses  &lt;—-&gt;  Convolution</li><li>complex exponential &lt;—-&gt;  Fourier analysis</li></ul><h4 id="将离散信号表示为冲激信号的线性组合"><a href="#将离散信号表示为冲激信号的线性组合" class="headerlink" title="将离散信号表示为冲激信号的线性组合"></a>将离散信号表示为冲激信号的线性组合</h4><script type="math/tex; mode=display">x[n] = \begin{cases}c& n = 0 \\0& n \neq 0\end{cases}\\其中c \neq 0</script><p>对于这个信号 可以表示为$x[0] \times \delta[n]$</p><p>同样可得$x[1] = x[1] \times \delta[n - 1]$还有$x[-1] = x[-1] \times \delta[n + 1]$</p><p>对于任意的一个信号$x[n]$ 就可以分解为$x[n] = \sum_{k=-\infty}^{+\infty}x[k]\delta[n-k]$</p><p><strong>这样表示的作用是什么？</strong></p><p>如果我们是在线性系统中，那么响应是线性组合的形式（假如$\delta[n-k]$的响应是$h_k[n]$），那么</p><script type="math/tex; mode=display">y[n] = \sum_{k=-\infty}^{+\infty}x[k]h_k[n]</script><p>如果系统是time-invariant 那么$h_k[n] = h_0[n - k]$ 将$h_0[n]$记为$h[n]$ 即为unit impulse</p><p>对于LTI系统                                       $y[n] = \sum_{k=-\infty}^{+\infty}x[k]h[n-k]$</p><h4 id="将连续信号表示为冲激信号的线性组合"><a href="#将连续信号表示为冲激信号的线性组合" class="headerlink" title="将连续信号表示为冲激信号的线性组合"></a>将连续信号表示为冲激信号的线性组合</h4><p>首先将连续时间信号分解为连续任意狭窄的矩形，当这些矩形的宽度变为0，会越近似于原信号，<strong>要注意的是 当每个矩形变得越来越窄就越来越多地与冲激信号对应</strong></p><script type="math/tex; mode=display">x(t) = \begin{cases}x(0) \times \delta_{\Delta}(t)\times\Delta& 0 \le t \le \Delta \\0& otherwise\end{cases}</script><script type="math/tex; mode=display">x(t) \approx x(0)\delta_{\Delta}(t)\Delta + x(\Delta)\delta_{\Delta}(t - \Delta)\Delta + x(-\Delta)\delta_{\Delta}(t + \Delta)\Delta + ... \\x(t) \approx \sum_{k=-\infty}^{+\infty}x(k\Delta)\delta_{\Delta}(t - k\Delta)\Delta \\x(t) = \lim_{\Delta \to 0}\sum_{k=-\infty}^{+\infty}x(k\Delta)\delta_{\Delta}(t - k\Delta)\Delta \\= \int_{-\infty}^{+\infty}x(\tau)\delta(t - \tau)d\tau  \quad sifting \ integral</script><p>对于线性系统输出为：</p><script type="math/tex; mode=display">y(t) = \lim_{\Delta \to 0}\sum_{k = -\infty}^{+\infty}x(k\Delta)h_{k\Delta}(t)\Delta \\=\int_{-\infty}^{\infty}x(\tau)h_{\tau}(t)d\tau</script><p>同样 如果系统是time invariant $h_{k\Delta} = h_0(t - k\Delta)$ 和 $h_{\tau}(t) = h_0(t - \tau)$ 可得：</p><script type="math/tex; mode=display">y(t) = \int_{-\infty}^{+\infty}x(\tau)h(t - \tau)d\tau</script><p>由上述可知 <strong>如果我们知道了对应于t=0或者n=0的冲激响应，那么通过卷积我们能得到任意输入的输出</strong></p><p><strong>我们将*符号表示卷积</strong></p><script type="math/tex; mode=display">y[n] = \sum_{k = -\infty}^{+\infty}x[k]h[n - k] = x[n] * h[n] \\y(t) = \int_{-\infty}^{+\infty}x(\tau)h(t - \tau)d\tau = x(t) * h(t)</script><p>从23分钟后就是介绍离散的例子 求$x[n] = u[n]$和$h[n] = \alpha^nu[n]$的响应，将$x[n] 变为截取的u[n]$</p><p>29分钟后是介绍连续的例子 $x(t) = u(t)$ 和 $h(t) = e^{-at}u(t)$ 同样也有$u(t)$的截取形式</p><p>视频里面的动画来显示卷积的过程挺不错</p><p>47分钟开始分析了一个连续时间卷积的例子</p><script type="math/tex; mode=display">y(t) = \int_{-\infty}^{\infty}x(\tau)h(t - \tau)d\tau \\= \int_{-\infty}^{\infty}u(\tau)e^{-a(t - \tau)}u(t - \tau)d\tau</script><p>考虑区间$t &lt; 0$， 因为$u(\tau)$在$\tau$小于0时值为0 当$\tau$大于等于0时$u(t - \tau)$为0 所以积分的值为0</p><p>考虑区间$t &gt; 0$ <strong>注意视频中$t = 0$的情况被忽略了，我觉得是因为重点不在$t = 0$</strong></p><p>当$\tau &lt; 0$时$u(\tau)$为0， 当$\tau &gt; t$时$u(t - \tau)$为0，也就是在区间$(-\infty, 0)$ 和$(t, +\infty)$积分的值为0</p><script type="math/tex; mode=display">\therefore \int_{-\infty}^{\infty}u(\tau)e^{-a(t - \tau)}u(t - \tau)d\tau \\= \int_{0}^{t}u(\tau)e^{-a(t - \tau)}u(t - \tau)d\tau  \\ = \int_{0}^{t}e^{-a(t - \tau)}d\tau \\= \frac{1}{a}[e^{at} - 1] \\\therefore y(t) = \begin{cases}0& t < 0 \\\frac{1}{a}{e^{at} - 1}& t > 0\end{cases}</script>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Signals and Systems Part II</title>
      <link href="2021/03/08/signal%20and%20system%203/"/>
      <url>2021/03/08/signal%20and%20system%203/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-3-Signals-and-Systems-Part-II"><a href="#Lecture-3-Signals-and-Systems-Part-II" class="headerlink" title="Lecture 3 Signals and Systems: Part II"></a>Lecture 3 Signals and Systems: Part II</h3><p>这是根据公开课视频写的笔记<a href="https://www.bilibili.com/video/BV1xy4y167DD?p=3">链接</a></p><h3 id="几个重要的函数"><a href="#几个重要的函数" class="headerlink" title="几个重要的函数"></a>几个重要的函数</h3><h5 id="离散的unit-step函数"><a href="#离散的unit-step函数" class="headerlink" title="离散的unit step函数"></a>离散的unit step函数</h5><script type="math/tex; mode=display">\begin{equation}u[n] = \begin{cases}1& n \ge 0 \\0& n < 0\end{cases}\end{equation}</script><h5 id="离散的unit-impulse-函数"><a href="#离散的unit-impulse-函数" class="headerlink" title="离散的unit impulse 函数"></a>离散的unit impulse 函数</h5><script type="math/tex; mode=display">\begin{equation}\delta[n] = \begin{cases}1& n = 0 \\0& otherwise\end{cases}\end{equation}</script><p>两者之间的关系是</p><script type="math/tex; mode=display">\delta[n] = u[n] - u[n - 1] \quad (first \quad difference) \\ u[n] = \sum_{m = -\infty}^{n}\delta[m] \quad (running \quad sum)</script><p>unit step sequence can be thought of as a succession of unit impulses one following another</p><p><strong>这个知识点重要</strong></p><script type="math/tex; mode=display">u[n] = \delta[n] + \delta[n - 1] + \delta[n - 2] + ... \\u[n] = \sum_{k = 0}^{\infty}\delta[n - k]</script><h5 id="连续的unit-step函数"><a href="#连续的unit-step函数" class="headerlink" title="连续的unit step函数"></a>连续的unit step函数</h5><script type="math/tex; mode=display">\begin{equation}u(t) = \begin{cases}1& t > 0 \\0& t < 0\end{cases}\end{equation}</script><p>关于unit step函数在t = 0的定义有多种。t = 0处的定义不是重点，重点是这个函数的关键在于t = 0时 这个函数是不连续的</p><p>unit step可以看作是一个approximation unit step函数取极限得到</p><script type="math/tex; mode=display">\begin{equation}u_{\Delta}(t) = \begin{cases}1& t > \Delta \\\frac{1}{\Delta}& 0 \le t \le \Delta \\0& t < 0\end{cases}\end{equation} \\u(t) = \lim_{\Delta \to 0}u_{\Delta}(t)</script><h5 id="连续的unit-impulse函数"><a href="#连续的unit-impulse函数" class="headerlink" title="连续的unit impulse函数"></a>连续的unit impulse函数</h5><p>类比unit step和unit impulse之间的关系 来定义连续的unit impulse函数</p><p>unit impulse 就是 unit step的导数，由approximation unit step函数的定义来得到</p><script type="math/tex; mode=display">\delta(t) = \frac{du(t)}{dt} \\\delta_{\Delta}(t) = \frac{du_{\Delta}(t)}{dt} \\\begin{equation}\delta_{\Delta}(t) = \begin{cases}\frac{1}{\Delta}& 0 \le t \le \Delta \\0& otherwise\end{cases}\end{equation} \\no \ matter \ what \ the \ value \ of \ \Delta \ is\ , \ the \ area \ is \ always \ equal \ to \ 1 \\\delta(t) = \delta_{\Delta}(t) \quad as \quad \Delta \to 0</script><p>连续时间下，unit step函数和unit impulse函数之间的关系</p><script type="math/tex; mode=display">\delta = \frac{du(t)}{dt} \\u(t) = \int_{-\infty}^{t}\delta(\tau)d\tau</script><h3 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h3><p><strong>定义：a transformation from an input signal to an output signal </strong></p><ul><li>cascade system</li><li>parallel system</li><li>feedback system</li></ul><p>对于LTI系统，the overall system transformation is independet of the order in which the systems are cascaded</p><h5 id="系统属性"><a href="#系统属性" class="headerlink" title="系统属性"></a>系统属性</h5><ul><li><p>memoryless:  $y(t_0) \quad depends \ only \ on \quad x(t_0)$</p><p>例如 $y(t) = x^2(t)$  和 $y[n] = x^2[n]$ 是memoryless</p><p>​        $y(t) = \int_{-\infty}^{t}x^2(\tau)d\tau$ 和$y[n] = x[n - 1] \quad unit \ delay$  不是memoryless</p></li><li><p>invertibility(可逆性)： given the output, there is only one input</p></li><li><p>Causality: its response at any time only depends on values of the input prior to that time</p><p>例如$y[n] = \frac{1}{3}(x[n - 1] + x[n] + x[n + 1]) $ 不是causality</p><p>​        $y[n] = \frac{1}{3}(x[n - 2] + x[n - 1] + x[n])$ 是causality</p></li><li><p>Stability： for every bounded input the output is bounded</p><p>​    feedback system的一个重要的应用是stablize unstable systems</p><p>​    例如$y(t) = \int_{-\infty}^{t}u(\tau)d\tau$  输出$y(t)$ 是ramp function， ramp is unbounded because if you try to establish any bound on it,  you can always go out far enough in time</p></li><li><p>Time invariance: 系统 if $x(t) -&gt; y(t)$ then  $x(t - t_0) -&gt; y(t - t_0)$</p><p>例如 $y(t) = sint \times x(t)$ 不是time invariance 因为当输入是$x(t-t_0)$时 输出时$sint \times x(t-t_0)$</p></li><li><p>Linearity: </p></li></ul><script type="math/tex; mode=display">if \quad x_1(t) -> y_1(t) \quad and \quad x_2(t) -> y_2(t) \\then \quad ax_t(t) + bx_2(t) -> ay_1(t) + by_2(t)</script>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Properties of Linear Time-invariant Systems</title>
      <link href="2021/03/08/signal%20and%20system%205/"/>
      <url>2021/03/08/signal%20and%20system%205/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-5-Properties-of-Linear-Time-invariant-Systems"><a href="#Lecture-5-Properties-of-Linear-Time-invariant-Systems" class="headerlink" title="Lecture 5 Properties of Linear Time-invariant Systems"></a>Lecture 5 Properties of Linear Time-invariant Systems</h3><p>这是根据公开课视频写的笔记<a href="https://www.bilibili.com/video/BV1xy4y167DD?p=5">链接</a></p><h5 id="Commutative"><a href="#Commutative" class="headerlink" title="Commutative"></a>Commutative</h5><script type="math/tex; mode=display">x[n] * h[n] = h[n] * x[n] \\x(t) * h(t) = h(t) * x(t)</script><h5 id="Associative"><a href="#Associative" class="headerlink" title="Associative"></a>Associative</h5><script type="math/tex; mode=display">x * \{h_1 * h_2\} = \{x * h_1 \} * h_2</script><h5 id="Distributive"><a href="#Distributive" class="headerlink" title="Distributive"></a>Distributive</h5><script type="math/tex; mode=display">x * \{ h_1 + h_2 \} = x * h_1 + x * h_2</script><p>视频9分40秒的例子 解释了在LTI系统中commutative性质的作用</p><p>We can interchange the role of input and impulse response, and from an output point of view, the output doesn’t care.</p><p>视频11分钟40秒的例子 解释了在LTI系统中Associative性质的作用</p><p><strong>如果我们有两个级联的LTI系统，任意方式的级联结果是一样的</strong></p><p>对于非LTI系统的级联 比如—&gt;平方根—&gt;加倍—&gt; 和 —&gt;加倍—&gt;平方根—&gt; 是不一样的</p><p>视频15分11秒的例子 解释了在LTI系统中Distributive性质的作用</p><p><strong>总结一下 这块讲的是从卷积的性质来看LTI系统互联时的性质</strong></p><h4 id="LTI系统中，其他系统属性可以关联具有系统的特定属性冲激响应"><a href="#LTI系统中，其他系统属性可以关联具有系统的特定属性冲激响应" class="headerlink" title="LTI系统中，其他系统属性可以关联具有系统的特定属性冲激响应"></a>LTI系统中，其他系统属性可以关联具有系统的特定属性冲激响应</h4><ul><li><p>Memoryless</p><p>$h(t - \tau)$ nonzero only at $\tau = t$  which implys $h(t) = K\delta(t)$ 和 $h[n] = K\delta[n]$</p><p>视频19分22秒有图 由于我们想让冲激响应contribute something after we multiply and go through an integral, and what that says is the only thing that it can be and meet all those conditions is a scaled impulse</p></li><li><p>Invertibility</p><p>视频21分钟有图 $y = x <em> (h </em> h^{-1}) = x$ 我觉得视频上的图有问题，$h$和$h^{-1}$之间应该是卷积符号</p></li><li><p>Stability</p><script type="math/tex; mode=display">\sum_{-\infty}^{+\infty}|h[k]| < \infty \\\int_{-\infty}^{+\infty}|h(\tau)|d\tau < \infty</script></li><li><p>Causality</p><p>引入概念 zero input response of a linear system(无论是否是time invariant 结论都成立)</p><p>if you put nothing into it you get nothing out of it</p><p>意思是如果$x(t) = 0$对于任意t成立， 则$y(t) = 0$对于任意t成立；如果$x[n] = 0$对于任意t成立， 则$y[n] = 0$对于任意t成立 原因是对于线性系统 如果$x(t) -&gt;y(t)$ 那么$ax(t) -&gt;ay(t)$ 令$a$为0 就可以得到这个情况</p><p><strong>causality这个性质说明的是系统无法预测输入（视频27分45秒）</strong></p><p>对于线性系统</p><p>if $x(t) = 0$ 对任意$t &lt; t_0$成立，那么$y(t) = 0$对任意$t &lt; t_0$成立， 称作initial rest</p><p>对于LTI系统</p><p>causality意味着$h(t) = 0$对任意$t&lt;0$成立，$h[n] = 0$对于任意$n &lt; 0$成立 因为相应的输入是$\delta(t)$和$\delta[n]$这两个在$t&lt;0$的时候 值均为0</p><p>视频33分8秒 示意了 如果$h(t) = 0$对任意$t &lt; 0$成立 则系统是causality的推导</p></li></ul><p>34分钟 举了一个例子</p><p>Accumulator: $y[n] = \sum_{k = -\infty}^nx[k]$， Accumulator是一个LTI系统中$h[n] = u[n]$的例子，下面分析</p><p>$\because h[n] \neq k\delta[n]$   我们可知系统是memory的</p><p>$\because h[n] = 0$对于任意n &lt; 0成立   我们可知系统是causal的</p><p>$\because \sum_{n = -\infty}^{+\infty}|h[n]| = \infty$ 我们可知系统是not stable的</p><p>38分钟关于Accumulator的求逆 没有看懂</p><p>40分钟的例子 视频里面写着 initial rest $\Rightarrow$ LTI 有点没懂</p><p>视频最后的部分讲的是</p><p>$\delta (t) * \delta (t) = \delta (t) $ 和</p><p> $ \delta[n] *\delta[n] = \delta[n]$</p><p>后者可以画图推理得到，但是前者比较tricky</p><p>因为考虑到$\delta(t) = \lim_{\Delta \to 0}\delta_{\Delta}(t)$ 如果对$\delta_{\Delta}(t) * \delta_{\Delta}(t)$得到的是一个$r_{\Delta}(t)$</p><script type="math/tex; mode=display">\begin{equation}r_{\Delta}(t) = \begin{cases}\frac{1}{\Delta^2}t& 0 < t < \Delta \\\frac{1}{\Delta} - (t - \Delta) * \frac{1}{\Delta^2}& \Delta < t < 2\Delta \\0& otherwise\end{cases}\end{equation}</script><p>那么$\delta(t) = \lim_{\Delta \to 0}r_{\Delta}(t)$</p><p>再考虑对于$\delta(t)$求导，会发现比较麻烦，老师说正确的解决方法是through a set of mathematics referred to as generalized functions</p><p><strong>当我们讨论一个函数时，我们讨论的是what the value of the function is at any instant of time，当面对冲激函数的时候就出现麻烦，那么就从另一种定义operational  definition来对待函数，即not related to what the impulse is, but to what the impulse does under the operation of convolution</strong></p><p><strong>SO WHAT IS IMPULSE?</strong></p><p>An impulse in something which under convolution retains the function !   666</p><p>意思就是impulse这个函数的定义是通过卷积来定义的，对于任意的函数，如果有一个函数和这个任意的函数卷积出来的结果不变，那么这个函数就是impulse函数</p><script type="math/tex; mode=display">x(t) * \delta(t) = x(t) \\\therefore \frac{d\delta(t)}{dt} = u_1(t) \\x(t) * u_1(t) = \frac{dx(t)}{dt} \\u_2(t) = u_1(t) * u_1(t) \\x(t) * u_2(t) = \frac{d^2x(t)}{dt}u_k(t) = u_1(t) * u_1(t) * ... \quad k \ times \\x(t) * u_k(t) = \frac{d^kx(t)}{dt} \\u_0(t) = \delta(t)</script><p>同理 对积分器引入$u_{-m}(t)$的定义 mth running integral</p><script type="math/tex; mode=display">u_k(t) * u_p(t) = u_{k + p}(t)</script>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何建站</title>
      <link href="2021/02/18/%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%99/"/>
      <url>2021/02/18/%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<h4 id="如何建站"><a href="#如何建站" class="headerlink" title="如何建站"></a>如何建站</h4><p>本站是在github上面建的静态网站，用的是hexo模板(这个资料很多)</p><p>主题是<a href="https://github.com/blinkfox/hexo-theme-matery">闪烁之狐</a></p><p>对于mathjax渲染问题，可能需要参考一下这篇博客<a href="https://www.cnblogs.com/Ai-heng/p/7282110.html">链接</a></p><p>对于发生spawn failed的错误 可以参考一下这篇博客<a href="https://1187100546.github.io/2019/11/24/spawn-failed/">链接</a></p><p>有问题欢迎留言</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
