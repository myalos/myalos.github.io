<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Deep Reinforcement Learning</title>
      <link href="2021/07/05/cs230/"/>
      <url>2021/07/05/cs230/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-9-Deep-Reinforcement-Learning"><a href="#Lecture-9-Deep-Reinforcement-Learning" class="headerlink" title="Lecture 9 Deep Reinforcement Learning"></a>Lecture 9 Deep Reinforcement Learning</h2><p><a href="https://www.bilibili.com/video/BV1p7411Y7M8?p=9">视频链接</a></p><p>cs230 先看这个lecture 为 Project2 服务</p><p><strong>deep neural networks have been shown to be very good function approximators</strong></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>How would you solve Go with classic supervised learning?</p><p>一种是输入是棋盘状态 标签是专家给出的下一步的意见  缺点是需要大量数据</p><script type="math/tex; mode=display">\#states = 3^{19\times 19} \approx 10^{170}</script><p>还有一个缺点，专家走的一步不一定是好的</p><p>介绍了3个RL应用： 机器人， 游戏， 广告</p><h3 id="Recycling-is-good-an-introduction-to-RL"><a href="#Recycling-is-good-an-introduction-to-RL" class="headerlink" title="Recycling is good: an introduction to RL"></a>Recycling is good: an introduction to RL</h3><p><img src="https://z3.ax1x.com/2021/07/04/RfW9Gq.png" alt="举了个例子"></p><p>只能走3步，如果discount是0.1 那么一开始往左走是最好的</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#todo 用Q-learning来看Q-learning 是否收敛于V*</span><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npACTIONS <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'left'</span><span class="token punctuation">,</span> <span class="token string">'right'</span><span class="token punctuation">]</span>N_STATES <span class="token operator">=</span> <span class="token number">5</span>EPSILON <span class="token operator">=</span> <span class="token number">0.9</span>ALPHA <span class="token operator">=</span> <span class="token number">0.1</span>GAMMA <span class="token operator">=</span> <span class="token number">0.9</span>points <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span>MAX_EPISODE <span class="token operator">=</span> <span class="token number">100000</span><span class="token keyword">def</span> <span class="token function">create_qtable</span><span class="token punctuation">(</span>n_states<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>n_states<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> columns <span class="token operator">=</span> ACTIONS<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">choose_action</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> qtable<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> state <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string">'right'</span>    <span class="token keyword">if</span> state <span class="token operator">==</span> <span class="token number">5</span><span class="token punctuation">:</span><span class="token comment">#这里改为 == 4</span>        <span class="token keyword">return</span> <span class="token string">'left'</span>    actions <span class="token operator">=</span> qtable<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>state<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> EPSILON <span class="token keyword">or</span> <span class="token punctuation">(</span>actions<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        action <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>ACTIONS<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        action <span class="token operator">=</span> actions<span class="token punctuation">.</span>idxmax<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> action<span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> action <span class="token operator">==</span> <span class="token string">'left'</span><span class="token punctuation">:</span>        next_state <span class="token operator">=</span> state <span class="token operator">-</span> <span class="token number">1</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        next_state <span class="token operator">=</span> state <span class="token operator">+</span> <span class="token number">1</span>    reward <span class="token operator">=</span> points<span class="token punctuation">[</span>next_state<span class="token punctuation">]</span>    <span class="token keyword">return</span> next_state<span class="token punctuation">,</span> reward<span class="token keyword">def</span> <span class="token function">rl</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    table <span class="token operator">=</span> create_qtable<span class="token punctuation">(</span>N_STATES<span class="token punctuation">)</span>    <span class="token keyword">for</span> episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>MAX_EPISODE<span class="token punctuation">)</span><span class="token punctuation">:</span>        S<span class="token punctuation">,</span> step_cnt <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span>        <span class="token keyword">while</span> step_cnt <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">:</span>            action <span class="token operator">=</span> choose_action<span class="token punctuation">(</span>S<span class="token punctuation">,</span> table<span class="token punctuation">)</span>            next_state<span class="token punctuation">,</span> reward <span class="token operator">=</span> step<span class="token punctuation">(</span>S<span class="token punctuation">,</span> action<span class="token punctuation">)</span>            q_predict <span class="token operator">=</span> table<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>S<span class="token punctuation">,</span> action<span class="token punctuation">]</span>            <span class="token comment">#这里加上if next_state == 4 or 0 就怎么怎么样</span>            q_target <span class="token operator">=</span> reward <span class="token operator">+</span> GAMMA <span class="token operator">*</span> table<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>S<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment">#这个地方应改为q_target = reward + GAMMA * table.iloc[next_state, :].max()</span>            table<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>S<span class="token punctuation">,</span> action<span class="token punctuation">]</span> <span class="token operator">+=</span> ALPHA <span class="token operator">*</span> <span class="token punctuation">(</span>q_target <span class="token operator">-</span> q_predict<span class="token punctuation">)</span>            step_cnt <span class="token operator">+=</span> <span class="token number">1</span>            S <span class="token operator">=</span> next_state        <span class="token keyword">print</span><span class="token punctuation">(</span>table<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    rl<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面这个代码EPISODE 100000</p><p>运行完后发现Q表收敛了</p><div class="table-container"><table><thead><tr><th></th><th>left</th><th>right</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>20</td><td>18</td></tr><tr><td>2</td><td>9</td><td>10</td></tr><tr><td>3</td><td>90</td><td>100</td></tr><tr><td>4</td><td>0</td><td>0</td></tr></tbody></table></div><p>对于这个表 0，right为0的原因是 算法有问题，<span style='color:red;font-weight:bold'>p现实的Q是目标状态的内容，而非原来状态的</span></p><p>改了之后 运行完的Q表内容为<br>|      | left | right |<br>| —— | —— | ——- |<br>| 0    | 0    | 9.473684     |<br>| 1    | 10.526316   | 9.000000    |<br>| 2    | 9.473684    | 10.000000    |<br>| 3    | 9.000000   | 10.000000  |<br>| 4    | 0    | 0     |</p><p>4 left 不应该为0， 发现代码又一处有问题choose_action里面 state == 5 没有这个state，应该改为state == 4</p><p>但是4 left 还是为0 原因是1走3步走到4 程序就停止了 没有机会再走了。 不过这个Q表不对啊</p><p>按Q表的话 最佳路线是左 右 左 总收益是 2 + 0 + 0.81* 2</p><p>但是存在右 右 右的路线 总收益是 0 + 0.9 + 8.1</p><p><span style="color:red;font-weight">我这个算法没有终止状态，这样就会出问题</span></p><p>如果每个episode开始S的取值为np.random.randint(0, 4) 4, left有值了 但是取值并不是我们想要的</p><p>如果设置终止状态在state == 4 Q表还是那个Q表，如果设置终止状态为0 或者 4 Q表就变为</p><div class="table-container"><table><thead><tr><th></th><th>left</th><th>right</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>2</td><td>9</td></tr><tr><td>2</td><td>8.1</td><td>10</td></tr><tr><td>3</td><td>9.0</td><td>10</td></tr><tr><td>4</td><td>0</td><td>0</td></tr></tbody></table></div><p>还是有个地方不对  2，left 为8.1 就不对，这个地方向左走 最多能获得的收益是1.8</p><p><span style="color:red;font-weight:bold">当有限步数的时候Q表会有问题，按Q表走可能不会是最优选择</span></p><p><strong>Bellman equation satisfied by the optimal Q-table</strong></p><h3 id="Deep-Q-Learning"><a href="#Deep-Q-Learning" class="headerlink" title="Deep Q-Learning"></a>Deep Q-Learning</h3><p>将Q table转变为Q function 就是从Q-Learning变为Deep Q-Learning</p><p><img src="https://z3.ax1x.com/2021/07/04/Rf5n0O.png" alt="用神经网络来做function approximator"></p><p>可以将状态和行为整体作为输入，也可以更快，<strong>将状态作为输入</strong></p><p>与传统的监督学习不同的是，这个没有标签。我们应该把这个视为回归问题，因为Q score doesn’t have to be a probability between zero and one. 假如有标签$y$存在 那么</p><script type="math/tex; mode=display">L = (y - Q(s, \leftarrow))^2</script><p>那么y是什么，注意Bellman equation，我们知道最优的Q值应该满足Bellman equation</p><p>有个问题，Bellman equation取决于自身的$Q$，即等式两边都有$Q$，这意味着如果将标签设置为</p><p>$r + \gamma \times \max_{a^\prime} (Q^\prime (s^\prime, a^\prime))$ 那么在BP阶段 将在这里有一个导数</p><p>我们定义一个目标值 target value 假设是$Q(s, \leftarrow) &gt; Q(s, \rightarrow)$</p><p>随机初始化网络，然后前向传播，然后左侧Q得分大于右侧Q得分，我们的行为就是向左走</p><p>我们定义y为</p><script type="math/tex; mode=display">y = r_{\leftarrow} + \gamma \max_{a^\prime}(Q(s_{\leftarrow}^{next}, a^\prime))</script><p>It is a proxy to a good label. We hope that if we use this proxy as our label, and we learn the difference between where we are now in this proxy, we can then update the proxy get closer to the optimality. 与deep learning不同的是，<strong>label在moving！！！</strong></p><p>我们将标签定义为对我们拥有的最佳Q函数的best guess</p><p>关键问题这个网络是否会收敛到最佳Q函数呢： 有一篇论文 Convergence of Q-Learning: a simple proof</p><p><strong>还有一个问题</strong>，如果将y放到loss function中，然后要求BP，W更新了，会得到一个feedback loop使得网络不稳定，这个feedback loop没有弄明白</p><p>解决这个问题的方法是 我们的目标Q将在许多迭代中得到固定 例如100万或10万次迭代</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#试一下 用DQN来解recycling is good</span><span class="token keyword">import</span> torch<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable<span class="token keyword">class</span> <span class="token class-name">DQN</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_state<span class="token punctuation">,</span> n_action<span class="token punctuation">,</span> n_hidden<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>criterion <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_state<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> n_action<span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> s<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        y_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>criterion<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span>        STATES <span class="token operator">=</span> <span class="token number">5</span>GAMMA <span class="token operator">=</span> <span class="token number">0.9</span>ALPHA <span class="token operator">=</span> <span class="token number">0.1</span>EPSILON <span class="token operator">=</span> <span class="token number">0.9</span>POINTS <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span>MAX_EPISODE <span class="token operator">=</span> <span class="token number">100000</span>MOD <span class="token operator">=</span> <span class="token number">5000</span><span class="token keyword">def</span> <span class="token function">choose_action</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> dqn<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> EPSILON<span class="token punctuation">:</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        q_values <span class="token operator">=</span> dqn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>state<span class="token punctuation">)</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>q_values<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">:</span>    ind <span class="token operator">=</span> state<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>    state<span class="token punctuation">[</span>ind<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    ind <span class="token operator">+=</span> <span class="token number">1</span> <span class="token keyword">if</span> action <span class="token keyword">else</span> <span class="token operator">-</span><span class="token number">1</span>    state<span class="token punctuation">[</span>ind<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>    <span class="token keyword">return</span> state<span class="token punctuation">,</span> POINTS<span class="token punctuation">[</span>ind<span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">rl</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    dqn <span class="token operator">=</span> DQN<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>MAX_EPISODE<span class="token punctuation">)</span><span class="token punctuation">:</span>        state<span class="token punctuation">,</span> cnt <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span>        <span class="token keyword">while</span> cnt <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">:</span>            action <span class="token operator">=</span> choose_action<span class="token punctuation">(</span>state<span class="token punctuation">,</span> dqn<span class="token punctuation">)</span>            nstate<span class="token punctuation">,</span> reward <span class="token operator">=</span> step<span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span>            q_predict <span class="token operator">=</span> dqn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>            q_target <span class="token operator">=</span> dqn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>nstate<span class="token punctuation">)</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>episode <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> MOD <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token keyword">print</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> nstate<span class="token punctuation">,</span> reward<span class="token punctuation">)</span>            <span class="token keyword">if</span> nstate<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> nstate<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>                cnt <span class="token operator">=</span> <span class="token number">2</span>                q_predict<span class="token punctuation">[</span>action<span class="token punctuation">]</span> <span class="token operator">=</span> reward            <span class="token keyword">else</span><span class="token punctuation">:</span>                q_predict<span class="token punctuation">[</span>action<span class="token punctuation">]</span> <span class="token operator">=</span> reward <span class="token operator">+</span> GAMMA<span class="token operator">*</span>torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>q_target<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            dqn<span class="token punctuation">.</span>update<span class="token punctuation">(</span>state<span class="token punctuation">,</span> q_predict<span class="token punctuation">)</span>            state <span class="token operator">=</span> nstate            cnt <span class="token operator">+=</span> <span class="token number">1</span>                    <span class="token keyword">if</span> <span class="token punctuation">(</span>episode <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> MOD <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                <span class="token builtin">input</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> dqn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token builtin">input</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"-"</span> <span class="token operator">*</span> <span class="token number">70</span><span class="token punctuation">)</span>rl<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>用神经网络来代替Q表 这个网络不是很稳定 运行多次 最后生成的Q表差的有点大</p><p>举例：</p><p><img src="https://z3.ax1x.com/2021/07/05/R5r8r4.png" alt="收敛情况1"></p><p><img src="https://z3.ax1x.com/2021/07/05/R5r3MF.png" alt="收敛情况2"></p><p><img src="https://z3.ax1x.com/2021/07/05/R5rlxU.png" alt="收敛情况3"></p><p>存在两个问题：</p><p>Q1 当在中间这个格子的时候 居然是要向左走   Q2 一直向左走</p><p>比较这个版本和之前的版本 <strong>我觉得Q2的原因是可能代码中缺少actions.all() == 0</strong></p><p>发现之前的代码 因为只会走3步 那么actions.all() == 0 永远为true，也就是random walk，如果去掉这个 走三步一下就会收敛了</p><p><span style="color:red;font-weight:bold;font-size:20px">debug发现，np.random.randint(0, 1)只会生成数字0</span></p><h3 id="新的代码"><a href="#新的代码" class="headerlink" title="新的代码"></a>新的代码</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable<span class="token keyword">class</span> <span class="token class-name">DQN</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_state<span class="token punctuation">,</span> n_action<span class="token punctuation">,</span> n_hidden<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>criterion <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_state<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> n_action<span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> s<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        y_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>criterion<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span>STATES <span class="token operator">=</span> <span class="token number">5</span>GAMMA <span class="token operator">=</span> <span class="token number">0.9</span>ALPHA <span class="token operator">=</span> <span class="token number">0.1</span>EPSILON <span class="token operator">=</span> <span class="token number">0.9</span>POINTS <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span>MAX_EPISODE <span class="token operator">=</span> <span class="token number">100000</span>MOD <span class="token operator">=</span> <span class="token number">5000</span><span class="token keyword">def</span> <span class="token function">choose_action</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> dqn<span class="token punctuation">)</span><span class="token punctuation">:</span>    ind <span class="token operator">=</span> state<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> EPSILON <span class="token keyword">or</span> <span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        q_values <span class="token operator">=</span> dqn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>state<span class="token punctuation">)</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>q_values<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">:</span>    ind <span class="token operator">=</span> state<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>    nstate <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    ind <span class="token operator">+=</span> <span class="token number">1</span> <span class="token keyword">if</span> action <span class="token keyword">else</span> <span class="token operator">-</span><span class="token number">1</span>    nstate<span class="token punctuation">[</span>ind<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>    <span class="token keyword">return</span> nstate<span class="token punctuation">,</span> POINTS<span class="token punctuation">[</span>ind<span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">rl</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    dqn <span class="token operator">=</span> DQN<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>MAX_EPISODE<span class="token punctuation">)</span><span class="token punctuation">:</span>        state<span class="token punctuation">,</span> cnt <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span>        <span class="token keyword">while</span> cnt <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">:</span>            action <span class="token operator">=</span> choose_action<span class="token punctuation">(</span>state<span class="token punctuation">,</span> dqn<span class="token punctuation">)</span>            nstate<span class="token punctuation">,</span> reward <span class="token operator">=</span> step<span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span>            q_predict <span class="token operator">=</span> dqn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>            q_target <span class="token operator">=</span> dqn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>nstate<span class="token punctuation">)</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>episode <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> MOD <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token keyword">print</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> nstate<span class="token punctuation">,</span> reward<span class="token punctuation">)</span>            <span class="token keyword">if</span> nstate<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> nstate<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>                cnt <span class="token operator">=</span> <span class="token number">2</span>                q_predict<span class="token punctuation">[</span>action<span class="token punctuation">]</span> <span class="token operator">=</span> reward            <span class="token keyword">else</span><span class="token punctuation">:</span>                q_predict<span class="token punctuation">[</span>action<span class="token punctuation">]</span> <span class="token operator">=</span> reward <span class="token operator">+</span> GAMMA<span class="token operator">*</span>torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>q_target<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            dqn<span class="token punctuation">.</span>update<span class="token punctuation">(</span>state<span class="token punctuation">,</span> q_predict<span class="token punctuation">)</span>            state <span class="token operator">=</span> nstate            cnt <span class="token operator">+=</span> <span class="token number">1</span>                    <span class="token keyword">if</span> <span class="token punctuation">(</span>episode <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> MOD <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                <span class="token builtin">input</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> dqn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token builtin">input</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"-"</span> <span class="token operator">*</span> <span class="token number">70</span><span class="token punctuation">)</span>rl<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不像单纯的Q-Learning 收敛稳定，这个会在一定范围内一直在波动</p><p>再没有出现之前 非常奇怪的收敛点了，15000episode之后 就会收敛到下面这张图附近</p><p><img src="https://z3.ax1x.com/2021/07/05/R5fsL6.png" alt="R5fsL6.png"></p><h3 id="Deep-Q-Learning-application-Breakout-Atari"><a href="#Deep-Q-Learning-application-Breakout-Atari" class="headerlink" title="Deep Q-Learning application: Breakout Atari"></a>Deep Q-Learning application: Breakout Atari</h3><p>input 图像 输出 游戏操作  存在一个问题 不知道球是在向上运动还是向下运动，解决方法 用多张图片。给定状态$S$ 计算$S$的函数$\phi(S)$ 这个give you the history of this state which is the four-sequence of four last frames.</p><p><img src="https://z3.ax1x.com/2021/07/04/RfbogH.png" alt="DQN算法"></p><p>与监督学习不同的是， 我们只对我们探索过的内容进行训练。这意味着当我从状态$S$开始，我在网络中前向传播了这个$\phi (S)$ 我得到我的Q值向量，我选择最佳的Q值，我得到一个新的状态$S^\prime$.</p><p>在监督学习中 同样的数据会在不同的epoch中使用 也就是会重复使用，而在DQN中不是这样，因为我们仅在探索时进行训练 我们可能永远不会回到那里。尤其时因为训练会受到我们去过位置的影响，这段是我自己人工翻译的， 感觉不是很通顺 这里我也弄得不是很明白</p><p>意思是we will never train on some parts of the game, so we need other techniques to keep this training stable. 那就是Experience replay</p><p><img src="https://z3.ax1x.com/2021/07/04/RfLkyd.png" alt="常规训练方法和加了experience replay的训练方法"></p><p>用experience replay的一个重大优势是破除连续数据之间的相关性</p><p>换个说法：当在用一堆猫狗图片来训练一个猫狗分类器，如果先在猫的图像上面训练，然后再在狗的图像上面训练，然后在猫，然后在狗，我们不会converge 因为网络会super biased towards predicting cat after seeing 10 images of cat. Super biased with predicting dogs when it sees 10 images of dog.</p><p>另一个优势是： you are basically trading computation and memory against exploration. Exploration is super costly. The state-space might be super big, but you know you have enough computation probably</p><h3 id="Advanced-topics"><a href="#Advanced-topics" class="headerlink" title="Advanced topics"></a>Advanced topics</h3><p>没听懂</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> cs230 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Q-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MDPs ValuePolicy Iteration</title>
      <link href="2021/07/03/cs229%2017/"/>
      <url>2021/07/03/cs229%2017/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-17-MDPs-ValuePolicy-Iteration"><a href="#Lecture-17-MDPs-ValuePolicy-Iteration" class="headerlink" title="Lecture 17 MDPs ValuePolicy Iteration"></a>Lecture 17 MDPs ValuePolicy Iteration</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=23">视频链接</a></p><p>start to talk about algorithms for solving MDPs</p><p>We need to define sth. called <strong>the value function</strong> which tells you how good it is to be in different states of the MDP and then we’ll define the value function and then talk about an algorithm called value iteration for computing the value function.</p><p>回忆lecture16的内容，为了找到最佳的policy，可能有成千上万的可选择的policies，如果有11个状态，每个状态可采取4个行动，那么可能的策略就有$4^{11}$种</p><h3 id="计算最佳策略"><a href="#计算最佳策略" class="headerlink" title="计算最佳策略"></a>计算最佳策略</h3><p>Define: $V^{\pi}$，$V^<em>$，$\pi^</em>$ ;  based on these definitions we’ll derive that $\pi^*$ is the optimal policy</p><p><span style='color:red;font-weight:bold'>For a policy </span>$\pi$, $V^\pi : S \rightarrow R $<span style='color:red;font-weight:bold'> is s.t.</span> $V^\pi (s)$ <span style='color:red;font-weight:bold'>is to the expected total payoff for starting in state s and executing</span> $\pi$</p><script type="math/tex; mode=display">V^\pi (s) = E[R(s_0) + \gamma R(s_1) + \cdots | \pi, s_0 = s]</script><p>$V^\pi$ 被称为value function for policy $\pi$</p><p><img src="https://z3.ax1x.com/2021/07/01/RyQhx1.png" alt="下面的图是某一个策略pi"></p><p><img src="https://z3.ax1x.com/2021/07/01/Ryl0Fe.png" alt="上面的策略对应的value function"></p><h3 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h3><script type="math/tex; mode=display">V^\pi (s) = R(s) + \gamma \sum_{s^\prime}P_{s\pi (s)}(s^\prime)V^\pi (s^\prime) \tag{1}</script><p>假如在$s=s_0$的时候开始，机器人马上获取在$s_0$的收益 即即使奖励$R(s_0)$ </p><script type="math/tex; mode=display">V^\pi (s) = E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots | \pi, s_0 = s] \\= E[R(s_0) + \gamma (R(s_1) + \gamma R(s_2) + \cdots) | \pi, s_0 = s] \\= E[R(s_0) + \gamma V^\pi(s_1) | \pi, s_0 = s]</script><p>方程式$(1)$ 的意义在于Given $\pi$, get a linear system of equation in terms of $V^\pi (s)$</p><p>举个例子 之前机器人的例子：</p><script type="math/tex; mode=display">V((3, 1)) = R((3, 1)) + \gamma(0.8V^\pi((3, 2)) + 0.1V^\pi((2, 1)) + 0.1V^\pi((4, 1)))</script><p>因为有11个状态 那么就会有11个未知数，和 11个Bellman equations，解线性方程得结果</p><p><img src="https://z3.ax1x.com/2021/07/02/RyyvNQ.png" alt="11个状态组成得线性方程组"></p><p>$V^<em>$ is the optimal value function   $V^</em> (s) = \max_\pi V^\pi (s)$</p><p>下面是关于$V^*$的bellman 方程</p><script type="math/tex; mode=display">V^* (s) = R(s) + \max_a \gamma \sum_{s^\prime}P_{sa} (s^\prime)V^*(s^\prime) \tag{2}</script><p>if take action $a$ 那么 $\sum_{s^\prime}P_{sa} (s^\prime)V^*(s^\prime)$ 就是expected future reward</p><p>由方程$(2)$ 就可以推出$\pi^* (s)$ 即</p><script type="math/tex; mode=display">\pi^* (s) = argmax_a \sum_{s^\prime}P_{sa}(s^\prime)V^*(s^\prime)</script><p>作个小总结 $V^<em> (s) = V^{\pi^</em>}(s) \ge V^\pi (s) \ for \ every \ \pi, s$ </p><p>我们要做的就是找到$V^<em>$， 使用argmax方程来求得$\pi^</em>$</p><h3 id="Value-iteration"><a href="#Value-iteration" class="headerlink" title="Value iteration"></a>Value iteration</h3><p> 是一个算法来找到$V^*$ </p><ul><li><p>初始化$V(s) := 0$  for every s</p></li><li><p>For every s, repeatedly update:</p><script type="math/tex; mode=display">V(s) := R(s) + \max_a \gamma \sum_{s^\prime}P_{sa}(s^\prime)V(s^\prime)</script><p>类似于梯度下降 就是update所有components of V simultaneously，有synchronous update</p><p>就是计算所有状态然后进行同时覆盖。有一种替代方法是异步更新 就是一个一个更新</p><p>一般用synchronous update</p></li></ul><p>用value iteration 计算出的$V^*$ 结果是：</p><p><img src="https://z3.ax1x.com/2021/07/02/R6CEZt.png" alt="R6CEZt.png"></p><p>由这张图 知道处于$(3, 1)$的时候应该向左走</p><h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><p>这个用来求解MDP问题</p><ul><li><p>Initialize $\pi$ randomly</p></li><li><p>Repeat until convergence:</p><p>​    Set $V := V^\pi$ i.e. solve Bellman’s equations to get $V^\pi$ </p><p>​    Set $\pi (s) := argmax_a \sum_{s^\prime}P_{sa}(s^\prime)V(s^\prime)$  假设$V$就是optimal value function</p><p>注意$V^\pi$ 是解linear system of equations</p></li></ul><p>对于状态很多的情况，解线性方程就很耗时，用value iteration 会更好一点</p><p>对于value iteration $V \rightarrow V^<em>$ <em>*就将梯度下降，向最优值收敛是越来越靠近最优值，但是永远达不到最优值</em></em></p><h3 id="如果不知道-P-sa"><a href="#如果不知道-P-sa" class="headerlink" title="如果不知道$P_{sa}$"></a>如果不知道$P_{sa}$</h3><p>当state transition probabilities are often not known in advance. And so in many MDP implementations we need to estimate this form data.</p><script type="math/tex; mode=display">P_{sa}(s^\prime) = \frac{\# \ times \ took \ action \ "a" \ in \ state \ s \ and \ got \ to \ s^\prime}{\# \ times \ took \ action \ "a" \ in \ state \ s}</script><p>or $\frac{1}{|S|}$  if above is $\frac{“0”}{0}$ </p><p>总结一下：</p><ul><li><p>Repeat {</p><p>​    Take actions w.r.t. $\pi$ to get experience in MDP</p><p>​    Update estimates of $P_{sa}$ and possibly $R$</p><p>​    Solve Bellman’s equation using value iteration to get $V$</p><p>​    Update $\pi (s) = argmax_a \sum_{s^\prime}P_{sa}(s^\prime)V(s^\prime)$</p><p>}</p></li></ul><p>上面这个算法没有exploration</p><p>如果加上exploration的话 那么就不是take actions $\pi$ 而是 0.9 take actions $\pi$ 0.1 take randomly</p><p>这种exploration policy is called $\epsilon$-greedy；在每个时间步长你仍一枚biased coin;</p><p>这里的$\epsilon$ 就是0.1</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>message 1</title>
      <link href="2021/07/03/zjj1/"/>
      <url>2021/07/03/zjj1/</url>
      
        <content type="html"><![CDATA[<h2 id="Z-大仙女-能加我好友-能和我说一句话吗"><a href="#Z-大仙女-能加我好友-能和我说一句话吗" class="headerlink" title="Z 大仙女 能加我好友 能和我说一句话吗"></a>Z 大仙女 能加我好友 能和我说一句话吗</h2><p>你不和我说话 一切都在原点啊</p>]]></content>
      
      
      
        <tags>
            
            <tag> S.P. </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PCA and ICA</title>
      <link href="2021/07/01/cs229%20pca/"/>
      <url>2021/07/01/cs229%20pca/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-hidden-PCA-and-ICA"><a href="#Lecture-hidden-PCA-and-ICA" class="headerlink" title="Lecture hidden PCA and ICA"></a>Lecture hidden PCA and ICA</h3><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=21">视频链接</a> 因为看Lecture 16的时候 感觉跳了一集，所以找到了这里</p><p><strong>PCA ： Principal Components Analysis</strong></p><p><strong>ICA ：Independent Components Analysis</strong></p><h3 id="motivation-example"><a href="#motivation-example" class="headerlink" title="motivation example:"></a>motivation example:</h3><p>Want measure children’s height in centimeters and also the height in inches</p><p>Because of <strong>round off</strong> to the nearest centimeter or round off to the nearest inch</p><p><strong>The data is almost prefectly correlated but not completely</strong> [7:21]的图</p><p>你知道这其实是一个一维的数据，但是以2维的形式存在；</p><p>另一个例子是 横坐标是pilot skill 纵坐标是pilot enjoyment，从这个2维数据里面得到pilot aptitude</p><p>PCA 就是将高维数据转化成低维数据；实际上 会用PCA 将10000维数据变成100维数据 而非2变1</p><p>在running PCA之前 要做Pre-processing</p><ul><li>$\mu = \frac{1}{m}\sum_{i = 1}^mx^{(i)}$</li><li>$x^{(i)} \leftarrow x^{(i)} - \mu$</li><li><p>$\sigma^2 = \frac{1}{m} \sum_i x_j^{(i)}$</p></li><li><p>$x_j^{(i)} \leftarrow \frac{x_j^{(i)}}{\sigma_j}$</p></li></ul><p>做完之后 features have zero mean， one standardize variance</p><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>[14:44] 二维的数据 进行1维压缩，有好的subspace 绿色的线， 也有差的subspace 红色的线，对于好的projection， <strong>所有点到subspace的和的总数距离小</strong>，对于PCA 我们就是想要找到绿色的线</p><p>图中绿色的点are spread apart very far from each other，红色的点are close to each other，So if you like to preserve as much of the variability of the data as possible</p><p>对于PCA有两种数学上等价的推导方式 老师介绍的是第二种</p><ul><li>一种是所有点到subspace的距离之和最小</li><li>另一种是所有点到subspace的投影长度之和最大</li></ul><p>A second intuition of PCA : Define PCA is you want ot find a line onto which project the data 使数据尽可能的spread as far as possible</p><p>要找到绿线 即要找到一个和绿线同方向的单位向量$u$  这里绿线就表示subspace</p><p>If $||u|| = 1$, then length of projection of $x^{(i)}$ onto $u$ is $u^Tx^{(i)}$ </p><p>So PCA choose $u$ to maximize:</p><script type="math/tex; mode=display">\max_{u : ||u || = 1} \frac{1}{m}\sum_{i = 1}^m (x^{(i)^T}u)^2 = \frac{1}{m}\sum_{i = 1}^m u^Tx^{(i)}x^{(i)^T}u = u^T(\frac{1}{m}\sum_{i = 1}^mx^{(i)}x^{(i)^T})u</script><p>令$\Sigma = \frac{1}{m}\sum_{i=1}^mx^{(i)}x^{(i)^T}$</p><p>那么就是$\max_{u : ||u|| = 1}u^T\Sigma u$  <strong>那么u就是principal eigenvector of matrix $\Sigma$</strong></p><p>这里的$\Sigma$是covariance matrix，我们做了preprocessing之后的$x^{(i)}$构成的矩阵</p><p>下面是关于上面加粗内容的证明：</p><script type="math/tex; mode=display">\max_{u : ||u|| = 1}u^T\Sigma u \ 构造拉格朗日函数 \\\mathcal{L}(u,\lambda) = u^T\Sigma u - \lambda(u^Tu - 1) \\ \nabla\mathcal{L} = \Sigma u - \lambda u  = 0 \Rightarrow \Sigma u = \lambda u</script><p>General case : If wish to project data to k dimensions</p><p>Set $u_1, u_2, \cdots, u_k$ to be top k eigenvectors of $\Sigma$，$\lambda_1, \lambda_2, \cdots, \lambda_k$ are corresponding eigenvalues</p><p>Have $x^{(i)} \in R^n$ $n = 1000$， 要用PCA降维到10维 $u_1, u_2, \cdots, u_k$ 其中$k = 10$</p><p>我们就有了New Representation $x^{(i)} \rightarrow (u_1^Tx^{(i)}, u_2^Tx^{(i)}, \cdots, u_{10}^Tx^{(i)})  = y^{(i)} \in R^k$</p><p>从$y$得到$x$ $x^{(i)} \approx y_1^{(i)}u_1 + y_2^{(i)}u_2 + \cdots + y_k^{(i)}u_k$ 其中$y_i$是标量</p><p><strong>666</strong></p><p>[33:00]的图 展示了<strong>为什么PCA需要 preprocessing</strong></p><h3 id="Application-of-PCA"><a href="#Application-of-PCA" class="headerlink" title="Application of PCA:"></a>Application of PCA:</h3><ul><li><p>Visualization</p><p>Project from n dimensional to 2 dimensional or 3 dimensional to view</p><p>举了个例子，猴子脑电信号50维 很难visualize</p></li><li><p>Compression for ML efficiency</p></li><li><p>用PCA来reduce overfitting 不是很合适，用正则化更好</p></li><li><p>用PCA来做outlier detection不是很合适</p></li></ul><p>总结一下： 基本上PCA 就是用在可视化</p><p>如果有训练集和测试集，eigenvectors 用训练集的</p><div class="table-container"><table><thead><tr><th>—</th><th>model $p(x)$ e.g. anomaly detection</th><th>Non-probabilistic</th></tr></thead><tbody><tr><td>“Subspaces”</td><td>Factor analysis</td><td>PCA</td></tr><tr><td>“Clusters”</td><td>GMM</td><td>K-means</td></tr></tbody></table></div><p>PS 对于PCA k dimension subspace 比较稳定， 单个eigenvector不怎么稳定 由于数值计算等原因</p><h3 id="如何选择k值"><a href="#如何选择k值" class="headerlink" title="如何选择k值"></a>如何选择k值</h3><p>有一种选择是：$\frac{\lambda_1 + \lambda_2 + \cdots + \lambda_k}{\lambda_1 + \lambda_2 + \cdots + \lambda_n} = 0.9$  人们称这个retain $90\%$ of variance of data</p><h3 id="ICA"><a href="#ICA" class="headerlink" title="ICA"></a>ICA</h3><p>ICA 可以用来解决Cocktail party problem 有点厉害</p><p>问题可以描述为：</p><p>Original source $s \in R$ (n speakers)  $s_j^{(i)} = $ signal from speaker $j$ at time $i$</p><p>在某个时刻$t$ 对于第一个speaker 振幅为 $s_1^{(t)}$ 对于第二speaker 振幅为$s_2^{(t)}$</p><p>我们观察到 $x^{(i)} = As^{(i)}$  其中$x^{(i)} \in R^n$ n microphones</p><p>$x_j^{(i)}$ is recording of microphone $j$ at time $i$  其中$j = 1, \cdots, n$</p><p>即 $x_j^{(i)} = \sum_kA_{jk}s_k^{(i)}$  目标是找到$W = A^{-1}$ so that $s^{(i)} = Wx^{(i)}$</p><p>Notation:</p><script type="math/tex; mode=display">W = \begin{pmatrix} w_1^T \\ w_2^T  \\ \vdots \\  w_n^T  \\\end{pmatrix}</script><p>则 $s_j^{(i)} = w_j^Tx^{(i)}$</p><p>老师讲了个二个 independent source 1 和 source 2 每个source产生random的数据</p><p>对于这个数据有两个ambiguity 一个是不知道which one is S1 and which one is S2</p><p>第二个是 sign ambiguity 我们不知道哪个是 + S1 哪个是 - S1</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Independent Component Analysis and RL</title>
      <link href="2021/07/01/cs229%2016/"/>
      <url>2021/07/01/cs229%2016/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-16-Independent-Component-Analysis-and-RL"><a href="#Lecture-16-Independent-Component-Analysis-and-RL" class="headerlink" title="Lecture 16 Independent Component Analysis and RL"></a>Lecture 16 Independent Component Analysis and RL</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=22">视频链接</a></p><h3 id="ICA"><a href="#ICA" class="headerlink" title="ICA"></a>ICA</h3><p>感觉这个有点神奇，不是很懂</p><ul><li>CDFs Cumulative distribution function</li><li>ICA model</li></ul><p>Recap: $S_1^{(t)}$ is the sound emitted by speaker one at time t</p><p>$S_j^{(i)} =  speaker \  j \ at \  time \  i \ ,  \  x^{(i)} = As^{(i)}, x\in\mathcal{R}^n$</p><p>Goal:  Find $W = A^{-1}, S^{(i)} = Wx^{(i)}$ $W = \begin{pmatrix} -w_1^T-  \\  \vdots \\ -w_n^T -\end{pmatrix}$</p><hr><p>PS：如果数据是均匀分布，ICA能分离，如果是高斯分布 ICA不能分离，由两个标准高斯分布组成的二维空间，S1和S2是旋转对称的，就会有旋转的歧义</p><p><strong>ICA  is possible only if  your data is non  Gaussian</strong></p><hr><p>为了使用ICA ，We need to figure out what is the density of S</p><p>我们知道S的概率密度 $P_S(s)$ 因为$x = As = W^{-1}s$ 我们就知道 $s = Wx$</p><p>我们需要计算$x$的概率密度，因为当你得到训练集的时候，只能观察$x$，用到MLE，必须要知道$x$的概率密度是多少</p><p>猜想 $P_X(x) = P_S(Wx)$是否是正确的，  这个对于离散的随机变量是正确的，对于连续的就不正确</p><p>反例：</p><script type="math/tex; mode=display">P_S(s) = 1\{  0  \le s \le 1 \} \quad \quad S \sim  Uniform(0, 1) \\x = 2s \quad  \quad A = 2 \quad  \quad  W = \frac{1}{2}</script><p>那么 x  就应该是0到2上的均匀分布 其概率密度应该为$P_X(x) = \frac{1}{2}1 \{  0 \le x \le 2 \}$</p><p>把$P_X(x)$和$P_S(Wx)$分别带入到等式，发现等式不成立，正确的式子应该是：</p><script type="math/tex; mode=display">P_X(x) = P_S(wx)|w|</script><p>还有一点重要 choose the density of what your speakers’ voices sound like</p><p>如果选择sigmoid函数 作为CDF 其概率密度 有点像 高斯，但是他有fatter tails 因为高斯函数是$e^{-x^2}$衰减的 goes to 0 very quickly, sigmoid的概率密度captures human voice  and many natural phenomena better than a Gaussian density because there are a larger number of extreme outliers that are more than one or two standard deviations away.</p><p>除了 sigmoid还可以有其它的选择  比如 双重指数分布</p><p><strong>因为n个speaker  speak  independently</strong></p><script type="math/tex; mode=display">P(s) = \prod_{i = 1}^n P_S(s_i)</script><script type="math/tex; mode=display">P_X(x) = P_S(wx)|w| = (\prod_{j = 1}^n P_S(w_j^Tx))|w|</script><p><strong>MLE</strong></p><script type="math/tex; mode=display">\ell(w) = \sum_{i = 1}^nlog[(\sum_jP_S(w_j^Tx^{(i)}))|w|]</script><p>求导结果参考讲义</p><p><strong>ICA通常用于清理EEG数据</strong></p><p>如果麦克风的数量少于speaker 那么这是个比较难的问题</p><p><strong>如果有两个麦克风，那么这两个麦克风记录的声音会不会很相似？？ </strong></p><h3 id="RL-666哇"><a href="#RL-666哇" class="headerlink" title="RL 666哇"></a>RL 666哇</h3><p>RL的一个挑战问题是 Credit assignment problem</p><p>play chess 如果是在第50步失败 得到奖励-1</p><p><strong>MDP</strong> Markov decision process</p><p><span style='color:red;font-weight:bold'>下面的内容就是说的如何将问题化为MDP的形式 </span></p><p>MDP是five tuple $(S, A, \{ P_{sa}\},\gamma,R)$</p><p>$S$ 是 set of states; $A$ 是 set of actions; $P_{sa}$ 是 state transition probabilities $\sum_s^\prime P_{sa}(s^\prime) = 1$</p><p>$\gamma$ 是discount factor 取值在0到1之间  $R$ 是 reward function</p><p><img src="https://z3.ax1x.com/2021/07/01/RyCic9.png" alt="RyCic9.png"></p><p><strong>上面这张图是我博客里面的第二张图，这张图是放在图床上面的</strong></p><p>这是一个非常简单的迷宫，这个MDP有11个state 机器人可能处在11个位置</p><p>对于Actions 可能是向北，向东，向南，向西</p><p>如果让人向北走，有可能他脚底一滑，向东走了  实际上课程里面的小人是小车，小车会打滑</p><p>对于$P_{sa}$  具体一点 $P_{(3,1)N}((3,2)) = 0.8$  $P_{(3,1)N}((4, 1)) = 0.1$ 和 $P_{(3,1)N}((2, 1)) = 0.1$ </p><p>假如要机器人走到右上角，那么我们将$(4, 3)$的位置放上奖励 + 1 如果不想让机器人走到哪一个格就在那一格上面放上 -1 比如在$(4, 2)$上面</p><p>还有其它的一些设计方法，一个常见的选择是put a very small penalty $R(s) = -0.02$ 对于其它的state</p><p>举个例： </p><ul><li><p>比如机器人一开始在$S_0$的位置</p></li><li><p>Choose action $a_0$, get to $s_1$  is distributed according to $P_{s_0a_0}$</p></li><li><p>Choose action $a_1$</p></li><li>Get to $s_2$ is distributed according to $P_{s_1a_1}$</li></ul><p>假设机器人的状态序列是$s_0, s_1, s_2, \cdots$ 那么总的收益是 $R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots$</p><p>RL的目标就是Choose actions over time to maximize</p><script type="math/tex; mode=display">E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots]</script><p><strong>大多数强化学习算法会提出a policy that maps from states to actions</strong> 即$\pi: S \rightarrow A$</p><p>因此大多数强化学习算法的输出是policy 另一种叫法是controller</p><p>强化学习的问题是给出了MDP的定义，然后<strong>有一个强化学习算法</strong> 找到使期望收益最大化的策略$\pi$</p><p>用强化学习解决chess问题的一个难点在于，一次action是包括自己的一步和对手的一步，但是你不知道对方会走什么 那么这就存在一个概率分布</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Project 1 用闲置电脑搭建centos服务器</title>
      <link href="2021/06/30/%E7%94%A8%E9%97%B2%E7%BD%AE%E7%94%B5%E8%84%91%E6%90%AD%E5%BB%BAcentos%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <url>2021/06/30/%E7%94%A8%E9%97%B2%E7%BD%AE%E7%94%B5%E8%84%91%E6%90%AD%E5%BB%BAcentos%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Project-1-用闲置电脑搭建centos服务器"><a href="#Project-1-用闲置电脑搭建centos服务器" class="headerlink" title="Project 1 用闲置电脑搭建centos服务器"></a>Project 1 用闲置电脑搭建centos服务器</h2><p>由于之前做网页做的觉得感觉还不错，想着以后在云服务器上面搭建网站，但是网页写的不好，就想自己搭一个服务器来模拟一下云服务器，一般云服务器上面都是centos，所以就选择了centos作为本地服务器的OS。虽然可以用虚拟机来搭服务器，不过家里有闲置的电脑的话，就用闲置的电脑吧。之前写的博客都没有图，这次贴一张图。</p><p><img src="https://ss0.baidu.com/7Po3dSag_xI4khGko9WTAnF6hhy/exp/w=500/sign=f3e4832706e9390156028d3e4bec54f9/0823dd54564e92585e34bde99b82d158ccbf4eb0.jpg" alt=""></p><h3 id="Start"><a href="#Start" class="headerlink" title="Start !!!"></a>Start !!!</h3><p>centos系统为7.5   iso镜像在vault上面下载 <a href="https://vault.centos.org/">网页链接</a></p><p>用ultraiso 将iso文件拷贝到U盘中</p><p>闲置电脑 开机启动设置为U盘  我的闲置电脑是联想的thinkpad 不用设置开机启动 之间在开机的时候按F12键 选择U盘启动，<strong>iso里面有方法可以将硬盘格式化 reclaim方法</strong></p><p>当时我选择了最小化安装，</p><p>分配硬盘是 /boot 2g  ext4 ; swap 8g   ;  其余是 / </p><p>开启了无线网卡 然后安装</p><p>安装完 发现和我之前在虚拟机上安装GNOME版的不一样，这个版本只有root，root的～路径是/root</p><p><strong>然后 ifconfig 发现没有ifconfig 这个命令！！</strong></p><p>然后发现ping www.baidu.com 也ping不通，下面进行了一系列操作来解决这个问题</p><ul><li><p>ip addr 查看无线网卡 我查到的名字是wlp3s0</p></li><li><p>ip link set wlp3s0 up</p></li><li><p>```text<br>wpa_supplicant -B -i wlp3s0 -c &lt;(wpa_passphrase “WiFi 名称” “密码”)</p><pre class="line-numbers language-none"><code class="language-none">+ dhclient wlp3s0+ service network restart此时查看ip addr 会发现 wlp3s0下面有ip地址在&#x2F;etc&#x2F;resolv.conf 设置dns&#96;&#96;&#96;txtnameserver 223.5.5.5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后ping baidu.com成功了</p></li></ul><p>根据清华源网站的教程 教源改成了清华源<a href="https://mirrors.cnnic.cn/help/centos/">链接</a> 按照上面进行操作</p><p>报错啦 <a href="http://mirrors.tuna.tsinghua.edu.cn/repodata/repomd.xml">http://mirrors.tuna.tsinghua.edu.cn/repodata/repomd.xml</a> [Errno 14] HTTP Error 404 - Not Found</p><p>看到这篇文章 <a href="https://blog.csdn.net/yuesichiu/article/details/53351324">文章链接</a> 上面说不要直接修改resolv.conf 而是要通过NetwrokManager来修改 其实不是这个原因，而是我误解了清华源网站上面的修改 应该是把<strong>mirrorlist中mirror.centos.org替换成mirrors.tuna.tsinghua.edu.cn 后面的url不变</strong></p><p>虽然404的问题没了，但是还是有新的问题</p><p>于是我按照这篇博文里面的设置 <a href="https://blog.csdn.net/xiaojin21cen/article/details/84726193">博客链接</a> 设置完之后yum makecache就OK了</p><p>yum install ifconfig  显示No package ifconfig available</p><p>用yum search ifconfig 显示net-tools.x86_64</p><p>然后我用yum install net-tools 然后ifconfig就能用了</p><p>即<strong>ifconfig 是 net-tools里面的内容</strong> 根据ifconfig里面显示的ip地址 用另一台电脑ping它 结果ping成功了</p><p>用ssh登录 mac的话 <strong>ssh 用户名@ip地址</strong> 用户名别掉了</p><p>ssh登录成功以后 w命令 发现有两个user 都是root</p><p><strong>一个是tty1   另一个是pts/0</strong>  发现登录时间不对 于是想看看linux如何查看时间</p><p>date命令查看时间 发现时间是CST，但是时间不对</p><p>yum -y表示安装过程中全部选yes  按照教程<a href="https://blog.csdn.net/king_wang10086/article/details/76178711">链接</a></p><p>yum install -y ntpdate  中间突然关机一次 重启后 报了个错</p><p>Delta RPMs disabled because /usr/bin/applydeltarpm not installed</p><p>突然发现ping www.baidu.com ping不通 要重新登录一下无线</p><p>登录完之后 yum install 就ok了 然后再 ntpdate us.pool.ntp.org 时间就正确了</p><h3 id="觉得用wpa登wifi比较麻烦"><a href="#觉得用wpa登wifi比较麻烦" class="headerlink" title="觉得用wpa登wifi比较麻烦"></a>觉得用wpa登wifi比较麻烦</h3><p>使用NetworkManager 的方法 <a href="https://blog.k4nz.com/949d40228a77b57d0fc0e7cd0ca4cbf1/">参考连接</a> 总结一下：</p><ul><li><strong>yum install -y NetworkManager-wifi</strong></li><li><strong>systemctl restart NetworkManager.service</strong></li><li><strong>nmcli device wifi list</strong></li><li><strong>nmcli device wifi connect “SSID-Name” password “your password”</strong></li></ul><p>出现问题 Error: Connection activation failed: 7 Secrets were required, but not provided</p><p>把电脑reboot之后 再来 就OK了</p><p>使用nmcli connection show 就能看到 NAME UUID TYPE DEVICE</p><p><strong>重启电脑后发现开机自动连接WIFI</strong></p><h3 id="centos-最小化安装查看电池电量"><a href="#centos-最小化安装查看电池电量" class="headerlink" title="centos 最小化安装查看电池电量"></a>centos 最小化安装查看电池电量</h3><p>在我的电脑上是 cat /sys/class/power_supply/BAT0/capacity</p><h3 id="安装python"><a href="#安装python" class="headerlink" title="安装python"></a>安装python</h3><p>使用命令python 发现电脑上默认的python 版本是2.7.5</p><p><a href="https://jingyan.baidu.com/article/19020a0ac2d827139c28423b.html">参考文章1</a>  <a href="https://www.cnblogs.com/jiangxiaobo/p/11734294.html">参考文章2</a></p><p>我想试试最简单的安装方法 先在官网上下载python<a href="https://www.python.org/downloads/source/">链接地址</a></p><p>在下载的Download XZ compressed source tarball上右键复制链接地址 发现</p><p><a href="https://www.python.org/ftp/python/3.9.6/Python-3.9.6.tar.xz">https://www.python.org/ftp/python/3.9.6/Python-3.9.6.tar.xz</a></p><p>准备用wget命令来下载 结果发现没有这个命令 yum -y install wget</p><p>下载得到Python-3.9.6.tar.xz 然后 使用xz -d Python-3.9.6.tar.xz <strong>得到Python-3.9.6.tar</strong></p><p>接着tar xf Python-3.9.6.tar</p><p>接着要安装gcc 来进行编译 发现安装的gcc 是4.8.5-44.el7版本</p><p>在install gcc的时候 有installing for dependencies 和 updating for dependencies</p><p><strong>接着安装make</strong> yum -y install make 发现安装的版本是1:3.82-24.el7</p><p><strong>安装的时候有个Repository 为base 我比较好奇</strong></p><p>然后 ./configure —prefix=/usr/local/python396</p><p>然后make 出现了个问题 fatal error: ffi.h: No such file or directory</p><p>使用yum install libffi-devel  再make 就成功了</p><p><strong>但是在指定目录下面没有找到python</strong></p><p>那是make 完了之后还有make install ！！！！</p><p>make install 报错 ModuleNotFoundError: No module named zlib</p><p>yum install zlib -y 之后再make install 还是同样的错</p><p>按照教程 安装 yum install zlib* -y <a href="https://blog.csdn.net/sirria1/article/details/83115582">关于zlib的相关资料</a></p><p><strong>设置系统默认的python为3.9.6版本</strong></p><ul><li>先看python的路径 which python 发现时/usr/bin</li><li>移除掉原来的python mv /usr/bin/python /usr/bin/python.bak</li><li>建立软链接 ln  -s /usr/local/python396/bin/python3 /usr/bin/python3</li><li>建立pip3的软链接 ln -s /usr/local/python396/bin/pip3 /usr/bin/pip3</li></ul><h3 id="centos最小化-怎么上下翻页"><a href="#centos最小化-怎么上下翻页" class="headerlink" title="centos最小化 怎么上下翻页"></a>centos最小化 怎么上下翻页</h3><p>shift + pageup  或者 shift + pagedown</p><p>但是发现往上翻的页数有限</p><h3 id="安装python虚拟环境"><a href="#安装python虚拟环境" class="headerlink" title="安装python虚拟环境"></a>安装python虚拟环境</h3><p>准备使用python的venv  <a href="https://docs.python.org/3/library/venv.html">参考链接</a>   <a href="https://www.jianshu.com/p/c5f973fd34d4">python3 venv简单使用</a></p><p>总结一下 就是在工作目录下使用 python -m venv . </p><p>启动虚拟环境 就是用source ./bin/activate</p><p><strong>用pip list</strong>来查看安装了哪些库 </p><p>进入了虚拟环境之后 我想要安装django 结果报错</p><p>WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.</p><p><a href="https://jingyan.baidu.com/article/cbf0e500475c042eab289362.html">解决方法</a></p><p><strong>准备安装openssl-devel的时候， 发现yum用不了了</strong> 网上搜了一下 找到<a href="https://blog.csdn.net/weixin_39541750/article/details/110558052">解决方法</a></p><p>方法就是修改yum配置文件 vi /usr/bin/yum，把文件头部的#!/usr/bin/python改成#!/usr/bin/python2.7保存退出即可。另外如果存在vim /usr/bin/yum-config-manager的话也需要改成python2.7。</p><p>此外在CentOS环境下安装其他命令报如下错误时，需要执行 vim /usr/libexec/urlgrabber-ext-down将/usr/bin/python改为/usr/bin/python2.7。修改完成后再一次执行，发现安装成功了。</p><p>yum install openssl-devel 版本为1.0.2 貌似不高 就准备卸了 再网上重装一个</p><p><a href="https://www.cnblogs.com/lemon-le/p/13419429.html">参考链接</a></p><h3 id="查看python3-是否编译openssl"><a href="#查看python3-是否编译openssl" class="headerlink" title="查看python3 是否编译openssl"></a>查看python3 是否编译openssl</h3><p>下面这个操作还是贼厉害的</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"> python3 -c &quot;import sysconfig; print(sysconfig.get_config_var(&#39;CONFIG_ARGS&#39;))&quot;&#39;--prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;python3&#39; &#39;--with-openssl&#x3D;&#x2F;usr&#x2F;local&#x2F;openssl&#39; &#39;--enable-shared&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>发现没有编译</p><p>查看软链接的方式  ls -il /usr/bin/python</p><p>wget <a href="https://www.openssl.org/source/openssl-1.1.1a.tar.gz">https://www.openssl.org/source/openssl-1.1.1a.tar.gz</a><br>tar -zxvf openssl-1.1.1a.tar.gz</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd openssl-1.1.1g&#x2F;.&#x2F;config --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;opensslmakemake install<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>安装报错 提示You need Perl 5</strong></p><p><a href="https://blog.csdn.net/pyd1040201698/article/details/98488982">安装perl 5的方法</a></p><ul><li>wget <a href="https://www.cpan.org/src/5.0/perl-5.28.0.tar.gz">https://www.cpan.org/src/5.0/perl-5.28.0.tar.gz</a> </li><li>cd perl-5.28.0</li><li>./Configure -des -Dprefix=$HOME/localperl</li><li>make</li><li>make test</li><li>make install</li></ul><p>写这篇文章的时候最新版本时5.34 奇数版本号不稳定 要安装偶数版本号</p><p><strong>又差东西pod2html command not found</strong></p><p>pod2html是perl的东西，因为我的perl安装在</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">$HOMD&#x2F;localperl上面<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>感觉这个问题可以先放一下 运行openssl的时候 报错</p><p>error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory</p><p><a href="https://www.cnblogs.com/xyb930826/p/6077348.html">解决方法</a></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">ln -s &#x2F;usr&#x2F;local&#x2F;lib64&#x2F;libssl.so.1.1 &#x2F;usr&#x2F;lib64&#x2F;libssl.so.1.1ln -s &#x2F;usr&#x2F;local&#x2F;lib64&#x2F;libcrypto.so.1.1 &#x2F;usr&#x2F;lib64&#x2F;libcrypto.so.1.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个不太行 换个方法 <a href="https://www.bswen.com/2018/11/others-Openssl-version-cause-error-when-loading-shared-libraries-libssl.so.1.1.html">方法链接</a></p><p><strong>里面有个重要信息：We can find that the ‘libcrypto.so.1.1’ is located in the /usr/local/lib64, But openssl try to find the .so libraries in the LD_LIBRARY_PATH</strong></p><p>又搜到另一个方法：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">echo &quot;&#x2F;usr&#x2F;local&#x2F;lib64&#x2F;&quot; &gt;&gt; &#x2F;etc&#x2F;ld.so.confldconfig<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>不太行</p><p><strong>cd到/usr/local/lib64发现根本就没有这两个文件</strong> 于是我用在编译的openssl中的lib进行软链</p><p>我的openssl 中 lib的路径是/usr/local/openssl/lib</p><p>用ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1  就OK了</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">wget https:&#x2F;&#x2F;www.openssl.org&#x2F;source&#x2F;openssl-1.1.1a.tar.gztar -zxvf openssl-1.1.1a.tar.gzcd openssl-1.1.1a# 2.编译安装.&#x2F;config --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;openssl no-zlib #不需要zlibmakemake install# 3.备份原配置mv &#x2F;usr&#x2F;bin&#x2F;openssl &#x2F;usr&#x2F;bin&#x2F;openssl.bakmv &#x2F;usr&#x2F;include&#x2F;openssl&#x2F; &#x2F;usr&#x2F;include&#x2F;openssl.bak# 4.新版配置ln -s &#x2F;usr&#x2F;local&#x2F;openssl&#x2F;include&#x2F;openssl &#x2F;usr&#x2F;include&#x2F;opensslln -s &#x2F;usr&#x2F;local&#x2F;openssl&#x2F;lib&#x2F;libssl.so.1.1 &#x2F;usr&#x2F;local&#x2F;lib64&#x2F;libssl.soln -s &#x2F;usr&#x2F;local&#x2F;openssl&#x2F;bin&#x2F;openssl &#x2F;usr&#x2F;bin&#x2F;openssl# 5.修改系统配置## 写入openssl库文件的搜索路径echo &quot;&#x2F;usr&#x2F;local&#x2F;openssl&#x2F;lib&quot; &gt;&gt; &#x2F;etc&#x2F;ld.so.conf## 使修改后的&#x2F;etc&#x2F;ld.so.conf生效 ldconfig -v# 6.查看openssl版本openssl version.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;python396 --with-openssl&#x3D;&#x2F;usr&#x2F;local&#x2F;opensslmakemake install<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><span style='color:red;font-size:30px'>全部搞定后 再pip install django 成功啦！！！</span></p><p>接着一波操作</p><ul><li>django-admin startproject gweb</li><li>python manage.py runserver 0.0.0.0:8000</li></ul><p>报错No module named ‘_sqlite3’</p><p>yum install sqlite-devel 然后重新编译</p><p>再运行 报新错 SQLite 3.9.0 or later is required found 3.7.17 <a href="https://blog.csdn.net/line_on_database/article/details/116058659">解决方法</a></p><p>export LD_LIBRARY_PATH = /usr/local/lib:$LD_LIBRARY_PATH</p><p>又报新错 deterministic=True requires SQLite 3.8.3 or higher</p><p><strong>重新编译一下python还是不行</strong> </p><p><a href="https://blog.csdn.net/weixin_42047023/article/details/110131770">找到新的解决方法</a>  核心思想是把代码里面的sqlite3 换成 pysqlite3</p><p>然后python manage.py runserver 0.0.0.0:8000 启动成功</p><p>用另一台电脑  访问192.168.1.106:8000 报错</p><p>DisallowedHost at /</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">Invalid HTTP_HOST header: &#39;192.168.1.106:8000&#39;. You may need to add &#39;192.168.1.106&#39; to ALLOWED_HOSTS.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>在setting.py里面 修改为ALLOWED_HOSTS = [‘*’] 就OK了</strong> </p><h3 id="PS-每次看电池电量的命令太长了"><a href="#PS-每次看电池电量的命令太长了" class="headerlink" title="PS 每次看电池电量的命令太长了"></a>PS 每次看电池电量的命令太长了</h3><p>设置简短的命令</p><ul><li>cd /root</li><li>vi .bashrc</li><li>加上alias capa=’cat /sys/class/power_supply/BAT0/capacity’</li><li>source .bashrc</li></ul><h3 id="最后再把之前export的临时变量-写到bashrc中去"><a href="#最后再把之前export的临时变量-写到bashrc中去" class="headerlink" title="最后再把之前export的临时变量 写到bashrc中去"></a>最后再把之前export的临时变量 写到bashrc中去</h3><p>export LD_LIBRARY_PATH=/usr/local/lib:/usr/lib64</p><p><span style='color:#DA70D6;font-size:30px;font-weight:bold'>Project 1 完结撒花~~~</span></p>]]></content>
      
      
      <categories>
          
          <category> Project系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Project </tag>
            
            <tag> centos </tag>
            
            <tag> 认真系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GDA Naive Bayes</title>
      <link href="2021/06/18/cs229%205/"/>
      <url>2021/06/18/cs229%205/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-5-GDA-Naive-Bayes"><a href="#Lecture-5-GDA-Naive-Bayes" class="headerlink" title="Lecture 5 GDA Naive Bayes"></a>Lecture 5 GDA Naive Bayes</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=7">视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes2.pdf">讲义链接</a></p><h3 id="Generative-Learning-Algorithms"><a href="#Generative-Learning-Algorithms" class="headerlink" title="Generative Learning Algorithms"></a>Generative Learning Algorithms</h3><p><strong>logistic regression 是在寻找一条直线，寻找一个将正例和负例分开的决策边界</strong></p><p>Generative Learning Algorithm没有寻找这种分离$($即并非是查看两个类然后去找到分离线$)$</p><p>而是<strong>一次查看一个类，根据这个类的所有训练数据尝试建立这个类的模型</strong>，在分类的时候，对于测试数据代入到每个类的模型之中，选择最有可能的一类。<strong>$($重要$)$</strong></p><ul><li><p>Discriminative learning algorithm：</p><p>Learn $\mathcal{P}(y|x)$ or learn $h_\theta(x)$ directly</p></li><li><p>Generative learning algorithm：</p><p>Learn $\mathcal{P}(x|y)$ 例如给定类别，what are the features likely gonna be like?</p><p>Learn $\mathcal{P}(y)$ (一般称作为class prior)</p></li></ul><p>根据Bayes rule $\mathcal{P}(y = 1 | x)  = \frac{\mathcal{P}(x | y = 1)\mathcal{P}(y = 1)}{\mathcal{P}(x)}$ 其中$\mathcal{P}(x) = \mathcal{P}(x | y = 1) \mathcal{P}(y = 1)+ \mathcal{P}(x | y = 0)\mathcal{P}(y = 0)$</p><p><strong>下面是两个generative learning algorithm的实例，一个针对continuous value features，另一个针对discrete features</strong></p><h3 id="Gaussian-Discriminant-Analysis-GDA"><a href="#Gaussian-Discriminant-Analysis-GDA" class="headerlink" title="Gaussian Discriminant Analysis$($GDA$)$"></a>Gaussian Discriminant Analysis$($GDA$)$</h3><p>假设$x \in \mathcal{R}^n$, <strong>高斯判别分析的核心假设是，我们将假定$\mathcal{P}(x|y)$服从高斯分布</strong></p><p>[10:00]开始讲多元高斯分布 讲到[19: 00]</p><p>GDA model：</p><script type="math/tex; mode=display">\mathcal{P}(x | y = 0) = \frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(x - \mu_0)^T\Sigma^{-1}(x - \mu_0)) \\\mathcal{P}(x | y = 1) = \frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(x - \mu_1)^T\Sigma^{-1}(x - \mu_1)))</script><p>这个模型的参数有$\mu_0,\mu_1,\Sigma,\phi$，<strong>注意模型假设这两个多元高斯分布的协方差矩阵是相同的</strong></p><p><strong>老师说可以使两个协方差矩阵不同，但是一般不这么做，一般都假设两个协方差矩阵是相同的</strong></p><p>对$\mathcal{P}(y)$进行建模，$y$取二元值，对此建模为伯努利，即$\mathcal{P}(y) = \phi^y(1 - \phi)^{1- y}$</p><p><strong>PS: 奥本海姆读$\phi$ 最后一个音节是i，吴恩达读$\phi$ 最后一个音节是ai</strong></p><h4 id="fit-parameters"><a href="#fit-parameters" class="headerlink" title="fit parameters"></a>fit parameters</h4><p>我们有训练集$\{ (x^{(i)},y^{(i)}\} \quad i = 1, \cdots, m$ 我们要做的是maximize the joint likelihood</p><script type="math/tex; mode=display">\begin{aligned}\ell(\phi, \mu_0, \mu_1, \Sigma) &= \prod_{i = 1}^{m}\mathcal{P}(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma) \\&= \prod_{i = 1}^{m}\mathcal{P}(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)\mathcal{P}(y^{(i)};\phi)\end{aligned}\tag{1}</script><p>对于discriminative learning algorithm 我们maxmize的是conditional likelihood：</p><script type="math/tex; mode=display">\ell(\theta) = \prod_{i = 1}^{m}\mathcal{P}(y^{(i)} | x^{(i)}, \theta)</script><p>对$(1)$进行最大似然估计，解得$\phi = \frac{\sum_{i = 1}^my^{(i)}}{m} = \frac{\sum_{i = 1}^m1\{y^{(i)} = 1 \}}{m}$ 其中1是一个indicator，$1\{ y^{(i)} = 1 \}$ 返回0或者1 根据里面的东西是否正确 $1\{ true \} = 1$ 和 $1\{false\} = 0$</p><p>$\mu_0 = \frac{\sum_{i = 1}^m1\{y^{(i)} = 0 \}x^{(i)}}{\sum_{i = 1}^mi\{y^{(i)}\} = 0}$ 和 $\mu_1 = \frac{\sum_{i = 1}^m1\{y^{(i)} = 1 \}x^{(i)}}{\sum_{i = 1}^mi\{y^{(i)}\} = 1}$ 从这个式子可以看出，均值的最大似然估计就是把一个类所有的数据 然后对所有的数据取均值，这个值就是均值$\mu$的最大似然估计</p><script type="math/tex; mode=display">\Sigma = \frac{1}{n}\sum_{i = 1}^n(x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T</script><p>用这个模型进行预测$arg\max_y\mathcal{P}(y | x)  = arg\max_y \frac{\mathcal{P}(x | y)\mathcal{P}(y)}{\mathcal{P}(x)}$ 相对于$y$ $\mathcal{P}(x)$是个常数</p><p>所以$arg\max_y\mathcal{P}(y | x)  = arg\max_y \mathcal{P}(x | y)\mathcal{P}(y)$</p><p><strong>区别argmax 和 max 两个notation </strong></p><script type="math/tex; mode=display">\min_z(z - 5)^2 = 0 \\arg\min_z(z - 5)^2 = 5</script><p>[37:20]开始讲用Logistic Regression 和 GDA 来对数据进行分类（如果GDA两个分布的协方差矩阵不相同，那么分界面可能就不是直线了</p><h4 id="与Logistic-Regression进行比较"><a href="#与Logistic-Regression进行比较" class="headerlink" title="与Logistic Regression进行比较"></a>与Logistic Regression进行比较</h4><p>这个部分讲义里面写的非常的详细</p><p>Compared to logistic regression，for a fixed $\phi, \mu_0, \mu_1, \Sigma$, lets plot $\mathcal{P}(y = 1 | x;\phi,\mu_0,\mu_1,\Sigma) $ as a function of x [43:40] 用1维的两个高斯 来举例GDA模型</p><p>核心在于<strong>GDA for a fixed $\phi, \mu_0, \mu_1, \Sigma$, lets plot $\mathcal{P}(y = 1 | x;\phi,\mu_0,\mu_1,\Sigma)$ as a function of x 是一个sigmoid函数，但是与logistic regression的sigmoid函数得到的decision boundary是不同的</strong></p><p>GDA 是一个比logistic regression <strong>更强</strong>的假设</p><p>一般做一个更强的假设，如果假设正确，那么模型就会做的很好$($即使对于非常小的数据集$)$</p><p>如果假设错误$( \mathcal{P}(x|y)$是服从泊松分布，结果得到的也是一个sigmoid函数$)$，那么模型效果就很差；对于弱的假设，模型就会更加稳定，在大数据的时代 数据很多，用logistic regression能克服模型偏弱的缺陷</p><p><strong>老师说他仍然使用GDA之类的算法的原因是 it’s actually quite computationally efficient, 因为求$\mu$就是求个加起来取平均，求个$\Sigma$就是求个矩阵乘法</strong></p><p><strong>老师说判断一个人机器学习能力 是在于对于小样本的数据集能否训练出相当好的模型</strong></p><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><p>naive bayes是适用于$x$离散数据</p><p>naive bayes的核心假设是<strong>Assume $x_i$’s are conditionally independent given y</strong></p><script type="math/tex; mode=display">\mathcal{P}(x_1, \cdots, x_{10000}) = \mathcal{P}(x_1|y)\mathcal{P}(x_2|y) \cdots \mathcal{P}(x_{10000}|y) = \prod_{i = 1}^{10000}\mathcal{P}(x_i | y)</script><p>参数有$\phi_{j|y=1} = \mathcal{P}(x_j = 1| y = 1), \phi_{j|y=0} = \mathcal{P}(x_j = 0| y = 0), \phi_y \quad j = 1, \cdots, n$</p><p>Joint likelihood: $\ell(\phi_j, \phi_{j|y} = \prod_{i = 1}^m \mathcal{P}(x^{(i)}, y^{(i)}; \phi_y, \phi_{j|y}))$</p><p>用MLE 解得</p><script type="math/tex; mode=display">\phi_y = \frac{\sum_{i = 1}^m1\{y^{(i)} = 1\}}{m}  \\\phi_{j | y = 1} = \frac{\sum_{i = 1}^m1\{x_j^{(i)} = 1,y^{(i)} = 1\}}{\sum_{i = 1}^m1\{y^{(i)} = 1\} }</script><p>老师说对于垃圾邮件分类，logistic regression要做的比这个好，naive bayes的优点是computationally efficient</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EM Algorithm Factor Analysis</title>
      <link href="2021/05/22/cs229%2015/"/>
      <url>2021/05/22/cs229%2015/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-15-EM-Algorithm-Factor-Analysis"><a href="#Lecture-15-EM-Algorithm-Factor-Analysis" class="headerlink" title="Lecture 15 EM Algorithm Factor Analysis"></a>Lecture 15 EM Algorithm Factor Analysis</h2><p>这一章会讲到Factor analysis 这个模型对于高维少量的数据也有用</p><p>前12分钟 用上一节课讲的EM算法 来推GMM算法 每个参数是怎么更新的，M步对函数求偏导</p><h3 id="Factor-Analysis"><a href="#Factor-Analysis" class="headerlink" title="Factor Analysis"></a>Factor Analysis</h3><p>在这个模型中$z^{(i)}$不是离散的，$z^{(i)}$是连续的，而且$z^{(i)}$是高斯分布的</p><p><strong>再回顾一下上节课讲的EM算法</strong></p><p>定义$J(\theta, Q) = \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})}$</p><p>由上节课 我们知道$l(\theta) \ge J(\theta, Q)$ for any $\theta, Q$ ， 另一种角度来看EM算法</p><p>E步 ：Maximize $J$ w.r.t. $Q$  即选Q让等号成立             M步 ：Maximize $J$ w.r.t. $\theta$</p><p>EM算法也被称作为 a coordinate ascent algorithm relative to this cost function $J$</p><p>老师举了个例子 n = 2 m = 100的时候 [18:30]图，两个高斯混合就够了，即$m &gt;&gt; n$的时候用GMM效果是很好的，<strong>如果$m \approx n$ 或者 $m &lt;&lt; n$的时候</strong>  比如 m = 30 n = 100</p><p>model a single Gaussian: $X \sim N(\mu, \Sigma)$   MLE: $\mu = \frac{1}{m}\sum_ix^{(i)}$ $\Sigma = \frac{1}{m}\sum_{i = 1}^m(x^{(i)} - \mu)(x^{(i)} - \mu)^T$</p><p>If $m \le n$, the $\Sigma$ will be <strong>singular</strong> 那么概率密度里面$\frac{1}{|\Sigma|^\frac{1}{2}}$就会无定义 而且对于指数项里面的$\Sigma^{-1}$也无法求, MLE失效，举了个二维空间 两个训练样本的例子，训练出来的高斯模型的轮廓将是infinitely skinny</p><p>Factorial analysis 最早出现于心理学研究中 研究100 psychological attributes 研究对象是30人</p><p>针对这个问题的想到解决方法：</p><ul><li>Constrain $\Sigma$ to be diagonal 这个模型是假设所有的特征都是不想关的，这不是一个好的假设</li><li>Constrain $\Sigma$ to be $\sigma^2I$  这个模型更糟</li></ul><p>$z \sim N(0, I),\  z \in R^d \ (d &lt; n)$  举个例子 d = 3 m = 30 n = 100</p><p>我们假设 $X = \mu + \Lambda z + \epsilon$  其中 $\epsilon \sim N(0, \Phi)$   这个模型中 每个参数为</p><script type="math/tex; mode=display">\mu \in R^n, \Lambda \in R^{n\times d},\Phi \in R^{n \times n} \ diagonal</script><p>那么 $x | z \sim N(\mu + \Lambda z, \Phi)$</p><p>d为d个main driving factors affecting the temperature of this room</p><p>老师用100个sensor测学校温度的例子来说明 这个模型，$\Phi$为对角线 说明每个sensor的噪声是相互独立的</p><p>老师举了两个例子 第一个例子 d = 1 n = 2 m = 7  第二个例子 d = 2 n = 3 m = 5</p><p>感觉有点降维的味道 第二个例子说明大部分的数据会集中在 线性变换后的平面附近，二维在三维线性变换 构成的是平面，但也可以是距离平面有点的远的地方，只要方差比较大，每一维的方差是不一样的</p><h3 id="The-derivation-of-EM-for-factor-analysis"><a href="#The-derivation-of-EM-for-factor-analysis" class="headerlink" title="The derivation of EM for factor analysis"></a>The derivation of EM for factor analysis</h3><p>推理比较tricky 视频最后30分钟 推导公式中比较tricky的部分 这要看讲义 光看视频看不懂 一脸懵逼</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Expectation-Maximization Algorithms</title>
      <link href="2021/05/22/cs229%2014/"/>
      <url>2021/05/22/cs229%2014/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=19">视频链接</a></p><h2 id="Lecture-14-Expectation-Maximization-Algorithms"><a href="#Lecture-14-Expectation-Maximization-Algorithms" class="headerlink" title="Lecture 14 Expectation-Maximization Algorithms"></a>Lecture 14 Expectation-Maximization Algorithms</h2><h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><p>[2:45] 开始演示了K-means的动画</p><p>Data $\{x^{(1)},x^{(2)},x^{(3)},,\cdots,x^{(m)}\}$</p><p>step 1 Initialize cluster centroids  $\mu_1,\cdots,\mu_k$ randomly，对于高维数据 一般是随机选k个训练样本，把这k个样本做为质心</p><p>step 2 Repeat until convergence</p><ul><li><p>Set $c^{(i)} := argmin_j ||x^{(i)} - \mu_j||^2$</p></li><li><p>For $j = 1, \cdots, k$</p><p>​    $\mu_j := \frac{\sum_{i = 1}^m 1\{c^{(i)} = j \} x^{(i)}}{\sum_{i = 1}^m i\{c^{(i) }= j\}}$</p></li></ul><p>这个算法是会收敛的</p><p>这个算法的cost function是$j(c,\mu) = \sum_{i = 1}^m || x^{(i)} - \mu_{c^{(i)}}||^2$</p><p>K-means 会收敛到局部最小值， 所以要多多初始化几次 选最好的一次</p><h3 id="Density-estimation"><a href="#Density-estimation" class="headerlink" title="Density estimation"></a>Density estimation</h3><p>可用于anomaly detection， model $p(x)$ 如果一个样本 $p(x) &lt; \epsilon$ 那么这个样本就是异常</p><p>the mixture of Gaussians model [21:03] 有图形，motivation就是anomaly detection</p><p>老师举了一个一维的例子，这个算法很像GDA，但是GMM不知道每个示例是属于哪个高斯的</p><p>EM算法能让我们尽管不知道每个样本是来自哪个高斯也能fit a model</p><p>Suppose there is a latent random variable $z$ and $x^{(i)},z^{(i)} $ </p><script type="math/tex; mode=display">P(x^{(i)}, z^{(i)}) = P(x^{(i)} | z^{(i)})P(z^{(i)})</script><p>where $z^{(i)} \sim Multinomial(\phi)$  $x^{(i)} | z^{(i)} = j \sim N(\mu_j, \Sigma_j)$</p><p>If we knew the $z^{(i)}$’s, we can use MLE  这就是GDA了</p><p><strong>E-step</strong> Guess value of $z^{(i)}$’s</p><p>Set $w^{(i)}_j = P(z^{(i)}) = j |x^{(i)}; \phi, \mu, \Sigma) = \frac{P(x^{(i)}|z^{(i)} = j)P(z^{(i) = j})}{\sum_{i = 1}^k p(x^{(i)}|z^{(i)} = l)P(z^{(i)} = l)}$</p><p>$w^{(i)}_j$ is  how much $x^{(i)}$ is assigned to the $\mu_j$ Gaussian</p><p>where $P(x^{(i)}|z^{(i)} = j) \sim N(\mu_j, \Sigma_j)$ and $P(z^{(i)} = j) = \phi_j$</p><p><strong>M-step</strong> 用E步里面得到的$w^{(i)}_j$来计算$\phi_j$和$\mu_j$和$\Sigma_j$</p><p><strong>发现没</strong> 这个很像K-means的过程！<strong>EM算法是一个框架</strong></p><p>当EM算法结束，就有了$P(x, z)$的联合分布 那么x的概率就是</p><script type="math/tex; mode=display">P(x) = \sum_kP(x, z)</script><p><strong>666</strong></p><h3 id="EM推理"><a href="#EM推理" class="headerlink" title="EM推理"></a>EM推理</h3><p><strong>Jensen’s inequality</strong></p><p>Let $f$ be a convex function e.g. $f^{\prime\prime}(x) \ge 0$</p><p>Let $X$ be a random variable Then $f(E[X]) \le E[f(X)]$</p><p>如果$f^{\prime\prime}(x) &gt; 0$ 那么$E[f(x)] = f(E[x]) \iff X \ is \ a\ constant$</p><p>如果$f$是一个strictly convex function 那么意思是$f$是一条直线</p><p>Have model for $P(x, z; \theta)$ Only observe $x$ , $\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$</p><script type="math/tex; mode=display">l(\theta) = \sum_{i = 1}^m logP(x^{(i)}; \theta) = \sum_{i = 1}^mlog\sum_{z(i)}P(x^{(i)},z^{(i)};\theta) \tag{1}</script><p>want to find $argmax_\theta l(\theta)$</p><p>图[61:37]E步是构造一个下界函数 即绘制绿色曲线，M步是找到这个曲线的最大值； 这个动画非常棒</p><p>EM算法可能会收敛于局部最优值，需要进行多次初始化 取最佳 就可以得到全局最优值 </p><p>1式继续变化得</p><script type="math/tex; mode=display">=\sum_i log\sum_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}</script><p>where $Q_i(z^{(i)})$是概率分布 i.e. $\sum_{z^{(i)}}Q_i(z^{(i)}) = 1$</p><p>继续化简</p><script type="math/tex; mode=display">= \sum_ilogE_{z^{(i)}\sim Q_i}[\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}] \ge \sum_iE_{z^{(i)}\sim Q_i}[log\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]</script><script type="math/tex; mode=display">= \sum_i \sum_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \tag{3}</script><p>对于3式 x式固定得，z是求和得，只有$\theta$是变量 可看作是$\theta$的函数</p><p>On a given iteration of EM with parameters $\theta$, we want:</p><script type="math/tex; mode=display">\sum_ilogE_{z^{(i)}\sim Q_i}[\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}] = \sum_iE_{z^{(i)}\sim Q_i}[log\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]</script><p>For this to hold true 需要$\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$ 是常数</p><p>Set $Q_i(z^{(i)})$正比于$P(x^{(i)}, z^{(i)}; \theta)$</p><p>取法就是$Q_i(z^{(i)}) = \frac{P(x^{(i)}, z^{(i)}; \theta)}{\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}; \theta)} = P(z^{(i)} | x^{(i)};\theta)$</p><p>总结一下就是：</p><p>E-step : Set $Q_i(z^{(i)}) = P(z^{(i)} | x^{(i)};\theta)$</p><p>M-step: $\theta := argmax_\theta\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Debug ML Models and Error Analysis</title>
      <link href="2021/05/22/cs229%2013/"/>
      <url>2021/05/22/cs229%2013/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-13-Debug-ML-Models-and-Error-Analysis"><a href="#Lecture-13-Debug-ML-Models-and-Error-Analysis" class="headerlink" title="Lecture 13 Debug ML Models and Error Analysis"></a>Lecture 13 Debug ML Models and Error Analysis</h2><p>感觉这节课很重要~  PS 想到深度学习能自动提取特征就非常的nice</p><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=17">视频链接</a></p><h3 id="Motivating-example"><a href="#Motivating-example" class="headerlink" title="Motivating example"></a>Motivating example</h3><p>当训练出的模型 的准确率不符合预期的时候：</p><ul><li><p>Common approach: Try improving the algorithm in different ways</p><ul><li>Try getting more training examples        Fixes high variance</li><li>Try a smaller set of features         Fixes high variance</li><li>Try a larger set of features          Fixes high bias</li><li>Try changing the features: Email header vs. email body features   Fixes high bias</li><li>Run gradient descent for more iterations    Fixes optimization algorithm</li><li>Try Newton’s method                    Fixes optimization algorithm</li><li>Use a different value for $\lambda $           Fixes optimization objective</li><li>Try using another algorithm     Fixes optimization objective</li></ul><p>一般会玄学地从中选一个</p></li><li><p><strong>bias vs variance diagnostic</strong></p><ul><li>Variance: Training error will be much lower than test error</li><li>Bias: Training error will also be high</li></ul></li></ul><p>老师画的图 [14:16] 横坐标是<strong>training set size</strong> 纵坐标是<strong>error</strong></p><p>当training set个数少的时候，training error是可以为0的 比如对于线性回归，1个点或两个点 training error就为0，数据量变多的时候 training error 会跟着涨</p><p><strong>重要</strong> as you increase the training set size , you find that the gap between training and test error usually closes <strong>如果test error 和 training error 有较大的gap 那么就是high variance</strong></p><p>对于high bias的情况，training error is unacceptable high，small gap between training and test error</p><p><strong>A very small gap between the training and the test error, 当图像是这样的话，不管再获得多少数据，error基本不会变</strong> </p><p><strong>PS 对于现在batch的形式，老师的这个图是有效的吗，如何实现training set增加，比如我有1000个数据集 我想让数据集从0到1000 数据集大小为100的时候 我该怎么挑数据</strong></p><p>常见的问题  Is the algorithm converging? 就是gradient descent有没有收敛</p><p>或者是 是否在优化错误的函数</p><p>即 到底是优化的函数的锅 还是优化方法的锅</p><p>对于区分non-spam 和 spam邮件，可以会不同类型设置不同的权重，比如希望能检测出spam，希望尽量减小漏掉spam的概率，spam的cost就大一些</p><h3 id="SVM-LR-Example"><a href="#SVM-LR-Example" class="headerlink" title="SVM LR Example"></a>SVM LR Example</h3><p>An SVM outperforms logistic regression, but you really want to deploy logistic regression for your application.</p><p>Let $\theta_{SVM}$ be the parameters learned by an SVM</p><p>Let $\theta_{BLR}$ be the parameters learned by logistic regression</p><p>You care about weighted accuracy:</p><script type="math/tex; mode=display">a(\theta)  = max_\theta \sum_i \omega^{(i)}1\{ h_\theta(x^{(i)}) = y^{(i)}\}</script><p>$\theta_{SVM}$ outperforms $\theta_{BLR}$. So:  $a(\theta_{SVM}) &gt; a(\theta_{BLR})$</p><p>BLR Binary Logistic Regression tries to maximize:</p><script type="math/tex; mode=display">J(\theta) = \sum_{i = 1}^mlogp(y^{(i)}|x^{(i)},\theta) - \lambda||\theta||^2</script><p>Diagnostic: $J(\theta_{SVM}) &gt; J(\theta_{BLR}) ?$</p><p>Case 1：$a(\theta_{SVM}) &gt; a(\theta_{BLR}) \ \ J(\theta_{SVM}) &gt; J(\theta_{BLR}) $</p><p>This means that $\theta_{BLR}$ fails to maximize $J$, adn the problem is with the convergence of the algorithm. Problem is with optimization algorithm</p><p>Case 2：$a(\theta_{SVM}) &gt; a(\theta_{BLR}) \ \ J(\theta_{SVM}) \le J(\theta_{BLR}) $</p><p>This means that BLR succeeded at maximizing $J(\theta)$, But the SVM, which does worse on $J(\theta)$, actually does better on weighted accuracy $a(\theta)$.</p><p>This means that $J(\theta)$ is the wrong function to be maximizing, if you care about $a(\theta)$. Problem is with objective function of the maximization problem.</p><h3 id="自动飞行器的例子"><a href="#自动飞行器的例子" class="headerlink" title="自动飞行器的例子"></a>自动飞行器的例子</h3><p>1 Build a simulator of helicopter</p><p>2 Choose a cost function</p><p>3 Run reinforcement learning algorithm to fly helicopter in simulation, so as to try to minimize cost function</p><p>如果训练出来的$\theta_{RL}$ 比人类pilot差很多  那么要么改进模拟器 要么改进cost function 要么改进optimization algorithm</p><p>Let $\theta_{human}$ be the human control policy. If $J(\theta_{human}) &lt; J(\theta_{RL})$，then the problem is in the reinforcement learning algorithm.</p><p>If  $J(\theta_{human}) \ge J(\theta_{RL})$，then the problem is in the cost function.</p><p>Finding a good cost function is really difficult</p><h3 id="Error-Analysis-Tool"><a href="#Error-Analysis-Tool" class="headerlink" title="Error Analysis Tool"></a>Error Analysis Tool</h3><pre class="line-numbers language-mermaid" data-language="mermaid"><code class="language-mermaid">graph LRid(Camera image) --&gt; id2(Preprocessing)id2 --&gt; id3(Facedetection) --&gt;id4(Eyes Segmentation) &amp; id5(Nose segmentation) &amp; id6(Mouth segmentation) --&gt; id7(Logistic regression) --&gt; id8(Label)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>现实中的项目 涉及到多个component</p><p>How much error is attributable to each of the components ?</p><p>Plug in ground-truth for each component, and see how accuracy changes.</p><p>可以是累积式 即到检测某个模块之前的所有模块都是真实数据，也可以是非累积性的， 即测哪个哪个真实 其余非真实</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS基础 III</title>
      <link href="2021/05/16/css3/"/>
      <url>2021/05/16/css3/</url>
      
        <content type="html"><![CDATA[<p>这是学堂在线上面的前端课程<a href="https://www.xuetangx.com/search?query=%E5%89%8D%E7%AB%AF&amp;page=1">课程链接</a></p><h3 id="CSS-Transform"><a href="#CSS-Transform" class="headerlink" title="CSS Transform"></a>CSS Transform</h3><p>是对元素进行平移translate， 旋转rotate，缩放scale， 倾斜skew</p><p><strong>transform不会的对其它元素布局产生影响</strong></p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.emoji</span><span class="token punctuation">&#123;</span>    <span class="token property">transform</span> <span class="token punctuation">:</span> <span class="token function">translate</span><span class="token punctuation">(</span>100px<span class="token punctuation">,</span> 100px<span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token selector">.emoji</span><span class="token punctuation">&#123;</span>    <span class="token property">transform</span> <span class="token punctuation">:</span> <span class="token function">translateX</span><span class="token punctuation">(</span>100px<span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token selector">.emoji</span><span class="token punctuation">&#123;</span>    <span class="token property">transform</span> <span class="token punctuation">:</span> <span class="token function">translateY</span><span class="token punctuation">(</span>-1em<span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token selector">.emoji</span><span class="token punctuation">&#123;</span>    <span class="token property">transform</span> <span class="token punctuation">:</span> <span class="token function">translate</span><span class="token punctuation">(</span>100%<span class="token punctuation">,</span> 100%<span class="token punctuation">)</span> 这是移动自身大小的距离<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.emoji</span><span class="token punctuation">&#123;</span>    <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">translateX</span><span class="token punctuation">(</span>100px<span class="token punctuation">)</span>           <span class="token function">rotate</span><span class="token punctuation">(</span>90deg<span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>先延X轴移动100px  然后顺时针旋转90度</p><p><strong>如果是先rotate 90度， 然后再translate 100px 那么效果不一样，因为rotate 90度之后 X轴变为向下方向的了</strong></p><p>还可以进行3D 变换</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.emoji</span><span class="token punctuation">&#123;</span>    <span class="token property">transform</span> <span class="token punctuation">:</span> <span class="token function">perspective</span><span class="token punctuation">(</span>100px<span class="token punctuation">)</span>        <span class="token function">rotateY</span><span class="token punctuation">(</span>40deg<span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>perspective 设置的是屏幕举例人眼的距离</p><p>translate3d(dx, dy, dz) 最后一个是z轴方向的偏移量</p><h3 id="CSS-Transition-即过渡效果"><a href="#CSS-Transition-即过渡效果" class="headerlink" title="CSS Transition 即过渡效果"></a>CSS Transition 即过渡效果</h3><p>指定从一个状态到另一个状态时如何过渡</p><p>动画的意义：告诉用户发生了什么</p><p>transition属性：</p><ul><li>transition-property 哪个属性改变的时候 需要发生过渡</li><li>transition-duration 这个过渡需要持续多长时间</li><li>transition-timing-function 这个变化是匀速的 还是先快后慢 先慢后快</li><li>transition-delay  发生改变的延时</li></ul><p><strong>666</strong></p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>box<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">.box</span><span class="token punctuation">&#123;</span>        <span class="token property">height</span> <span class="token punctuation">:</span> 200px<span class="token punctuation">;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> coral<span class="token punctuation">;</span>        <span class="token property">transition-property</span> <span class="token punctuation">:</span> height<span class="token punctuation">;</span>        <span class="token property">transition-duration</span> <span class="token punctuation">:</span> 1s<span class="token punctuation">;</span>        <span class="token property">transition-timing-function</span> <span class="token punctuation">:</span> linear<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.box : hover</span><span class="token punctuation">&#123;</span>        <span class="token property">height</span> <span class="token punctuation">:</span> 400px<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>transition: all 1s linear  任意一个属性发生变化 都会有变化效果，变化时间为1s， 匀速变化</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.box</span><span class="token punctuation">&#123;</span>    <span class="token property">height</span> <span class="token punctuation">:</span> 200px<span class="token punctuation">;</span>    <span class="token property">background</span> <span class="token punctuation">:</span> coral<span class="token punctuation">;</span>    <span class="token property">transition</span> <span class="token punctuation">:</span> all 1s linear<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">.box : hover</span><span class="token punctuation">&#123;</span>    <span class="token property">height</span> <span class="token punctuation">:</span> 400px<span class="token punctuation">;</span>    <span class="token property">background</span> <span class="token punctuation">:</span> green<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>也可以对于不同属性的变化 设置不同的效果</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.box</span><span class="token punctuation">&#123;</span>    <span class="token property">height</span> <span class="token punctuation">:</span> 200px<span class="token punctuation">;</span>    <span class="token property">background</span> <span class="token punctuation">:</span> coral<span class="token punctuation">;</span>    <span class="token property">transition</span> <span class="token punctuation">:</span> height 500ms linear<span class="token punctuation">,</span>         background 2s linear<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面是设置delay</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.box</span><span class="token punctuation">&#123;</span>    <span class="token property">height</span> <span class="token punctuation">:</span> 200px<span class="token punctuation">;</span>    <span class="token property">width</span> <span class="token punctuation">:</span> 400px<span class="token punctuation">;</span>    <span class="token property">background</span> <span class="token punctuation">:</span> coral<span class="token punctuation">;</span>    <span class="token property">transition</span> <span class="token punctuation">:</span> height 1s linear<span class="token punctuation">,</span>             width 1s linear 1s<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">.box:hover</span><span class="token punctuation">&#123;</span>    <span class="token property">height</span> <span class="token punctuation">:</span> 400px<span class="token punctuation">;</span>    <span class="token property">width</span> <span class="token punctuation">:</span> 100px<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="CSS-Animation"><a href="#CSS-Animation" class="headerlink" title="CSS  Animation"></a>CSS  Animation</h3><p>实现更复杂的样式变化效果</p><p>使用方法：</p><ul><li>定义关键帧样式</li><li>应用动画到元素上</li></ul><p><strong>666</strong></p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>scroll-down<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    ↓<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token atrule"><span class="token rule">@keyframes</span> down</span> <span class="token punctuation">&#123;</span>        <span class="token selector">from</span><span class="token punctuation">&#123;</span>            <span class="token property">margin-top</span> <span class="token punctuation">:</span> 0<span class="token punctuation">;</span>            <span class="token property">opacity</span> <span class="token punctuation">:</span> 1<span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>        <span class="token selector">50%</span><span class="token punctuation">&#123;</span>            <span class="token property">margin-top</span> <span class="token punctuation">:</span> 0.5em<span class="token punctuation">;</span>            <span class="token property">opacity</span> <span class="token punctuation">:</span> 0.3<span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>        <span class="token selector">to</span><span class="token punctuation">&#123;</span>            <span class="token property">margin-top</span><span class="token punctuation">:</span> 0<span class="token punctuation">;</span>            <span class="token property">opacity</span> <span class="token punctuation">:</span> 1<span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span>        <span class="token selector">.scroll-down</span><span class="token punctuation">&#123;</span>        <span class="token property">position</span><span class="token punctuation">:</span> fixed<span class="token punctuation">;</span>        <span class="token property">top</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>        <span class="token property">left</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>        <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">translate</span><span class="token punctuation">(</span>-50%<span class="token punctuation">,</span> 50%<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token property">font</span><span class="token punctuation">:</span> normal normal 100px/1 Helvetica<span class="token punctuation">;</span>        <span class="token property">color</span><span class="token punctuation">:</span> #f66<span class="token punctuation">;</span>        <span class="token property">animation</span><span class="token punctuation">:</span> down 1.5s ease infinite<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>animation 属性：</p><ul><li>animation-name 关键帧的名字</li><li>animation-duration 动画的持续时间</li><li>animation-timing-function 关键帧之间过渡的快慢</li><li>animation-delay 动画播放的延时</li><li>animation-iteration-count 动画播放的次数</li><li>animation-direction 动画是正序播放还是倒叙序播放</li></ul><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>beats<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    ❤<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token atrule"><span class="token rule">@keyframes</span> beats</span><span class="token punctuation">&#123;</span>        <span class="token selector">from</span><span class="token punctuation">&#123;</span>            <span class="token property">transform</span> <span class="token punctuation">:</span> <span class="token function">scale</span><span class="token punctuation">(</span>1<span class="token punctuation">)</span>        <span class="token punctuation">&#125;</span>        <span class="token selector">to</span><span class="token punctuation">&#123;</span>            <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">scale</span><span class="token punctuation">(</span>2<span class="token punctuation">)</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.beats</span><span class="token punctuation">&#123;</span>        <span class="token property">position</span><span class="token punctuation">:</span> fixed<span class="token punctuation">;</span>        <span class="token property">top</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>        <span class="token property">left</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>        <span class="token property">margin-top</span><span class="token punctuation">:</span> -0.5em<span class="token punctuation">;</span>        <span class="token property">margin-left</span><span class="token punctuation">:</span> -0.5em<span class="token punctuation">;</span>        <span class="token property">font</span><span class="token punctuation">:</span> normal normal 100px/1 Helvetica<span class="token punctuation">;</span>        <span class="token property">color</span><span class="token punctuation">:</span> #f66<span class="token punctuation">;</span>        <span class="token property">animation</span><span class="token punctuation">:</span> beats 1s ease infinite<span class="token punctuation">;</span>        <span class="token property">animation-direction</span><span class="token punctuation">:</span> alternate<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="响应式设计"><a href="#响应式设计" class="headerlink" title="响应式设计"></a>响应式设计</h3><p>响应式设计是同一个页面可以适应不同屏幕大小设备的设计方案</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">name</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>viewport<span class="token punctuation">"</span></span> <span class="token attr-name">content</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>width = device-width, initial-scale = 1.0<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>对于PC页面采用的视口大小 就是浏览器窗口的大小， 对于手机浏览器就不是这样，出于兼容以前PC网页的考虑，</p><p>手机浏览器在布局的时候使用的视口宽度是980px，而手机浏览器的宽度一般比这个要小，所以 整个页面会被缩小，为了让手机浏览器有更好的体验，要告诉手机浏览器不要使用980px的视口大小，而是使用和设备宽度的小</p><p>上面的代码就是干这个的</p><p>图片尺寸设置 适应不同的宽度</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>...<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">img</span><span class="token punctuation">&#123;</span>        <span class="token property">max-width</span> <span class="token punctuation">:</span> 100%<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>让背景图片适应不同的宽度</p><p>background-size : contain; 把图片自动缩放到容器内，把图片完整展示出来</p><p>background-size : cover;  把图片缩放到覆盖整个容器的大小 某些内容会被裁切掉</p><p>针对 不同屏幕尺寸 应用不同的样式 用media query</p><pre class="line-numbers language-html" data-language="html"><code class="language-html">@media screen and (min-width: 480px)&#123;.box&#123;font-size : 16px;&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以查询的media： width、height 、device-height、device-width、device-pixel-ratio、orientation</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">body</span><span class="token punctuation">&#123;</span>    <span class="token property">margin</span><span class="token punctuation">:</span> 0<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">nav</span><span class="token punctuation">&#123;</span>    <span class="token property">display</span><span class="token punctuation">:</span> flex<span class="token punctuation">;</span>    <span class="token property">background</span><span class="token punctuation">:</span> lightblue<span class="token punctuation">;</span>    <span class="token property">line-height</span><span class="token punctuation">:</span> 2<span class="token punctuation">;</span>    <span class="token property">justify-content</span><span class="token punctuation">:</span> space-around<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token atrule"><span class="token rule">@media</span> screen <span class="token keyword">and</span> <span class="token punctuation">(</span><span class="token property">max-width</span><span class="token punctuation">:</span> 480px<span class="token punctuation">)</span></span><span class="token punctuation">&#123;</span>    <span class="token selector">nav</span><span class="token punctuation">&#123;</span>        <span class="token property">flex-direction</span><span class="token punctuation">:</span> column<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用不同尺寸的图片 要考虑流量的问题，在手机 上不需要高分辨率的图片</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">srcset</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>/img/large.jpg 1200w,               /img/normal.jpg 800w,               /img/small.jpg 400w<span class="token punctuation">"</span></span>     <span class="token attr-name">sizes</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>(max-width: 320px) 400px,              (max-width: 640px) 800px,              1200px<span class="token punctuation">"</span></span>     <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>...<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>设置字体大小的套路之一</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">html</span><span class="token punctuation">&#123;</span>    <span class="token property">font-size</span> <span class="token punctuation">:</span> 18px<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">article h1</span><span class="token punctuation">&#123;</span>    <span class="token property">font-size</span> <span class="token punctuation">:</span> 2rem<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">article p</span><span class="token punctuation">&#123;</span>    <span class="token property">font-size</span> <span class="token punctuation">:</span> 1rem<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token atrule"><span class="token rule">@media</span> screen <span class="token keyword">and</span> <span class="token punctuation">(</span><span class="token property">max-width</span><span class="token punctuation">:</span> 480px<span class="token punctuation">)</span></span><span class="token punctuation">&#123;</span>    <span class="token selector">html</span><span class="token punctuation">&#123;</span>        <span class="token property">font-size</span><span class="token punctuation">:</span> 14px    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>里面的r 代表着root的意思</p><h3 id="处理CSS的兼容性"><a href="#处理CSS的兼容性" class="headerlink" title="处理CSS的兼容性"></a>处理CSS的兼容性</h3><p>了解浏览器支持的情况</p><ul><li>caniuse.com</li><li>MDN CSS Reference</li><li>Codrops CSS Reference</li><li>QuirksMode.org CSS</li></ul><p><strong>处理浏览器不支持的情况</strong></p><p>浏览器hack原理 同一个属性，后面书写的值覆盖前面书写的值</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">p</span><span class="token punctuation">&#123;</span>    <span class="token property">display</span><span class="token punctuation">:</span> table<span class="token punctuation">;</span>    <span class="token property">display</span><span class="token punctuation">:</span> flex<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>对于不支持flex的浏览器就会用table </p><p>条件注释</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token comment">&lt;!--[if IE 7]>&lt;p>只能在IE 7下看到我&lt;/p>&lt;![endif]--></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>还可以用浏览器的怪癖 略掉</p>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS基础 II</title>
      <link href="2021/05/16/css2/"/>
      <url>2021/05/16/css2/</url>
      
        <content type="html"><![CDATA[<p>这是学堂在线上面的前端课程<a href="https://www.xuetangx.com/search?query=%E5%89%8D%E7%AB%AF&amp;page=1">课程链接</a></p><h3 id="CSS-布局概述"><a href="#CSS-布局概述" class="headerlink" title="CSS 布局概述"></a>CSS 布局概述</h3><p>[00:15] 图片 显示了Header Nav Content Footer的布局</p><p>布局 Layout 确定内容的大小和位置的算法， 依据元素、容器、兄弟节点和内容等信息来计算</p><p>定位有三种模式： <strong>常规流</strong> Normal Flow， <strong>浮动</strong>， <strong>绝对定位</strong></p><p>常规流里面包含：行级， 块级， 表格布局， Flexbox，Grid布局</p><p>​    根元素、浮动和绝对定位的元素会脱离常规流</p><p>​    其它元素都在常规流之内， 常规流中的盒子，在某种排版上下文中参与布局</p><p>行级排版上下文 Inline Formatting Context IFC</p><p>只包含行级盒子的容器会创建一个IFC， IFC内的排版规则</p><ul><li>盒子在一行内水平摆放</li><li>一行放不下时，换行显示</li><li>text-align决定一行内盒子的水平对齐</li><li>vertical-align 决定一行内盒子的水平对齐</li><li>避开浮动 float 元素</li></ul><p>块级排版上下文 Block Formatting Context BFC</p><p>某些容器会创建一个BFC</p><ul><li>根元素</li><li>浮动、 绝对定位、 inline-block</li><li>Flex子项 和 Grid子项</li><li>overflow值不是visible的块盒</li></ul><p><strong>BFC内的排版规则</strong></p><ul><li>盒子从上到下摆放</li><li>垂直margin合并</li><li>BFC内盒子的margin不会与外面的合并</li><li>BFC不会和浮动元素重叠</li></ul><p>在流中 一个盒子的子盒子只能是全部是块级 或者全部是行级，如果一个容器中又有行级又有块级</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span><span class="token punctuation">></span></span>    This is atext and     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span><span class="token punctuation">></span></span>    block    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    and other text<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>html 会将行级的盒子包含在一个匿名的块级盒子里面</p><h3 id="Flexible-Box"><a href="#Flexible-Box" class="headerlink" title="Flexible Box"></a>Flexible Box</h3><p>Flexible Box是一种新的排版上下文，它可以控制子级盒子的摆放流向，摆放顺序，盒子宽度和高度，水平和垂直方向的对齐，是否允许拆行</p><p><strong>666</strong> 设置display: flex 使元素生成也给块级的Flex容器； display: inline-flex 使元素生成一个行级的Flex容器</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>a<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        A    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>b<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        B    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>c<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        C    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">.container</span><span class="token punctuation">&#123;</span>        <span class="token property">display</span> <span class="token punctuation">:</span> flex<span class="token punctuation">;</span>        <span class="token property">border</span> <span class="token punctuation">:</span> 2px solid red<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.a, .b, .c</span><span class="token punctuation">&#123;</span>        <span class="token property">text-align</span> <span class="token punctuation">:</span> center<span class="token punctuation">;</span>        <span class="token property">padding</span> <span class="token punctuation">:</span> 1em<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.a</span> <span class="token punctuation">&#123;</span> <span class="token property">background</span> <span class="token punctuation">:</span> #fcc<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>    <span class="token selector">.b</span> <span class="token punctuation">&#123;</span> <span class="token property">background</span> <span class="token punctuation">:</span> #cfc<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>    <span class="token selector">.c</span> <span class="token punctuation">&#123;</span> <span class="token property">background</span> <span class="token punctuation">:</span> #ccf<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面代码里面的div 就不是按块级的排列 从上到下，而是按照flex的排版，从左到右</p><p><strong>可以用flex-direction来控制flex内的流向</strong></p><p>flex-direction: row 或者 row-reverse 或者 column 从上到下 或者 column-reverse</p><p>当子元素比较多的时候 可以用flex-wrap来控制是否换行  默认使nowrap 就是不换行</p><p><strong>之所以叫flex 是因为里面的元素是可以设置弹性的 即flexibility</strong></p><p>flex-grow 有剩余空间时的伸展能力 flex-shrink 容器空间不足时收缩能力 flex-basis 没有伸展或收缩时的基础长度</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.a, .b, .c</span><span class="token punctuation">&#123;</span><span class="token property">width</span> <span class="token punctuation">:</span> 100px<span class="token punctuation">&#125;</span><span class="token selector">.a</span><span class="token punctuation">&#123;</span> <span class="token property">flex-grow</span> <span class="token punctuation">:</span> 2<span class="token punctuation">&#125;</span><span class="token selector">.b</span><span class="token punctuation">&#123;</span> <span class="token property">flex-grow</span> <span class="token punctuation">:</span> 1<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>c是100px， a 和 b填充剩余空间， 且a的宽度是b的两倍 计算方法是把容器的宽度减去300px 剩余的空间按照2 ： 1分别分配给a 和 b</p><p>flex-shrink 的默认值是1 默认是可以收缩的</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.a, .b, .c</span> <span class="token punctuation">&#123;</span> <span class="token property">width</span> <span class="token punctuation">:</span> 70% <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>显示出来是三个平分整个空间，但不是70%</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.a, .b, .c</span> <span class="token punctuation">&#123;</span> <span class="token property">width</span> <span class="token punctuation">:</span> 70% <span class="token punctuation">&#125;</span><span class="token selector">.a</span> <span class="token punctuation">&#123;</span><span class="token property">flex-shrink</span> <span class="token punctuation">:</span> 0<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>a不能收缩了，a的宽度必须是70% ； 如果b也设为不能收缩的时候， a和b就会溢出容器</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.a</span><span class="token punctuation">&#123;</span>    <span class="token property">width</span> <span class="token punctuation">:</span> 20%<span class="token punctuation">;</span>    <span class="token property">flex-basis</span> <span class="token punctuation">:</span> 50%<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>flex-basis的优先级是高于width的 会覆盖width</p><p>如果div a 、div b、 div c里面的字的个数是不一样的，那么宽度不一样</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.a, .b, .c</span><span class="token punctuation">&#123;</span>    <span class="token property">flex-grow</span> <span class="token punctuation">:</span> 1<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>三个框就不一样宽，因为是总宽减三宽之和，得的差除3 后分别加到三个div上 宽度不一样</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.a, .b, .c</span><span class="token punctuation">&#123;</span>    <span class="token property">flex</span> <span class="token punctuation">:</span> 1 1 0<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这样设置 三个宽度就是一样的了</p><p>flex-direction的方向称为主轴，和主轴的垂直方向称为侧轴，要设置元素在主轴方向的对齐 用</p><p>justify-content 可以设置为flex-start flex-end center space-between space-around space-evenly</p><p>flex-start 主轴出发的方向  space-between 是a、b、c中间有空白，space-around 是 a、b、c中间和两边都有空白，align-items 是设置测轴方向的对齐 flex-start flex-end center baseline stretch</p><p><strong>可以给容器中某一个元素设置对齐方式</strong> 就是设置align-self</p><p>[05:17]的图片 显示了 align-items 和 align-content 一起使用时的效果，一个行内 一个行间</p><p>可以设置order 属性来决定排列顺序，排列顺序是按order的从小到大的顺序来排列</p><p><strong>感觉按照这个知识点 可以写出一个2048的游戏</strong></p><p>视频举了一个用flex来布局的博客</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>header</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>logo<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        My Blog    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>nav</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Home<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Posts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>About<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>nav</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>header</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>wrapper<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>main</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>article</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h2</span><span class="token punctuation">></span></span>                 My blog title            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h2</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>                Some content in paragraph 1            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>                Some content in paragraph 2            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>                Some content in paragraph 3            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>                Some content in paragraph 4            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>article</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>main</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>aside</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h3</span><span class="token punctuation">></span></span>            Hot topics        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h3</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ul</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span> Topic 1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span> Topic 2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span> Topic 3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ul</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>aside</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>footer</span><span class="token punctuation">></span></span>    Copyrights <span class="token entity named-entity" title="&copy;">&amp;copy;</span> 2018<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>footer</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">header</span><span class="token punctuation">&#123;</span>        <span class="token property">display</span> <span class="token punctuation">:</span> flex<span class="token punctuation">;</span>        <span class="token property">align-items</span> <span class="token punctuation">:</span> center<span class="token punctuation">;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> coral<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">nav</span><span class="token punctuation">&#123;</span>        <span class="token property">flex-grow</span> <span class="token punctuation">:</span> 1<span class="token punctuation">;</span>        <span class="token property">text-align</span> <span class="token punctuation">:</span> right<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.wrapper</span><span class="token punctuation">&#123;</span>        <span class="token property">display</span><span class="token punctuation">:</span>flex<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">main</span><span class="token punctuation">&#123;</span>        <span class="token property">flex-grow</span> <span class="token punctuation">:</span> 1<span class="token punctuation">;</span>        <span class="token property">order</span> <span class="token punctuation">:</span> 1<span class="token punctuation">;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> lightblue<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">aside</span><span class="token punctuation">&#123;</span>        <span class="token property">width</span> <span class="token punctuation">:</span> 180px<span class="token punctuation">;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> lightgreen<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">footer</span><span class="token punctuation">&#123;</span>        <span class="token property">text-align</span> <span class="token punctuation">:</span> center<span class="token punctuation">;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> lightgray<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Grid布局"><a href="#Grid布局" class="headerlink" title="Grid布局"></a>Grid布局</h3><p>flex是单向的摆放， Grid是二维的布局； 用display : grid 使元素生成一个块级的grid容器，使用grid-template相关属性将容器划分为网络， 设置每一个子项占哪些行、列</p><p>划分网格</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token property">grid-template-columns</span><span class="token punctuation">:</span> 100px 100px 200px<span class="token punctuation">;</span><span class="token property">grid-template-rows</span><span class="token punctuation">:</span> 100px 100px<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>没有指定元素占几格，默认一个元素 占一格</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token property">grid-template-columns</span><span class="token punctuation">:</span> 30% 30% auto<span class="token punctuation">;</span><span class="token property">grid-template-rows</span><span class="token punctuation">:</span> 100px auto<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>grid有个特殊的单位 是fr，fr表示fraction </p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token property">grid-template-columns</span><span class="token punctuation">:</span> 100px 1fr 1fr<span class="token punctuation">;</span><span class="token property">grid-template-rows</span><span class="token punctuation">:</span> 100px 1fr<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>宽度除了100px 后面剩余部分 1fr 占50%</p><p>网格区域是由线来定义的，比如 1 1 3 3  行开始的线 列开始的线 行结束的线 列结束的线</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.preview</span><span class="token punctuation">&#123;</span>    <span class="token property">display</span> <span class="token punctuation">:</span> grid<span class="token punctuation">;</span>    <span class="token property">grid-template-rows</span><span class="token punctuation">:</span> 1fr 1fr 1fr<span class="token punctuation">;</span>    <span class="token property">grid-template-columns</span><span class="token punctuation">:</span>1fr 1fr 1fr<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">.a</span><span class="token punctuation">&#123;</span>    <span class="token property">grid-row-start</span> <span class="token punctuation">:</span> 1<span class="token punctuation">;</span>    <span class="token property">grid-column-start</span> <span class="token punctuation">:</span> 1<span class="token punctuation">;</span>    <span class="token property">grid-row-end</span> <span class="token punctuation">:</span> 3<span class="token punctuation">;</span>    <span class="token property">grid-column-end</span> <span class="token punctuation">:</span> 3<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>class a 从左上角开始占4格</p><p><strong>网格线可以被命名</strong> 命名的方式是用中括号括起来</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.preview</span><span class="token punctuation">&#123;</span>    <span class="token property">grid-template-columns</span><span class="token punctuation">:</span>        [left] 100px [center] 1fr [right]<span class="token punctuation">;</span>    <span class="token property">grid-template-rows</span><span class="token punctuation">:</span>        [top]        1fr        [middle]        1fr        [bottom]<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">.a</span><span class="token punctuation">&#123;</span>    <span class="token property">grid-area</span> <span class="token punctuation">:</span> top/left/bottom/center<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>命名网格区域    666</strong></p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.preview</span><span class="token punctuation">&#123;</span>    <span class="token property">display</span> <span class="token punctuation">:</span> grid<span class="token punctuation">;</span>    <span class="token property">grid-template-columns</span> <span class="token punctuation">:</span> 200px 1fr<span class="token punctuation">;</span>    <span class="token property">grid-template-rows</span><span class="token punctuation">:</span> 50px                        1fr                        50px<span class="token punctuation">;</span>    <span class="token property">grid-template-areas</span><span class="token punctuation">:</span> <span class="token string">"header header"</span>         <span class="token string">"aside  main"</span>                         <span class="token string">"footer footer"</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">.a</span><span class="token punctuation">&#123;</span>    <span class="token property">grid-area</span><span class="token punctuation">:</span> header<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token selector">.d</span><span class="token punctuation">&#123;</span>    <span class="token property">grid-area</span>  <span class="token punctuation">:</span> footer<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>grid-gap属性  可以设置 网格之间的间距 默认值是0</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token property">grid-row-gap</span><span class="token punctuation">:</span> 10px<span class="token punctuation">;</span><span class="token property">grid-column-gap</span><span class="token punctuation">:</span> 20px<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>行方向上的对齐 是用justify-items 默认值是stretch 还 可以设置为start end center</p><p>列方向上的对齐 是用align-items 默认值是stretch 还 可以设置为start end center</p><p>也 可以设置单独容器的对其方式 用justify-self  align-self</p><p><strong>有时候设置的网格不会占满整个容器的</strong></p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.preview</span><span class="token punctuation">&#123;</span>    <span class="token property">grid-template-rows</span><span class="token punctuation">:</span> 100px 100px<span class="token punctuation">;</span>    <span class="token property">grid-template-columns</span><span class="token punctuation">:</span> 100px 100px 100px<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>通过 align-content 和 justify-content来设置区域在容器内的对齐方式 </p><h3 id="表格样式"><a href="#表格样式" class="headerlink" title="表格样式"></a>表格样式</h3><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">table th</span><span class="token punctuation">&#123;</span>    <span class="token property">width</span> <span class="token punctuation">:</span> 100px<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>设置表头的宽度为100px 实际上不一定为100px 因为这个会受每一列的内容影响</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">table th, table td</span><span class="token punctuation">&#123;</span>    <span class="token property">width</span><span class="token punctuation">:</span> 100px<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>此时列与列 宽度相同</p><p>table-layout : fixed 表格的宽度计算就不会根据里面的内容做自动调整</p><p>边框设置  用border属性， border-collapse 来设置相邻的边框是不是合并</p><p>table的 排版方式也 可以用在其它容器上 加上display: table 就OK了</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>nav</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Home<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Blog<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Links<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>About<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>nav</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">nav</span><span class="token punctuation">&#123;</span>        <span class="token property">display</span> <span class="token punctuation">:</span> table<span class="token punctuation">;</span>        <span class="token property">width</span> <span class="token punctuation">:</span> 100%<span class="token punctuation">;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> lightblue<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>        <span class="token selector">nav a</span><span class="token punctuation">&#123;</span>        <span class="token property">display</span> <span class="token punctuation">:</span> table-cell<span class="token punctuation">;</span>        <span class="token property">text-align</span><span class="token punctuation">:</span> center<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="table-container"><table><thead><tr><th>display属性值</th><th>对应的HTML标签</th></tr></thead><tbody><tr><td>table</td><td>table</td></tr><tr><td>table-row</td><td>tr</td></tr><tr><td>table-row-group</td><td>tbody</td></tr><tr><td>table-header-group</td><td>thead</td></tr><tr><td>table-footer-group</td><td>tfoot</td></tr><tr><td>table-cell</td><td>td/th</td></tr><tr><td>table-caption</td><td>caption</td></tr></tbody></table></div><h3 id="浮动Float"><a href="#浮动Float" class="headerlink" title="浮动Float"></a>浮动Float</h3><p>浮动的概念来源于传统的印刷排版中的文字环绕图片这种效果，左浮动就是把图片放在容器的左边 文字绕开图片进行展示 如下图</p><p><img src="img\img1.png" alt=""></p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>section</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span>  <span class="token attr-name">src</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>/m.jpg<span class="token punctuation">"</span></span> <span class="token attr-name">width</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>300<span class="token punctuation">"</span></span> <span class="token attr-name">alt</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>mojave<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>        text ...    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>section</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">img</span><span class="token punctuation">&#123;</span>        <span class="token property">float</span> <span class="token punctuation">:</span> left<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>浮动对布局的影响</p><ul><li>浮动元素脱离常规流，漂浮在容器左边或者右边</li><li>浮动元素贴着容器边缘或另外的浮动元素</li><li>浮动元素不会影响常规流里面的块级盒子</li><li>浮动元素后面的行盒会变短以避开浮动元素</li></ul><p>clear属性设置 一个 元素是否能和浮动元素重叠 clear : left 表示不能和左浮动重叠</p><p>下面的不是很懂</p><p>将浮动的影响控制在容器内</p><ul><li>Block Formatting Context<ul><li>BFC的高度包含浮动元素</li><li>BFC不会和浮动元素上下重叠</li></ul></li><li>创建BFC 意思是加了下面的内容就会使非BFC的 创建BFC<ul><li>overflow 非visible</li><li>float、inline-block、绝对定位</li></ul></li></ul><p>[08:01] 举例了应用</p><h3 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h3><p>position 属性： static  默认值， 非定位元素</p><p>relative <strong>相对自身原本位置偏移</strong>， 不脱离文档流</p><p>absolute 绝对定位，相对非static祖先元素定位</p><p>fixed        相对于浏览器窗口绝对定位</p><p>position : relative</p><ul><li>在常规流里面布局</li><li>相对于自己本应该在的位置进行偏移</li><li>使用top 、left 、 bottom 、 right 设置偏移长度</li><li>流内其它元素当它没有偏移一样布局</li></ul><p>666 我终于懂了~~</p><p>position : absolute</p><ul><li>脱离常规流</li><li>相对于最近的非static祖先定位 找到祖先position是absolute 或者使relative</li><li>不会对流内元素布局造成影响</li></ul><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span><span class="token punctuation">></span></span>    页面标题<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>box<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>段落内容堕落内容 1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>段落内容堕落内容 2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>段落内容堕落内容 3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>段落内容堕落内容 4<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">.container</span><span class="token punctuation">&#123;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> lightblue<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.box</span><span class="token punctuation">&#123;</span>        <span class="token property">position</span><span class="token punctuation">:</span> absolute<span class="token punctuation">;</span>        <span class="token property">top</span><span class="token punctuation">:</span> 0<span class="token punctuation">;</span>        <span class="token property">left</span><span class="token punctuation">:</span> 0<span class="token punctuation">;</span>        <span class="token property">width</span><span class="token punctuation">:</span> 100px<span class="token punctuation">;</span>        <span class="token property">height</span><span class="token punctuation">:</span> 100px<span class="token punctuation">;</span>        <span class="token property">background</span><span class="token punctuation">:</span> red<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>box 会找到根元素，会根据根元素进行定位</p><p>如果想要box 根据 container 进行定位， 直接给container 加上position : relative 就可以了</p><p>下面是绝对定位使用很常见的场景 用它来实现居中</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span><span class="token punctuation">></span></span>    页面标题<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>box<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>段落内容堕落内容 1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>段落内容堕落内容 2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>段落内容堕落内容 3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>段落内容堕落内容 4<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">.container</span><span class="token punctuation">&#123;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> lightblue<span class="token punctuation">;</span>        <span class="token property">position</span><span class="token punctuation">:</span> relative<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.box</span><span class="token punctuation">&#123;</span>        <span class="token property">position</span><span class="token punctuation">:</span> absolute<span class="token punctuation">;</span>        <span class="token property">top</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>        <span class="token property">left</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>        <span class="token property">width</span><span class="token punctuation">:</span> 100px<span class="token punctuation">;</span>        <span class="token property">height</span><span class="token punctuation">:</span> 100px<span class="token punctuation">;</span>        <span class="token property">background</span><span class="token punctuation">:</span> red<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此时box的左上角在container的正中心的位置 如果要实现元素居中</p><p>加上margin-top : -50px    margin-left : -50px  即自身宽度和高度的一半</p><p><strong>如果我们设置了left和right 两个值之后</strong> 宽度就可以计算出来了， 如果宽度 也设置了， 那么right就是根据left和宽度做变化，如果不设置top left right bottom 那么这些值就是auto</p><p>他就会在他原本应该在的位置， 当元素还在流内时的位置</p><p>position : fixed  脱离了常规流 是相对于浏览器窗口的位置，不会随页面滚动发生位置变化</p><p>[09:04] 举了一个例子</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>nav</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span> 首页 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span> 导航1 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>#<span class="token punctuation">"</span></span><span class="token punctuation">></span></span> 导航2 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>nav</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>main</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>section</span><span class="token punctuation">></span></span> 1 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>section</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>section</span><span class="token punctuation">></span></span> 2 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>section</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>section</span><span class="token punctuation">></span></span> 3 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>section</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>section</span><span class="token punctuation">></span></span> 4 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>section</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>section</span><span class="token punctuation">></span></span> 5 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>section</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>main</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>#<span class="token punctuation">"</span></span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>go-top<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>返回顶部<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">nav</span><span class="token punctuation">&#123;</span>        <span class="token property">postion</span> <span class="token punctuation">:</span> fixed<span class="token punctuation">;</span>        <span class="token property">line-height</span> <span class="token punctuation">:</span> 3<span class="token punctuation">;</span>        <span class="token property">background</span> <span class="token punctuation">:</span> <span class="token function">rgba</span><span class="token punctuation">(</span>0<span class="token punctuation">,</span> 0<span class="token punctuation">,</span> 0<span class="token punctuation">,</span> 0.3<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token property">width</span> <span class="token punctuation">:</span> 100%<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">nav a</span><span class="token punctuation">&#123;</span>        <span class="token property">padding</span> <span class="token punctuation">:</span> 0 1em<span class="token punctuation">;</span>        <span class="token property">color</span> <span class="token punctuation">:</span> <span class="token function">rgba</span><span class="token punctuation">(</span>255<span class="token punctuation">,</span> 255<span class="token punctuation">,</span> 255<span class="token punctuation">,</span> 0<span class="token punctuation">,</span>7<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.go-top</span><span class="token punctuation">&#123;</span>        <span class="token property">position</span><span class="token punctuation">:</span> fixed<span class="token punctuation">;</span>        <span class="token property">right</span><span class="token punctuation">:</span> 1em<span class="token punctuation">;</span>        <span class="token property">bottom</span><span class="token punctuation">:</span> 1em<span class="token punctuation">;</span>        <span class="token property">color</span> <span class="token punctuation">:</span> #fff<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>无论怎么滚动 nav 和 返回顶部都在浏览器固定的位置</p><h3 id="堆叠层级"><a href="#堆叠层级" class="headerlink" title="堆叠层级"></a>堆叠层级</h3><p><strong>z-index</strong></p><ul><li>为定位元素指定其在z轴的上下层级</li><li>用一个整数表示，数值越大，越靠近用户</li><li>初始值为auto， 可以为负数、0、正数</li></ul><p><strong>堆叠上下文的创建</strong></p><ul><li>Root元素</li><li>z-index值不为auto的relative / absolute</li><li>position 是 fixed的元素</li><li>设置了某些属性的元素<ul><li>opacity 为 1</li><li>transform</li><li>animation</li></ul></li></ul><p>[05: 33] 举了一个例子</p><p><strong>绘制顺序</strong></p><p>在每一个堆叠上下文中，从下到上：</p><ul><li>形成该上下文的元素的border 和 background</li><li>z-index 为负值的子堆叠上下文</li><li>常规流内的块级元素</li><li>浮动元素</li><li>常规流内行级元素</li><li>z-index 为 0的子元素或子堆叠上下文</li><li>z-index 为正数的子堆叠上下文</li></ul>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ApproxEstimation Error</title>
      <link href="2021/05/09/cs229%209/"/>
      <url>2021/05/09/cs229%209/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-9-ApproxEstimation-Error"><a href="#Lecture-9-ApproxEstimation-Error" class="headerlink" title="Lecture 9 ApproxEstimation Error"></a>Lecture 9 ApproxEstimation Error</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=12">视频链接</a></p><p>这节课大部分内容是基于两个假设</p><ul><li>there exists a data distribution D from which x，y pairs are sampled; training set and test set come from the same distribution</li><li><p>all the samples are sampled independently</p><p>[5:28]的图 m个训练样本$x^{(1)}, y^{(1)}, \cdots, x^{(m)}, y^{(m)}$都是随机变量，经过Learning Algorithm 这个Deterministic Function 得到了$\hat{h}$或者$\hat{\theta}$，这个也是random variable</p></li></ul><p>Distribution of $\theta$ 我们称之为Sampling Distribution 存在$\theta^<em>$ or $h^</em>$ 我们称之为“True” parameter，我们希望这个parameter 是learning algorithm的输出，然而我们永远不会知道$\theta^*$是什么</p><p>注意$\theta^<em>$ 和$h^</em>$ 不是随机的， 意味着没有与之相关的概率分布，只是一个我们不知道的常数</p><h3 id="bias-和-variance的概念"><a href="#bias-和-variance的概念" class="headerlink" title="bias 和 variance的概念"></a>bias 和 variance的概念</h3><p>[13: 55] 左上角是high bias low variance， 右上角是high bias high variance</p><p>左下角是low bias low variance， 右下角是low bias high variance</p><p>bias is basically  checking whether the sampling distribution kind of centered around the true parameter.</p><p><strong>variance is  measuring basically how dispersed the sampling distribution is</strong></p><p>偏差和方差基本上是采样分布的first and second moments of your sampling distribution</p><p>如果收集无数个samples 那么其variance将变成0</p><p>如果当$m \to \infty$时 $\hat{\theta} \to \theta^*$  我们称这样的算法时consistent</p><p>high bias algorithm means no matter how much data or evidence you provided, it kind of always keep away from $\theta^*$ </p><p>high variance algorithm can easily get swayed by noise in the data.</p><p>bias 和 variance 是相互独立的</p><p>fighting variance的方法</p><ul><li>more data</li><li>regularization</li></ul><h3 id="hypothesis的分析"><a href="#hypothesis的分析" class="headerlink" title="hypothesis的分析"></a>hypothesis的分析</h3><p>在the space of hypothesis中，</p><p>假设存在一个最好的hypothesis g，即take this hypothesis and take the expected value of the loss with respect to the data generating distribution across an infinite amount of data. you kind of have the lowest error with this. [26: 00]的图</p><p>$g$ - best possible hypothesis ;  $h^*$ best in class $\mathcal{H}$ ; $\hat{h}$ learnt from finite data</p><p>$\epsilon(h)$: Risk / Generalization Error $E_{(x, y) \sim D}[1\{h(x) \ne y\}]$</p><p>$\hat{\epsilon}(h)$: Empirical Risk $= \frac{1}{m} \sum_{i = 1}^m 1\{ h(x^{(i)} \ne y^{(i)}\}$ 相当于有限版的$\epsilon(h)$</p><p>$\epsilon(g)$: Bayes Error / Irreducible Error   $\epsilon(h^*) - \epsilon(g) $ 是 Approximation Error</p><p>$\epsilon(\hat{h}) - \epsilon(h^*)$ 是 Estimation Error</p><p>容易看出 $\epsilon(\hat{h}) $ 是 estimation error， approximation error， Irreducible error的和</p><p>第一种error 是由于limited data造成的， 第二种error 是由模型选择造成的， 第三种error是不可避免的 其中第一种error可以表示为Estimation Error + Estimation Variance</p><p>我们经常提到的bias 和 variance 是</p><p>Bias 为 Estimation Error + approximation error</p><p>Variance 为 Estimation Variance </p><p>Bias is basically trying to capture why is $\hat{h}$ far from a $g$</p><p>根据[38:40]的图，如果$\mathcal{H}$减小 那么圈子会缩小，variance会下降，但是有可能偏离g 即bias上升</p><p>正则化就是会使$\mathcal{H}$ 减小，bias可能增大，不一定</p><p>Fight high bias</p><ul><li>make $\mathcal{H}$ bigger</li></ul><h3 id="Empirical-risk-minimization-ERM"><a href="#Empirical-risk-minimization-ERM" class="headerlink" title="Empirical risk minimization ERM"></a>Empirical risk minimization ERM</h3><p>empirical risk minimizer是一个a very specific type of learning algorithms</p><script type="math/tex; mode=display">\hat{h}_{ERM} = argmin_{h \in \mathcal{H}} \frac{1}{m}\sum_{i = 1}^m 1\{h(x^{(i)}) \ne y^{(i)} \}</script><p>Uniform Convergence</p><p>有两个核心问题：</p><p>if we do empirical risk minimization what about the generalization of an effect ?</p><p>第一个问题 $\hat{\epsilon}(h)$ versus $\epsilon(h)$   第二个问题 $\hat{\epsilon}(h)$ versus $\epsilon(h^*)$   </p><p>Tools: 1 Union Bound</p><p>如果我们有k个事件$A_1$, $A_2$, $\cdots$, $A_k$ 不需要相互独立，则</p><script type="math/tex; mode=display">P(A_1 \cup A_2 \cup \cdots \cup A_k) \le P(A_1) + P(A_2) + \cdots+ P(A_k)</script><p>2 Hoeffding’s Inequality</p><p>Let $Z_1$, $Z_2$, $\cdots$,$Z_m \sim Bernoulli(\phi)$  $\hat{\phi} = \frac{1}{m}\sum_{i = 1}^mZ_i$ Let $\gamma &gt; 0$ [margin]</p><script type="math/tex; mode=display">Pr(|\hat{\phi} - \phi)| > \gamma) \le 2 exp(-2\gamma^2 m)</script><p>[62:00] $\epsilon(h)$ 和 $\hat{\epsilon}_S(h)$的图  $E[\hat{\epsilon}_S(h_i)] = \epsilon(h_i)$</p><script type="math/tex; mode=display">Pr(|\hat{\epsilon}(h_i) - \epsilon(h_i)| > \gamma) \le2exp(-2\gamma^2m) \tag{1}</script><p>对于$\hat{\epsilon}_S(h_i)$ 的图像是针对一个特定的size，当size变化了，$\hat{\epsilon}_S(h_i)$就会不同</p><p>我们对式子$(1)$扩展到所有的h，就是固定size 求关于h的期望 这个称为uniform convergence</p><p>我们试图查看经验风险曲线 converges uniformly to the 泛化风险曲线</p><p>[71:11]开始 证明了 当hypothesis class $\mathcal{H}$ 是有限的情况，思路是对所有k个假设使用union bound</p><p>感觉这部分有点硬核，建议对着讲义看</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Splits, Models &amp; Cross-Validation</title>
      <link href="2021/05/09/cs229%208/"/>
      <url>2021/05/09/cs229%208/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-8-Data-Splits-Models-amp-Cross-Validation"><a href="#Lecture-8-Data-Splits-Models-amp-Cross-Validation" class="headerlink" title="Lecture 8 Data Splits, Models &amp; Cross-Validation"></a>Lecture 8 Data Splits, Models &amp; Cross-Validation</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=11">课程视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes5.pdf">课程讲义链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/addendum_bias_variance.pdf">课程讲义链接2</a></p><p>听第七课的时候官网上的讲师还是Andrew Ng， 今天上官网的时候 发现讲师变成Moses Charikar</p><h3 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h3><p><strong>easy to learn but hard to master</strong></p><p>[06:26]画了三张图，左边是underfit the data，high bias；右边是overfit the data， high variance</p><p>bias在机器学习中的定义是 this learning algorithm had very strong preconceptions that the data could be fit by linear functions and this bias turns out not to be true.</p><p>Regularization 用来减少overfitting的情况</p><script type="math/tex; mode=display">min_\theta \frac{1}{2}\sum_{i = 1}^m ||y^{(i)} - \theta^Tx^{(i)}||^2 + \frac{\lambda}{2}||\theta||^2</script><p>其中$\frac{\lambda}{2}||\theta||^2$是正则项</p><p>对于SVM，通过核函数可以映射到无限维，为啥SVM没有明显的过拟合呢，老师说理由比较复杂，简单来说就是目标函数$||\omega||^2$有着和正则化相似的效果 </p><p>至于使用$\frac{\lambda}{2}||\theta||^2$ 而非 $\sum_j \lambda_j\theta_j$ 是因为会使训练的参数变得很多</p><p>PS： 对于不同的features 每个features的范围会不同，我们需要把features normalization到同一个范围下 比如$[0, 1]$ 这个范围 通过$(x - min) / (max - min)$</p><h3 id="Train-Dev-Test-sets"><a href="#Train-Dev-Test-sets" class="headerlink" title="Train/Dev/Test sets"></a>Train/Dev/Test sets</h3><p>对于10000个训练数据，我们要进行超参数的选择的话</p><p>$S_{train}, S_{dev}, S_{test}$ 其中dev 表示development，train each model（option for degree of polynomial 或者不同的正则系数等等) on $S_{train}$，get some hypothesis $h$ and measure erroe on $S_{dev}$ and pick model with lowest error on $S_{dev}$ </p><p>传统的划分方法是60% train, 20% dev, 20% test, 对于数据集很大的情况 这个比例会变化，可能是90% train， 5% dev，5% test</p><p>对于少量的训练数据，比如100个训练数据，用K-fold  k为10是typical choice</p><p>假如对于k为5的情况，就是把100个数据集分为五个不同的子集，接着要做的是</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">For i &#x3D; 1, ... ,kTrain parameters on k - 1 pieces;Test on remaining one piece;Average k on test errorsOptional: Refit model on 100% of data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果对于更小的数据集 比如m = 20</p><p>Leave-one-out 即设K = m 缺点是计算量很大 老师用这个方法 只在m小于等于100的情况下</p><p>老师推荐了 mlyearning.org这个网站 这里有进一步讨论 如果训练集和测试集是不同的分布</p><h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><p>a special case of model selection</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">start with F &#x3D; 空Repeat:1) Try to add each feature i to F, and see which single-feature addition most improves dev set performance2) Add that feature to F<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Neural Networks</title>
      <link href="2021/05/09/cs229%2011/"/>
      <url>2021/05/09/cs229%2011/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-11-amp-12-Neural-Networks"><a href="#Lecture-11-amp-12-Neural-Networks" class="headerlink" title="Lecture 11&amp;12 Neural Networks"></a>Lecture 11&amp;12 Neural Networks</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=14">视频链接</a></p><h3 id="将逻辑回归解释为一种特殊的神经网络"><a href="#将逻辑回归解释为一种特殊的神经网络" class="headerlink" title="将逻辑回归解释为一种特殊的神经网络"></a>将逻辑回归解释为一种特殊的神经网络</h3><p>视频前40分钟 是讲这个部分的</p><p><strong>将logistics regression作为神经网络的intuitive 我觉得挺好的</strong></p><p>目标1是对一张图来进行分类，判断是不是猫，假设图片是64*64大小 把图片展成向量 变成12288维向量，用logistics regression进行训练$\hat{y} = \sigma(\theta^Tx) = \sigma(wx + b)$</p><p>训练的步骤是:</p><ul><li>初始化w和b</li><li>找到w和b的最优值</li><li>用$\hat{y} = \sigma(wx + b)$来进行预测</li></ul><p>对于神经网络里面的神经元 我们定义是linear + activation</p><p>对于模型 我们的定义是 architecture + parameters</p><p>目标2 对于图片进行三分类</p><p>把一个神经元 扩展成3个神经元 分别计算$\hat{y_i} = a^{[1]}_i = \sigma(w_i^{[1]}x + b_i^{[1]}) \ \ i = 1,2,3$</p><p>上标1 代表layer 1，对于第一类的表示为$(1, 0, 0)$ 第二类的表示为$(0, 1, 0)$ 第三类的表示为$(0, 0, 1)$ </p><p>训练方法同上，三个神经元之间没有联系，可以独立的训练这三个神经元；这个模型robust</p><p>目标3 加上约束 图片上只有一个动物 softmax multi-class regression</p><p>对于softmax 其损失函数为cross-entropy loss $\mathcal{L} = -\sum_{k = 1}^3y_klog\hat{y_k}$</p><h3 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h3><p>这个部分对应视频的后40分钟</p><p>对于之前分辨一个图片是否是猫的图片 用了一个3 layers的网络</p><p>layer denotes neurons that are not connected to each other 一层神经元</p><p>在这个三层的网络单中[45:44]的图，第一层are going to understand the fundamental concepts of the image which is the edges. Then what’s gonna happen is that these neurons are going to communicate what they found on the image to the next layer’s neuron.</p><p><strong>end to end learning</strong>: we have an input, a ground truth and we don’t constrain the network in the middle i.e. we are just training based on the input and the output</p><p>视频中介绍的向量化的式子：</p><script type="math/tex; mode=display">Z^{[1]} = W^{[1]}X + b^{[1]}</script><p>m个训练数据 输入feature的维度是n 其中Z的shape是$(3, m)$ W的shape是$(n, m)$，b的维度是$(3, 1)$</p><h3 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h3><p>下面的公式 是根据一个三层全连接神经网络 其中第一层3个神经元 第二层2个神经元 最后一层1个神经元</p><p>用batch of examples，而非single example，原因是我们想要vectorization</p><script type="math/tex; mode=display">cost \ function: \mathcal{J}(\hat{y}, y) = \frac{1}{m}\sum_{i = 1}^m\mathcal{L}^{(i)}(\hat{y}, y) \\with: \ \mathcal{L}^{(i)} = -[y^{(i)}log\hat{y}^{(i)} + (1 - y^{(i)})log(1 - \hat{y}^{(i)}))] \\Update: \ \omega^{[l]} = \omega^{[l]} - \alpha\frac{\partial y}{\partial \omega^{[l]}}</script><p>反向来计算导数：</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial \omega^{[3]}} = -[y^{(i)}\frac{\partial}{\partial\omega^{[3]}}(log\sigma(\omega^{[3]}a^{[2]} + b^{[3]})) + (1 - y^{(i)})\frac{\partial}{\partial\omega^{[3]}}(log(1 - \sigma(\omega^{[3]}a^{[2]} + b^{[3]})))] \\= -[y^{(i)}\frac{1}{a^{[3]}}a^{[3]}(1 - a^{[3]})(a^{[2]})^T +(1-y^{(i)})\frac{1}{1 - a^{[2]}}(-1)a^{[3]}(1 - a^{[3]})(a^{[2]})^T \\= -[y^{(i)}(1 - a^{[3]})(a^{[2]})^T - (1 - y^{(i)})a^{[3]}(a^{[2]})^T] \\= -[y^{(i)}(a^{[2]})^T - a^{[3]}(a^{[2]})^T] = -(y^{(i)} - a^{[3]})(a^{[2]})^T \\\therefore \frac{\partial \mathcal{J}}{\partial \omega^{[3]}} = -\frac{1}{m}\sum_{i = 1}^m(y^{(i)} - a^{[3]})(a^{[2]})^T</script><p>反向传播到$\omega^{[2]}$</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial \omega^{[2]}} = \frac{\partial \mathcal{L}}{\partial a^{[3]}}\frac{\partial a^{[3]}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial a^{[2]}}\frac{\partial a^{[2]}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial \omega^{[2]}}</script><p>用之前算的结果来加速这个式子的算法</p><script type="math/tex; mode=display">\because \frac{\partial \mathcal{L}}{\partial \omega^{[1]}} = \frac{\partial \mathcal{L}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial \omega^{[3]}}=-(y^{(i)} - a^{[3]})(a^{[2]})^T \ \ \frac{\partial z^{[3]}}{\partial \omega^{[3]}} = (a^{[2]})^T\\ \therefore \frac{\partial \mathcal{L}}{\partial a^{[3]}}\frac{\partial a^{[3]}}{\partial z^{[3]}} = a^{[3]} - y^{(i)} \\\frac{\partial z^{[3]}}{\partial a^{[2]}} = (\omega^{[3]})^T \ \ \frac{\partial a^{[2]}}{\partial z^{[2]}} = a^{[2]}(1 - a^{[2]}) \ \ \frac{\partial z^{[2]}}{\partial \omega^{[2]}} = (a^{[1]})^T \\\therefore \frac{\partial \mathcal{L}}{\partial\omega^{[2]}} = (a^{[3]} - y^{(i)})(\omega^{[3]})^Ta^{[2]}(1 - a^{[2]})(a^{[1]})^T</script><p>注意 $\frac{\partial z^{[3]}}{\partial a^{[2]}}\frac{\partial a^{[2]}}{\partial z^{[2]}}$ 因为$a^{[2]}$和$z^{[2]}$两个都是$(2, 1)$向量  所以导数也是$(2, 1)$向量 它们之间的乘法应该是点乘</p><h3 id="Improving-NN"><a href="#Improving-NN" class="headerlink" title="Improving NN"></a>Improving NN</h3><ul><li><p>Activation function</p><p>sigmoid， ReLU， tanh</p><p>Q: Why we need activation function ?  A: to get nonlinearity</p></li><li><p>Initialization method 优势在于方便optimization</p><p>对于输入数据为$(x_1, x_2)^T$  令新的数据$x_i^{(i)} = x_i^{(i)} - \mu_i$ 其中$\mu = \frac{1}{m}\sum_{i = 1}^m x^{(i)}$</p><p>然后令$\sigma^2 = \frac{1}{m}\sum_{i = 1}^m(x^{(i)})^2$  取$\tilde{x_i} = \frac{x^{(i)}}{\sigma}$</p><p>注意$\mu$和$\sigma$是根据训练集里面计算的，对于测试集也用这两个参数</p></li><li><p>Vanishing， Exploding gradients</p><p>[55:00]举了一个例子，说明随着网络的层数增加，矩阵相乘的error也会增加，有可能gradients是一个超级大的值 或者是非常小的值 权值初始化在1左右是非常好的</p><p>一个神经元的输入为n， n越大$w_i$越小，$\frac{1}{n}$的初始化时非常好的，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token operator">*</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> n<span class="token punctuation">)</span> <span class="token comment">#对于sigmoid函数 这个初始化效果很好</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token operator">*</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">/</span> n<span class="token punctuation">)</span> <span class="token comment">#对于ReLU函数 这个初始化效果很好</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>上面的n 时 $n^{[l - 1]}$</p></li><li><p>其它Initialization</p><p>Xavier Initialization $W^{[l]} \sim \sqrt{\frac{1}{n^{[l - 1]}}}$   for tanh     He Initialization $W^{[l]} \sim \sqrt{\frac{2}{n^{[l]} + n^{[l - 1]}}}$</p></li><li><p>Optimization</p><p>mini-batch algorithm</p></li><li><p>Momentum algorithm</p><p>Assume you have the loss that is very extended in one direction. [72:00]有示例</p><p>What momentum is going to say is look at the past updates that you did and try to consider these past updates in order to find the right way to go</p><script type="math/tex; mode=display">v = \beta v + (1 - \beta)\frac{\partial \mathcal{L}}{\partial \omega} \\\omega = \omega - \alpha v</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS基础</title>
      <link href="2021/05/09/css1/"/>
      <url>2021/05/09/css1/</url>
      
        <content type="html"><![CDATA[<p>这是学堂在线上面的前端课程<a href="https://www.xuetangx.com/search?query=%E5%89%8D%E7%AB%AF&amp;page=1">课程链接</a></p><h2 id="CSS基础"><a href="#CSS基础" class="headerlink" title="CSS基础"></a>CSS基础</h2><h3 id="CSS简介"><a href="#CSS简介" class="headerlink" title="CSS简介"></a>CSS简介</h3><p>css Cascading Style Sheets 可设置颜色大小，动画效果</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">h1</span><span class="token punctuation">&#123;</span>    <span class="token property">color</span><span class="token punctuation">:</span>white<span class="token punctuation">;</span>    <span class="token property">font-size</span><span class="token punctuation">:</span>14px<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>h1是selector选择器  color是属性， white是属性值</p><p>在页面中使用CSS</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">&lt;!-- 外链 -->&lt;link rel = "stylesheet" href=" ">&lt;!-- 嵌入 -->&lt;style>li</span> <span class="token punctuation">&#123;</span><span class="token property">margin</span><span class="token punctuation">:</span> 0<span class="token punctuation">;</span><span class="token property">list-style</span><span class="token punctuation">:</span> none<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span><span class="token selector">p</span>  <span class="token punctuation">&#123;</span><span class="token property">margin</span><span class="token punctuation">:</span> lem 0<span class="token punctuation">;</span><span class="token punctuation">&#125;</span>&lt;/style>&lt;!-- 内联  不推荐使用-->&lt;p style=<span class="token string">"margin:lem 0"</span>>Example Content&lt;/p><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>css工作原理</p><p>加载HTML 之后会解析HTML 然后会创建DOM树， 解析HTML时也会加载CSS 解析CSS 然后会添加样式到DOM节点</p><h3 id="基础选择器"><a href="#基础选择器" class="headerlink" title="基础选择器"></a>基础选择器</h3><p>可使用多种方式选择元素 按照标签名，类名或id； 按照属性； 按照DOM树中的位置</p><p>通配选择器*， 标签选择器</p><p>id选择器 #logo 表示id为logo的标签 一般只有一个； 类选择器 .done class为done的所有标签 可多个</p><p>属性选择器 input[type=”password”]； a[href^=”#”] 表示href的值以#开头， a[href$=”.jpg”] 表示href的值以.jpg结尾</p><p><strong>伪类 666</strong></p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">&lt;!-- 动态伪类 根据元素所处的状态来选择-->a : link</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token selector">选中一个正常连接a : visited</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token selector">访问过的链接a : hover</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token selector">鼠标放在链接上a : active</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token property">链接被鼠标按下去了</span><span class="token punctuation">:</span>focus  对于输入框 点下去后 &lt;!-- 结构性伪类 --><span class="token property">li</span><span class="token punctuation">:</span>first-child 表示li标签的第一个孩子<span class="token property">li</span><span class="token punctuation">:</span>last-child 表示li标签的最后一个孩子<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>组合器</strong></p><p>直接组合 AB 满足A同时满足B input:focus</p><p>后代组合 A B 选中B 如果它时A的子孙  nav a</p><p>亲子组合 A &gt; B 选中B， 如果它时A的子元素   section &gt; p</p><p>选择器组，多个选择器放一起， 用逗号隔开</p><h3 id="设置字体"><a href="#设置字体" class="headerlink" title="设置字体"></a>设置字体</h3><p>font-family 设置字体 </p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">h1</span><span class="token punctuation">&#123;</span>    <span class="token property">font-family</span><span class="token punctuation">:</span> Optima<span class="token punctuation">,</span> Georgia<span class="token punctuation">,</span> serif<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>设置多个字体就是为了防止浏览器有些字体没有</p><p>通用字体族 Serif 衬线体 Sans-Serif 无衬线体 Cursive 手写体 Monospace 等宽字体</p><p>使用font-family的建议 字体列表最后写上通用字体簇 英文字体放在中文字体前面</p><p>使用web fonts @font-face</p><p><strong>font-size 关键字</strong>， 长度， 百分数， 百分数时相对于父元素字体大小来的</p><p>长度px 像素， 2em 表示是父元素字体长度的两倍</p><p><strong>font-weight 设置粗细</strong></p><p><strong>font:</strong> style weight size/height family</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">h1</span><span class="token punctuation">&#123;</span>    <span class="token property">font</span><span class="token punctuation">:</span> bold 14px/1.7 Helvetica<span class="token punctuation">,</span> sans-serif<span class="token punctuation">;</span><span class="token punctuation">&#125;</span>行高是14px * 1.7<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="设置文字样式"><a href="#设置文字样式" class="headerlink" title="设置文字样式"></a>设置文字样式</h3><p>text-align 设置对齐模式， justify 是左右两边对齐</p><p>letter-spacing 是字母与字母之间的距离， word-spacing 是词与词之间的距离</p><p>text-shadow 设置文字阴影</p><h3 id="盒模型"><a href="#盒模型" class="headerlink" title="盒模型"></a>盒模型</h3><p>在浏览器进行渲染的时候 它会把页面中的元素格式化成一个一个盒子，每一个盒子都有padding, border, margin, content； content表示盒子里面内容的大小，padding是内容和边框的内边距，border是边宽，<strong>margin是盒子和其它盒子之间的距离</strong></p><ul><li><p>width 指定content box的宽度， 取值为长度，百分数-相对于容器的宽度来计算的，auto由浏览器根据其它属性确定，<strong>容器有指定高度时，百分数才生效</strong></p></li><li><p>padding-top, padding-left, padding-right, padding-bottom 默认值为0，百分数相对于容器宽度</p></li><li><p>border none表示没有边框，solid实线，dashed虚线，dotted 点状线； 指定容器边框样式、粗细和颜色</p><ul><li>border-width, border-style, border-style</li><li>或者 border: 1px solid #ccc</li><li>四个方向 border-top, border-right, border-bottom, border-left</li><li>border-left-width: 3px</li><li>可以利用border来模拟三角形</li></ul></li><li><p>margin</p><ul><li><p>使用margin: auto  水平居中</p></li><li><p>```css<br>div{</p><pre><code>width: 200px;height: 200px;backgroud: coral;margin-left: auto;margin-right: auto;</code></pre><p>}</p><pre class="line-numbers language-none"><code class="language-none">+ margin collapse  &#96;&#96;&#96;css  .a&#123;      background: lightblue;      height: 100px;      margin-bottom: 100px;  &#125;  .b&#123;      background: coral;      height: 100px;      margin-top: 100px;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">'</span>a<span class="token punctuation">'</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">'</span>b<span class="token punctuation">'</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>按照margin的定义两个div的垂直距离应该是200px，然而实际上是100px，因为在垂直方向上相邻的margin会被合并，取两者最大的值，这个现象只会在垂直方向上发生</p></li><li><p>margin可以为负值</p></li></ul></li><li><p>默认的height和width是关于content的，如果设置box-sizing为border-box 那么height和width就是关于content + border的</p></li><li><p>当容器的内容比较多的时候 就会溢出容器，称为overflow，overflow hidden表示超出的内容被隐藏，overflow scroll 表示内容超过的时候会有一个滚动条出现</p></li><li><p>当宽度是百分比或者auto的时候，其宽度我们并不能预先确定好，因为要根据浏览器宽度的大小和内容的多少，我们可以通过min-width和max-width 来限制实际的宽度</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">article</span><span class="token punctuation">&#123;</span>    <span class="token property">margin</span><span class="token punctuation">:</span> auto<span class="token punctuation">;</span>    <span class="token property">line-height</span><span class="token punctuation">:</span> 1.7<span class="token punctuation">;</span>    <span class="token property">max-width</span><span class="token punctuation">:</span> 20em<span class="token punctuation">;</span>    <span class="token property">min-width</span><span class="token punctuation">:</span> 10em<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面意味着一行最多20个字，最少10个字</p></li><li><p>min-height 和 max-height</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">article img</span><span class="token punctuation">&#123;</span>    <span class="token property">max-width</span> <span class="token punctuation">:</span> 100%<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="CSS中的盒子"><a href="#CSS中的盒子" class="headerlink" title="CSS中的盒子"></a>CSS中的盒子</h3><p>DOM树与盒子</p><p>根据DOM树生成一系列的盒子，然后摆放盒子 称作Layout</p><p>摆放盒子</p><ul><li>盒子尺寸和类型</li><li>定位模式</li><li>节点但在DOM树中的位置</li><li>其它信息 窗口大小、图片大小等</li></ul><p>块级元素会生成块级的盒子，行级元素会生成行级元素，<strong>块级的不能和其它盒子并列摆放</strong></p><p>行级盒子可以和其它的行级盒子摆放在一行或拆开成多行，<strong>盒模型中width、height不适用</strong></p><p>行级盒子的宽度是由内容和容器来决定的</p><p>行级元素有span、em、strong、cite、code等</p><p>可以通过display属性来把任意标签设置成行级或者块级，display: block, display:inline, </p><p>display:inline-block 本身是行级，可以放在行盒中；可以设置宽高；作为一个整体不会被拆散成多行</p><p>display:none 会被忽略不会被展示出来</p><h3 id="盒子的效果"><a href="#盒子的效果" class="headerlink" title="盒子的效果"></a>盒子的效果</h3><p>圆角border-radius —- 让我想到了小米的logo 可以通过设置border-radius来设置完成</p><p>圆角可以设置成圆形，也可以设置成椭圆 20px / 50px； 也可以设置成百分比</p><p>10px 20px 30px 40px / 50px 60px 70px 80px 前面是水平方向上的，后面是垂直方向上的</p><p>background</p><ul><li>background-color 背景颜色</li><li>background-image 设置背景图片</li><li>background-repeat 背景图片是否重复</li><li>background-position 背景图片所在位置</li><li>background-size 背景图片的大小</li></ul><p>background position的一个常用的使用场景</p><p>CSS Sprites 为了让页面加载的更快，我们会把小的一些背景图合并到一张大图中，然后通过background position来使用每一张小的图片</p><p>background size默认是图片的大小</p><p>background-size: 100px auto， 宽度是100px， 高度是根据宽度来调整的</p><p>background-size: auto 100% , 高度是100%， 宽度按照比例缩放</p><p>background-size: contain 表示图片能够完全的展示出来</p><p>background-size: cover 表示图片能够完全把容器覆盖住 这个时候图片可能会被裁剪</p><p>background-clip border-box， padding-box， content-box</p><p><strong>box-shadow</strong> : 1px 1px 8px 0 gray 前面两个值 是水平和垂直方向上一个偏移量，第三个值表示阴影模糊的程度，第四个值表示阴影扩张的大小，最后是阴影的颜色， <strong>不占用空间</strong></p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token property">box-shadow</span><span class="token punctuation">:</span> 0 0 0 2px red<span class="token punctuation">,</span>0 0 0 4px orange<span class="token punctuation">,</span>0 0 0 6px yellow<span class="token punctuation">,</span>            0 0 0 8px green<span class="token punctuation">,</span>            0 0 0 10px blue<span class="token punctuation">,</span>            0 0 0 12px cyan<span class="token punctuation">,</span>            0 0 0 14px purple<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个会形成一个彩虹边框</p><h3 id="行高和垂直对齐"><a href="#行高和垂直对齐" class="headerlink" title="行高和垂直对齐"></a>行高和垂直对齐</h3><p>Font Metrics： baseline-最重要， meanline， mean line减去baseline 是x-height 是小写字母的高度，有些字母比如h 会上超过mean line 或者比如p向下超过descender line 上界时ascender line 下界是descender line，盒子的摆放需要依赖这几条线来定位，一个包含文字的行盒的高度，line-height，ascender line到顶部和descender line到底部的距离是一样的，行盒的对齐默认是根据baseline</p><p>视频[5:00] 有详细的关于垂直对齐的介绍</p><p>vertical-align: 0px  是基于baseline来对齐</p><h3 id="CSS继承"><a href="#CSS继承" class="headerlink" title="CSS继承"></a>CSS继承</h3><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>article</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>title<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        abc    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>article</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">.title</span><span class="token punctuation">&#123;</span>        <span class="token property">color</span> <span class="token punctuation">:</span> blue<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">article h1</span><span class="token punctuation">&#123;</span>        <span class="token property">color</span> <span class="token punctuation">:</span> red<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>哪条规则生效？ 涉及到选择器的优先级 是由特殊程度来决定的</p><h1 id="nav-list-li-a-link-id-：1-伪-类-2-伪-元素-2"><a href="#nav-list-li-a-link-id-：1-伪-类-2-伪-元素-2" class="headerlink" title="nav .list li a:link       id ：1  $($伪$)$类: 2   $($ 伪$)$ 元素 : 2"></a>nav .list li a:link       id ：1  $($伪$)$类: 2   $($ 伪$)$ 元素 : 2</h1><p>.hd ul.links a              id ：0  $($伪$)$类: 2   $($ 伪$)$ 元素 : 2   上面是122 下面是22 上面优先级高</p><p>对于前面那个例子 blue 是10  red 是 2  所以是blue</p><p>高优先级的选择器的属性会覆盖低优先级的选择器中相同的属性的值</p><p>应用， 写一个按钮的基础样式， 然后再写特有的，利用属性覆盖实现代码复用</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>button</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>btn<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    普通按钮<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>button</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>button</span> <span class="token attr-name">class</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>btn primary<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    主要按钮<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>button</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">    <span class="token selector">.btn</span><span class="token punctuation">&#123;</span>        一些基本属性    <span class="token punctuation">&#125;</span>    <span class="token selector">.btn.primary</span><span class="token punctuation">&#123;</span>        特有属性    <span class="token punctuation">&#125;</span></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>某些属性会自动继承其父元素的计算值，除非显示指定一个值</strong></p><p>有些属性是不能继承的 比如box-sizing，但是我们可以显示继承，比如使用*</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">*</span><span class="token punctuation">&#123;</span>    <span class="token property">box-sizing</span> <span class="token punctuation">:</span> inherit<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>可以使用initial 关键字显示重制为初始值</p><p>[8:45]的图介绍了 CSS求值的过程</p><h3 id="CSS中的值和单位"><a href="#CSS中的值和单位" class="headerlink" title="CSS中的值和单位"></a>CSS中的值和单位</h3><p>常见的CSS的值有9种</p><ul><li>关键字 比如 initial、inherit</li><li>字符串 “Microsoft Yahei”</li><li>URL </li><li>长度 100px、5em</li><li>百分数</li><li>整数 z-index : 5</li><li>浮点数 line-height : 1.8</li><li>颜色 #ff0000</li><li>时间 300ms</li></ul><p>长度单位</p><p>px  像素点 ； in 英寸 ； cm 厘米 ； mm 毫米； pt 1/72英寸 ; pc 1/6 英寸</p><p>em 元素字体大小 ; rem html字体大小；vh 1%窗口高 ; vw 1%窗口宽 ; vmax vw vh较大者</p><p>[6:26]介绍了 HSL的颜色表示方式</p>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Continuous-Time Fourier Transform</title>
      <link href="2021/03/29/signal%20and%20system%208/"/>
      <url>2021/03/29/signal%20and%20system%208/</url>
      
        <content type="html"><![CDATA[<h2 id="Continuous-Time-Fourier-Transform"><a href="#Continuous-Time-Fourier-Transform" class="headerlink" title="Continuous-Time Fourier Transform"></a>Continuous-Time Fourier Transform</h2><p><a href="https://www.bilibili.com/video/BV1xy4y167DD?p=8">视频地址</a></p><h3 id="用复指数信号的线性组合来表示非周期信号"><a href="#用复指数信号的线性组合来表示非周期信号" class="headerlink" title="用复指数信号的线性组合来表示非周期信号"></a>用复指数信号的线性组合来表示非周期信号</h3><p>对于一个非周期信号，基于这个信号来建立周期信号，就是把原来的信号重复一下，</p><p>形成周期为$T_0$的周期信号，图片参考视频[03:22]的图片</p><p>这个周期信号在一个周期内是和非周期信号是相同的，<strong>当周期$T_0$趋近于无穷时，周期信号变成了非周期信号</strong> </p><p><strong>核心思想</strong>：用傅里叶级数来表示这个周期信号，然后让周期趋近于无穷，得到的就是非周期信号的傅里叶变换</p><script type="math/tex; mode=display">\widetilde{x}(t) = x(t) \quad for \ |t| <\frac{T_0}{2} \\\widetilde{x}(t) = \sum_{k = -\infty}^{\infty}a_ke^{jk\omega_0t} \quad \quad \omega_0 = \frac{2\pi}{T_0} \\a_k = \frac{1}{T_0}\int_{-\frac{T_0}{2}}^{\frac{T_0}{2}}\widetilde{x}(t)e^{-jk\omega_0t}dt \\a_k = \frac{1}{T_0}\int_{-\infty}^{+\infty}x(t)e^{-jk\omega_0t}dt \\</script><p>Define: $X(\omega) \triangleq \int_{-\infty}^{+\infty}x(t)e^{-j\omega t}dt \tag{1}$       Then: $T_0a_k = X(\omega)\bigg|_{\omega = k\omega_0} \tag{*}$</p><p>$X(\omega)$ is the envelope$($ 包络函数 $)$ of $T_0a_k$ ； 这告诉了我们如何通过对包络函数的sample来建立周期信号的傅里叶级数系数</p><script type="math/tex; mode=display">\widetilde{x}(t) = \sum_{k = -\infty}^{+\infty}a_ke^{jk\omega_0t} = \sum_{k = -\infty}^{+\infty}\frac{1}{T_0}X(k\omega_0)e^{jk\omega_0t} \\\widetilde{x}(t) = \frac{1}{2\pi}\sum_{k = -\infty}^{+\infty}X(k\omega_0)e^{jk\omega_0t}\omega_0 \\As \ T_0 \rightarrow \infty, \\\omega_0 \rightarrow0,\widetilde{x}(t) \rightarrow x(t), \omega_0 \rightarrow d\omega, \Sigma \rightarrow \int \\</script><p>则得到 $x(t) = \frac{1}{2\pi}\int_{-\infty}^{+\infty}X(\omega)e^{j\omega t}d\omega \tag{2}$</p><p>方程$(1)$ 称为analysis equation   也成为傅里叶变换                       方程$(2)$称为synthesis equation</p><p>[11:00]开始的图示说明非常棒！！</p><p>$x(t)$和$X(\omega)$的关系是一一对应，即使$x(t)$是实函数，$X(\omega)$也是复函数</p><p>$X(\omega) = Re\{ X(\omega) \} + j Im \{  X(\omega) \} = |X(\omega)|e^{j\angle X(\omega)}$</p><h3 id="傅里叶变换的例子"><a href="#傅里叶变换的例子" class="headerlink" title="傅里叶变换的例子"></a>傅里叶变换的例子</h3><p>对于函数$x(t) = e^{-at}u(t)$ 其傅里叶变换是</p><p>$X(\omega) = \int_{-\infty}^{+\infty}x(t)e^{-j\omega t}dt = \int_{0}^{+\infty}e^{-at}e^{-j\omega t}dt = \int_{0}^{+\infty}e^{-t(a + j\omega)}dt = \frac{1}{a + j\omega}e^{-(a + j\omega)t}\bigg|^\infty_0$</p><p>当$a &gt; 0$ 积分求$t = +\infty$时 式子为0，当$t = 0$时 式子为$\frac{1}{a + j\omega}$   可得$e^{-at}u(t)$对应的傅里叶变换是$\frac{1}{a + j\omega}$</p><p>[18:30] 展示了 该函数与其傅里叶变换形成的函数的图像(有振幅的图，有幅度的图)</p><p>[21:30] 展示了该图像的波特图版本 振幅是$20log_{10}X(\omega)\triangleq decibels \ (dB)$ 横坐标是频率的对数，对于横坐标上面等距离移动，每次移动频率变为原来的10倍</p><p>由于横坐标是对数，所以对应的频率是大于0的频率，但是由于傅里叶变换出的函数的性质 我们可以推测出频率为负时的图像的样子</p><p><strong>Fourier Series coefficients equal $\frac{1}{T_0}$ times samples of Fourier transform of one period</strong></p><p>也就是说 对于之前求周期信号的傅里叶级数 我们可以考虑求一个周期的傅里叶变换[25:30]</p><p>可以可以！</p><h3 id="将傅里叶变换和傅里叶级数用一个通用框架来表示"><a href="#将傅里叶变换和傅里叶级数用一个通用框架来表示" class="headerlink" title="将傅里叶变换和傅里叶级数用一个通用框架来表示"></a>将傅里叶变换和傅里叶级数用一个通用框架来表示</h3><p>观察傅里叶级数和傅里叶变换的synthesis equation 我们可以发现</p><p>$\widetilde{x}(t)$的傅里叶级数是$a_k$  $\widetilde{x}(t)$的傅里叶变换是$\widetilde{X}(\omega)$ </p><p>则$\widetilde{X}(\omega) \triangleq \sum_{-\infty}^{\infty}2\pi a_k \delta(\omega - k\omega_0) \tag{3}$</p><script type="math/tex; mode=display">\widetilde{x}(t) = \frac{1}{2\pi}\int_{-\infty}^{+\infty}\widetilde{X}(\omega)e^{j\omega t}d\omega \\= \frac{1}{2\pi}\sum_{k = -\infty}^{+\infty}2\pi a_k \int_{-\infty}^{+\infty}\delta(\omega - k\omega_0)e^{-j\omega t}d\omega</script><h2 id="通用框架的例子"><a href="#通用框架的例子" class="headerlink" title="通用框架的例子"></a>通用框架的例子</h2><p>symmetric square wave     666哇</p><p>还讲了一个$x(t) = \delta(t)$的例子 $x(t)$的傅里叶变换是常数，</p><p>对于周期函数$\widetilde{x}(t) = \sum_{k = -\infty}^{+\infty}\delta(t - kT_0)$ 其对应的傅里叶变换$\widetilde{X}(\omega)$  这函数是怎么得到的 没有弄明白</p>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fourier Transform Properties</title>
      <link href="2021/03/29/signal%20and%20system%209/"/>
      <url>2021/03/29/signal%20and%20system%209/</url>
      
        <content type="html"><![CDATA[<h2 id="Fourier-Transform-Properties"><a href="#Fourier-Transform-Properties" class="headerlink" title="Fourier Transform Properties"></a>Fourier Transform Properties</h2><p><a href="https://www.bilibili.com/video/BV1xy4y167DD?p=9">视频链接</a></p><p><strong>Continuous —- Time Fourier Transform</strong></p><script type="math/tex; mode=display">x(t) = \frac{1}{2\pi}\int_{-\infty}^{+\infty}X(\omega)e^{j\omega t}d\omega \quad \quad  sysnthesis \\X(\omega) = \int_{-\infty}^{+\infty}x(t)e^{-j\omega t}dt \quad \quad analysis</script><h3 id="Symmetry"><a href="#Symmetry" class="headerlink" title="Symmetry"></a>Symmetry</h3><script type="math/tex; mode=display">x(t) \ real \ \ \Rightarrow \ X(-\omega) = X^*(\omega) \\ReX(\omega) = ReX(-\omega) \quad |X(\omega)| = |X(-\omega)| \quad</script><p>the real part is an even function of frequency, and the magnitude is an even function of frequency</p><script type="math/tex; mode=display">Im X(\omega) = -ImX(-\omega) \quad \angle X(\omega) = -\angle X(-\omega)</script><p>the imaginary part is an odd function of frequency, and the phase angle is an odd function of frequency</p><h3 id="Time-and-frequency-scaling"><a href="#Time-and-frequency-scaling" class="headerlink" title="Time and frequency scaling"></a>Time and frequency scaling</h3><script type="math/tex; mode=display">if \ x(t) \iff X(\omega) \quad then \quad x(at) \iff \frac{1}{|a|}X(\frac{\omega}{a})</script><p><strong>如果以两倍的速度播放磁带，意味着2倍线性压缩时间轴，然而实际发生的是频率变成2倍</strong></p><p>[09:07]实验展示非常好</p><h3 id="duality-relationship"><a href="#duality-relationship" class="headerlink" title="duality relationship"></a>duality relationship</h3><p>看最开始的synthesis equation 和 analysis equation</p><p>如果$X(\omega)$是时间信号$x(t)$的傅里叶变换，那么$x(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}X(\omega)e^{j\omega t} d\omega$  非常像傅里叶变换 变换的是$X(-\omega)$，再加上一个额外的因子$\frac{1}{2\pi}$  即我们可得对偶性</p><script type="math/tex; mode=display">x(t) \iff X(\omega) \quad \quad X(t) \iff 2\pi x(-\omega)</script><p><strong>这个性质非常重要！</strong> 对于离散傅里叶变换 这个性质是没有的 [15:00]显示了例子</p><h3 id="Parseval’s-relation"><a href="#Parseval’s-relation" class="headerlink" title="Parseval’s relation:"></a>Parseval’s relation:</h3><script type="math/tex; mode=display">\int_{-\infty}^{+\infty}|x(t)|^2dt = \frac{1}{2\pi}\int_{-\infty}^{+\infty}|X(\omega)|^2d\omega \\\frac{1}{T_0}\int_{T_0}|\widetilde{x}(t)|^2dt = \sum_{-\infty}^{+\infty}|a_k|^2</script><p>下面这个式子 是对于周期信号，由于周期信号的能量是无穷大，所以对于周期信号 只考虑一个周期的能力，$\widetilde{x}(t)$是一个周期的信号</p><h3 id="Time-shifting"><a href="#Time-shifting" class="headerlink" title="Time shifting:"></a>Time shifting:</h3><script type="math/tex; mode=display">x(t - t_0) \iff e^{-j\omega t_0}X(\omega)</script><p>右边的$e^{j\omega t_0}$的幅度为1，并且有一个与频率成线性关系的相位</p><p>即 a time shift corresponds to a linear change in phase and frequency</p><h3 id="Differentiation"><a href="#Differentiation" class="headerlink" title="Differentiation:"></a>Differentiation:</h3><script type="math/tex; mode=display">\frac{dx(t)}{dt} \iff j\omega X(\omega)</script><h3 id="Integration"><a href="#Integration" class="headerlink" title="Integration:"></a>Integration:</h3><script type="math/tex; mode=display">\int_{-\infty}^tx(\tau)d\tau \iff \frac{1}{j\omega}X(\omega) + \pi X(0)\delta(\omega)</script><p>因为对函数求导的时候会把常数项给消掉，所以在积分性质中 就尝试把常数项带回来 所以后面加上$\pi X(0)\delta(\omega)$</p><h3 id="Linearity"><a href="#Linearity" class="headerlink" title="Linearity:"></a>Linearity:</h3><script type="math/tex; mode=display">ax_1(t) + bx_2(t)  \iff aX_1(\omega) +bX_2(\omega)</script><p>老师说 这四个性质 for the most part, apply both to Fourier series and Fourier transforms, because what we’ve done is to incorporate the Fourier series within the framework of the Fourier transform.</p><h3 id="Two-addtional-major-properties-convolution-property-and-modulation-property"><a href="#Two-addtional-major-properties-convolution-property-and-modulation-property" class="headerlink" title="Two addtional major properties: convolution property and modulation property"></a>Two addtional major properties: convolution property and modulation property</h3><p><strong>the convolution property forms the mathematical and conceptual basis for the whole notion of filtering</strong></p><script type="math/tex; mode=display">h(t) * x(t) \iff H(\omega)X(\omega)</script><p>LTI中，对于输入信号是$\delta(t)$ 冲激响应是$h(t)$ 由于$\delta(t)$的傅里叶变换是常数1，所以输出信号的傅里叶变换是$H(\omega)$</p><p>对于卷积性质的更直观的解释是：（这个解释来源于冲激响应的傅里叶变换和频率响应之间的关系）</p><p>LTI中 如果输入信号是$e^{j\omega_0 t}$ 那么输出信号是$e^{j\omega_0 t}H(\omega_0)$ 这里的$H(\omega_0)$被称为频率响应 这个频率响应就是冲激响应的傅里叶变换</p><p>老师说 “the synthesis equation tells us how to decompose x of t as a linear combination of complex exponentials. What are the complex amplitudes of those complex exponentials? The complex amplitude of those complex exponentials is $X(\omega)$ [参考26:53的图] or proportional to x of omega $(2\pi X(\omega)d\omega)$,  as this signal goes through this linear time invariant system, what happens to each of those exponential is each one get multiplied by the frequency response at associated frequency. What comes out is the amplitude of the complex exponentials that are used to build the output</p><p>So in fact, the convolutional property simply is telling us that in terms of decomposition of the signal in terms of complex exponentials as we push that signal through a linear time invariant system, we’re separately multiplying by the frequency response the amplitudes of the exponential components used to build the input. And that sum in turn is the decomposition of the output in terms of complex exponentials.”  要尝试在概念性的程度上理解卷积特性 </p><p><strong>modulation property</strong></p><p>因为time domain和frequency domain是interchangeable 因为duality 这意味着</p><p><strong>if we multiply in the time domain, that would correspond to convolution in the frequency domain</strong>   666</p><script type="math/tex; mode=display">Modulation: \quad s(t)p(t) \iff \frac{1}{2\pi}[S(\omega)*P(\omega)] \\Convolution: \quad s(t)*p(t) \iff S(\omega)P(\omega)</script><p>对于modulation property 它是the entire basis for amplitude modulation systems as used almost universally in communications</p><p>如果we have a signal with a certain spectrum, and we multiply by a sinusoidal signal whose Fourier transform is a set of impulses, then in a frequency domain we convolve. And that corresponds to taking the original spectrum and translating it shifting it in frequency up to the frequency of the carrier.</p><h3 id="filtering的简单介绍"><a href="#filtering的简单介绍" class="headerlink" title="filtering的简单介绍"></a>filtering的简单介绍</h3><p>filtering 意思是modify separately the individual frequency components in a signal</p><p>卷积特性告诉我们 如果我们看各个频率分量，它们被乘以频率响应 那么意味着 我们可以放大或减小任何一个components separately using a linear time invariant system</p><p><strong>Ideal lowpass filter</strong> [29:54]的图  </p><p><strong>differentiator</strong> $y(t) = \frac{dx(t)}{dt} \Rightarrow H(\omega) = j\omega $ [31:10] 图，它的作用是放大高频率 衰减低频</p><p>举个例子：对于空间信号，它有高频部分和低频部分，高频对应things that are varying rapidly spatially, and low frequencies varying slowly spatially. 对[32:42]的图 进行低通滤波 就是问问视频摄像组是否可以稍微defocus it. 图像散焦[32:53] 也就是失去边缘 lost edges，edges指的是rapid variation， 我们保留的是broader slow variation.</p><p>如果想要图像的边缘增强，那么我们就differentiated the image 这个会attenuate slowly varying background and amplify the rapidly varying edges.[35:15]是经过了differentiator的图</p><h3 id="用傅里叶变换解线性常系数微分方程"><a href="#用傅里叶变换解线性常系数微分方程" class="headerlink" title="用傅里叶变换解线性常系数微分方程"></a>用傅里叶变换解线性常系数微分方程</h3><script type="math/tex; mode=display">\frac{dy(t)}{dt} + ay(t) = x(t) \\\Rightarrow j\omega Y(\omega) + aY(\omega) = X(\omega) \\\Rightarrow Y(\omega) = \frac{1}{j\omega + a}X(\omega) \\\Rightarrow h(t) \iff H(\omega) \\\Rightarrow e^{-at}u(t) \iff \frac{1}{j\omega + a}</script><p>这里用到了 differential property 和 linearity property</p><p>看到$\frac{1}{j\omega + a}$的图像 是减弱高频 保持低频，之前的将图像defocus 就相当于将图像通过这个线性微分方程</p><p>另一个例子：</p><script type="math/tex; mode=display">\frac{dy(t)}{dt} + 2y(t) = e^{-t}u(t) \\j\omega Y(\omega) + 2Y(\omega) = \frac{1}{j\omega + 1} \\Y(\omega) = \frac{1}{j\omega + 2}\frac{1}{j\omega + 1} = \frac{-1}{j\omega + 2} + \frac{1}{j\omega + 1} \\\iff -e^{-2t}u(t) + e^{-t}u(t) = y(t)</script>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kernels</title>
      <link href="2021/03/22/cs229%207/"/>
      <url>2021/03/22/cs229%207/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-7-Kernels"><a href="#Lecture-7-Kernels" class="headerlink" title="Lecture 7 Kernels"></a>Lecture 7 Kernels</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=10">视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes3.pdf">讲义链接</a></p><p><a href="https://link.springer.com/article/10.1007/BF00994018">SVM论文链接</a></p><p>感觉关于SVM的数学对于我来说还是挺硬核的</p><h3 id="视频笔记"><a href="#视频笔记" class="headerlink" title="视频笔记"></a>视频笔记</h3><p>这节课的主要内容是 <strong>optimization problem, representer theorem, kernels, examples of kernel</strong></p><p>optimal margin classifier plus kernels —-&gt; Support Vector Machine</p><p>关于上次课所讲的optimal margin classifer的优化式子为</p><script type="math/tex; mode=display">max_{\gamma, \omega, b} \gamma \\s.t. \frac{y^{(i)}(\omega^Tx^{(i)} + b)}{||\omega||} \ge \gamma \quad i = 1, \cdots, m</script><p><strong>下面的限制 表示 每个训练数据的geometric margin 都要比$\gamma$大</strong></p><p>因为同时对$\omega$和$b$进行常数倍乘，直线$\omega^Tx^{(i)} + b$是不会变的，我们可以设置$||\omega|| = \frac{1}{\gamma}$  六六六，得到</p><script type="math/tex; mode=display">\max \frac{1}{||\omega||} \\s.t. y^{(i)}(\omega^Tx^{(i)} + b)\gamma \ge \gamma</script><p>等价于</p><script type="math/tex; mode=display">min_{\omega,b} \frac{1}{2} ||\omega||^2 \\s.t. y^{(i)}(\omega^Tx^{(i)} + b) \ge 1 \quad i = 1, \cdots, m \tag{1}</script><p><strong>关于Kernels的部分 讲义写的非常好了</strong> $($这个思想贼棒$)$</p><p>假设 $\omega = \sum_{i  = 1}^m \alpha_i y^{(i)}x^{(i)}$ !!! 这个假设能让我们设计出可以有效解决高维特征的算法</p><p><strong>有一个叫做representer的定理 表示做出这个假设 我们不会损失任何性能</strong></p><p>假设成立时，通过$($batch $)$gradient descent来更新$\omega$，无论更新多少次，$\omega$永远都可以被$x^{(i)}$线性表示</p><p>这个可以用数学归纳法证明出来， 具体参考讲义里面的内容</p><p>根据这个假设来改写方程$(1)$</p><script type="math/tex; mode=display">\min_\alpha \frac{1}{2}(\sum_{i = 1}^m\alpha_iy^{(i)}x^{(i)})^T(\sum_{j = 1}^m\alpha_jy^{(j)}x^{(j)}) \\= \min_\alpha \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy^{(i)}y^{(j)}(x^{(i)})^Tx^{(j)} \\denote \quad (x^{(i)})^Tx^{(j)} \ as \ \langle x^{(i)},x^{(j)}\rangle \\s.t. \quad y^{(i)}((\sum_j\alpha_jy^{(j)}x^{(j)})^Tx^{(i)} + b) \ge 1 \\y^{(i)}(\sum_j\alpha_jy^{(j)}\langle x^{(j)}, x^{(i)}\rangle + b) \ge 1 \tag2</script><p>如果能计算$\langle x^{(j)}, x^{(i)} \rangle$  那么就能处理高维特征向量的情况</p><p>$(2)$可以继续改写，通过dual optimization，得到 讲义中的方程$(24)$ </p><p>进行预测 就是</p><script type="math/tex; mode=display">h_{\omega, b}(x) = g(\omega^Tx + b) = g((\sum_i\alpha_iy^{(i)}x^{(i)})^Tx + b) \\ = g(\sum_i\alpha_iy^{(i)} \langle x^{(i)}, x^{(j)}\rangle + b)</script><h4 id="Kernel-trick："><a href="#Kernel-trick：" class="headerlink" title="Kernel trick："></a>Kernel trick：</h4><p>1$)$  Write algorithm in terms of $\langle x^{(i)}, x^{(j)} \rangle$ or $\langle x, z \rangle$ x和z表示不同的训练数据</p><p>2$)$ Let there be mapping from $x \rightarrow \phi(x)$</p><p>3$)$ Find way to compute the kernel function $K(x, z) = \phi(x)^T\phi(z)$</p><p>4$)$ Replace $\langle x, z \rangle$ in algorithm with $K(x, z)$</p><p>总结kernel的作用：</p><p>如果训练集的特征是n维向量$x \in \mathcal{R}^n$ 将$x \rightarrow \phi(x)$ 之后维数增大，比如变成了$n^2$ 那么计算</p><p>$\phi(x)^T\phi(z)$的时间复杂度就是$O(n^2)$ <strong>如果用kernel 即用$\langle x, z \rangle$来表示出$\phi(x)^T\phi(z)$ </strong>那么时间复杂度就变成了$O(n)$   用实际的例子来说明 $x = (x_1, x_2, x_3)^T , \phi(x) = (x_1x_1,x_1x_2,x_1x_3,x_2x_1,x_2x_2,x_2x_3,x_3x_1,x_3x_2,x_3x_3)^T$ 那么kernal $\mathcal{K}( x, z ) = \phi(x)^T\phi(z) = (x^Tz)^2$  [41:30] 介绍了另一个kernel $\mathcal{K}(x,z) = (x^Tz + c)^2$</p><p>对于kernel $\mathcal{K}(x, z) = (x^Tz + c)^d$  计算这个kernel的时间复杂度还是$O(n)$，$\phi(x)$ has all  ${n+d\choose d}$ features of monomial up to order d</p><p>[47:30] 开始是SVM with a polynomial Kernel visualization (没看懂 为什么映射到二维平面映射出的是非线性边界线) </p><p>用kernel的作用是 希望能将feature映射到高维空间中，希望在低维线性不可分的数据 在高维中变得线性可分</p><p>常见的几个核： Linear Kernel $\mathcal{K}(x, z) = x^Tz$   Gaussian Kernel</p><p>[64:46] 如果数据有点noisy，想要找到一个复杂的非线性边界线，不希望try so hard to separate every little example that’s defined a really complicated decision boundary. So sometimes either the low-dimensional space or in the high dimensional space $\phi$， you don’t actually want the algorithms to separate out your data perfectly and then sometimes even in hight dimensional space your data may not be linearly separable.</p><p>$L_1$范数soft margin SVM  式子就变成了讲义25页上面的那个式子</p><p>[68:30] 举了soft margin SVM的作用 作用之一是为了解决outliner的情况 和 解决距离边界线非常近的例子</p><h3 id="SVM补充"><a href="#SVM补充" class="headerlink" title="SVM补充"></a>SVM补充</h3><p><a href="https://blog.csdn.net/Gamer_gyt/article/details/51265347">scikit-learn之SVM算法</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine</title>
      <link href="2021/03/22/cs229%206/"/>
      <url>2021/03/22/cs229%206/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-6-Support-Vector-Machine"><a href="#Lecture-6-Support-Vector-Machine" class="headerlink" title="Lecture 6 Support Vector Machine"></a>Lecture 6 Support Vector Machine</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=8">视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes3.pdf">讲义链接</a></p><p>虽然这课叫support vector machine，但是只有最后半节课讲了support vector machine</p><h3 id="Laplace-Smoothing"><a href="#Laplace-Smoothing" class="headerlink" title="Laplace Smoothing"></a>Laplace Smoothing</h3><p>如果出现了一个单词，这个单词在之前的训练集里面没有出现过，那么按照朴素贝叶斯</p><script type="math/tex; mode=display">\mathcal{P}(x_{6017} = 1 | y = 1) = \frac{0}{\# \{ y = 1\}} = 0 \\\mathcal{P}(x_{6017} = 1 | y = 0) = \frac{0}{\# \{ y = 0\}} = 0</script><p>从统计学上说，说一个没有看见过的事情的出现概率是0 是 一个坏主意</p><p>使朴素贝叶斯崩溃的地方是 如果根据上面的式子得到的参数<script type="math/tex">\phi_{6017| y = 1} = 0</script> 和<script type="math/tex">\phi_{6017|y = 0} = 0</script></p><p>用这个参数进行估计的时候，由于式子是<script type="math/tex">\prod_{i = 1}^{10000} \mathcal{P}(x_i | y)</script> 有一项为0 结果就为0</p><p>进行预测<script type="math/tex">\mathcal{P}(y = 1 | x) = \frac{\mathcal{P(x|y = 1)}\mathcal{P}(y = 1)}{\mathcal{P}(x | y = 1)\mathcal{P}(y = 1) + \mathcal{P}(x | y = 0 )\mathcal{P}(y = 0)} = \frac{0}{0 + 0}</script></p><p><strong>拉普拉斯平滑就是用来解决这个问题</strong></p><p>老师用一个motivation来引入拉普拉斯平滑：</p><p>足球队在9.12 10.10 10.17 11.21 四场球赛都输掉了 预测12.31号的足球比赛</p><p>如果用MLE的方法<script type="math/tex">\mathcal{P}(x = 1) = \frac{\# '1's}{\#'1's + \#'0's} = \frac{0}{0 + 4} = 0</script></p><p>拉普拉斯 就是把<script type="math/tex">\#'1'</script>和<script type="math/tex">\#'0'</script>比原来多1 那么<script type="math/tex">\mathcal{P}(x = 1) = \frac{\# '1's}{\#'1's + \#'0's} = \frac{(0 + 1)}{(0+1) + (4+1)} = \frac{1}{6}</script></p><p><strong>对于more generally的情况</strong></p><p>Estimate <script type="math/tex">\mathcal{P}(x = j) = \frac{\sum_{j = 1}^m1\{x^{(i)} = j\} + 1}{M + k}</script></p><p>对于Naive Bayes：</p><script type="math/tex; mode=display">\phi_{j|y = 0} = \frac{\sum_{i = 1}^m1\{x_j^{(i)} = 1,y^{(i)} = 0\} + 1}{\sum_{i = 1}^m1\{y^{(i)} = 0 \} + 2}</script><p><strong>Tips: 将连续的特征转化为离散的特征:</strong> 将连续的数值划分称若干个区间，然后对每个区间进行标注</p><p>根据经验 一般将连续的feature 分成10个bucket 效果比较好</p><p>用naive bayes来进行text classfication的一个缺点是 naive bayes丢掉了一个单词出现次数的信息</p><p>因为每个特征<script type="math/tex">x_i \in \{0, 1\}</script> <script type="math/tex">(Multi-variate Bernoulli)</script></p><p>有一种另外的表现方式 是专门用于text data的 每个特征<script type="math/tex">x_i \in \{1, \cdots, \#classes \}（Multinomial）</script> </p><p>用第二种模型的出来的式子是<script type="math/tex">\mathcal{P}(x, y) = \mathcal{P}(x | y)\mathcal{P}(y) = \prod_{j = 1}^n \mathcal{P}(x_j|y)\mathcal{P}(y)</script></p><p> <strong>与第一种模型的区别是 这里的n 是邮件里面的字数 而非所有单词的数量</strong></p><p>参数<script type="math/tex">\phi_{k|y = 0} = \mathcal{P}(x_j = k | y = 0)</script> 其中<script type="math/tex">\mathcal{P}(x_j = k|y = 0)</script>的表示的是 如果<script type="math/tex">y = 0</script> 那么第j个单词为k的概率为<script type="math/tex">\mathcal{P}(x_j = k | y = 0)</script></p><p>值得注意的是参数<script type="math/tex">\phi_{k|y=0}</script>中j并没有出现，原因是对于邮件中每个位置 第一个单词出现drugs的机会和第二个单词出现drugs的机会相同 $($drugs是随便一个单词$)$</p><p>根据MLE 求得的结果为:</p><script type="math/tex; mode=display">\phi_{k|y = 0} =  \frac{\sum_{i = 1}^m1\{y^{(i)} = 0\}\sum_{j = 1}^n1\{x_j^{(i)} = k\} + 1}{\sum_{i = 1}^m1\{y^{(i)} = 0\}n_i + \# words}</script><p><strong>这个公式 分母是训练集中所有non-spam的单词和，<script type="math/tex">n_i</script>是第<script type="math/tex">i</script>个non-spam文档的单词数目, 分子是训练集中non-spam中k单词出现的个数</strong></p><p>如果遇见不在words里面的单词，一种方法是直接丢弃，另一种方法是take the rare words and map them to a special token which traditionally is denoted UNK for unknown words.</p><p><strong>Tips: 如果要写一个包含10000行代码的程序，一种方法是先写10000行代码 然后进行第一次编译，另一种方法是编写小模块，运行小模块然后进行测试，最后构建出程序，然后开始看看第一次遇到什么语法错误；对于机器学习也是一样 一开始建立简单模型 测试 然后看问题出在哪 然后进行改进</strong></p><h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><p>如果要找一个非线性的边界线，支持向量机可以帮助我们找到潜在的非线性的决策边界</p><p>如果用logistc regression来得到一个非线性的边界，一般用的方法是对feature进行扩展</p><p>比如<script type="math/tex">x_1,x_2</script>变为<script type="math/tex">x_1,x_2,x_1^2,x_2^2,x_1x_2</script>  即原来的向量<script type="math/tex">(x_1,x_2)^T</script>变成了<script type="math/tex">\phi(x) = (x_1,x_2,x_1^2,x_2^2,x_1x_2)^T</script></p><p>high dimensional features 选择这些featrues是很痛苦的一件事情</p><p>如果是support vector machine的话，就能够推导出输入是特征<script type="math/tex">x_1,x_2</script>的算法，把它们映射到更高维度的features</p><p>老师说 support vector machines are not as effective as neural networks for many problems, but one great property of support vector machine is turn key. You kind of just turn the key and it works and there isn’t as many parameters like the learning rate and other things that you had to fiddle with</p><h4 id="Optimal-margin-classfier-separable-case"><a href="#Optimal-margin-classfier-separable-case" class="headerlink" title="Optimal margin classfier(separable case)"></a>Optimal margin classfier(separable case)</h4><p>Defn: Functional margin 对于logistic function <script type="math/tex">h_\theta(x) = g(\theta^Tx)</script> 输出的是0和1 而不是概率</p><p>logistic regression的预测是</p><p>if <script type="math/tex">\theta^Tx \ge 0 ( h_\theta(x) = g(\theta^Tx) \ge 0.5)</script> predict 1 otherwise predict 0</p><p>If <script type="math/tex">y^{(i)} = 1</script>, hope that <script type="math/tex">\theta^Tx^{(i)} \gg 0</script>   If <script type="math/tex">y^{(i)} = 0</script>, hope that <script type="math/tex">\theta^Tx^{(i)} \ll 0</script>  这就是functional margin（没懂啊）</p><p>Defn: Geometric margin</p><p>就是边界线离训练数据的距离</p><p>[60:00]的图 告诉我们对于线性可分的数据集，我们可以得到多个分界线 比如红色的和绿色的，SVM就是帮助我们找到绿色的分界线</p><h5 id="下面是正式的定义"><a href="#下面是正式的定义" class="headerlink" title="下面是正式的定义"></a>下面是正式的定义</h5><p>Notation: labels $y\in\{-1, +1\}$ have h output value in $\{-1, +1\}$</p><script type="math/tex; mode=display">g(z) = \begin{cases}1& if \ z \ge 0 \\-1& otherwise\end{cases}</script><p>Previously $h_\theta(x) = g(\theta^Tx)$ 其中$x \in \mathcal{R}^{n + 1}, x_0 = 1$</p><p>在SVM中 $h_{\omega, b}(x) = g(\omega^Tx + b )$  其中$x \in \mathcal{R}^n, b \in \mathcal{R}$ drop convetions $x_0 = 1$</p><p>Functional margin of hyperplane defined by $(\omega, b)$ w.r.t. $(x^{(i)}, y^{(i)})$</p><script type="math/tex; mode=display">\hat{\gamma}^{(i)} = y^{(i)}(\omega^Tx^{(i)} + b) \tag{1}</script><p>为了要large functional margin 即 如果$y^{(i)} = $1 我们想要$\omega^Tx^{(i)} + b \gg 0 $ 如果$y^{(i)} = -1$ 我们想要 $\omega^Tx^{(i)} + b \ll 0$， 由式子$(1)$ 等价于要$\gamma$非常的大</p><p>functional margin w.r.t. training set $\hat{\gamma} = \min_i\hat{\gamma}^{(i)} \ i = 1,\cdots, m$</p><p><strong>观察到如果将$\omega$和b 加倍 那么function margin也会加倍</strong>  所以我们要规范化长度即$||\omega|| = 1$</p><p>那么$(\omega, b) \rightarrow (\frac{\omega}{||\omega||}, \frac{b}{||b||})$ 实际上 分母可以取其他的值</p><h5 id="Geometric-margin："><a href="#Geometric-margin：" class="headerlink" title="Geometric margin："></a>Geometric margin：</h5><p>[73:44]的图 那张图清晰的定义了geometric margin</p><p>Geometric margin of hyperplane $(\omega, b)$ w.r.t. $(x^{(i)}, y^{(i)})$ is </p><script type="math/tex; mode=display">\gamma^{(i)} = \frac{y^{(i)}(\omega^Tx^{(i)} + b)}{||\omega||} \\\gamma^{(i)} = \frac{\hat{\gamma}^{(i)}}{||\omega||}</script><p>geometric margin w.r.t. training set $\gamma = \min_i\gamma^{(i)}$</p><p><strong>那么 optimal margin classifier 就是choose $\omega, b$ to maximize $\gamma$</strong></p><p>解决这个问题的方法之一是 </p><script type="math/tex; mode=display">max_{\gamma, \omega, b} \gamma \\s.t. \frac{y^{(i)}(\omega^Tx^{(i)} + b)}{||\omega||} \ge \gamma \quad i = 1, \cdots, m</script><p>这不是一个凸优化问题 所以很难用梯度下降来解决</p><p>我们把这个式子 改变一下，改成一个凸优化问题</p><script type="math/tex; mode=display">\min_{\omega, b} ||\omega||^2 \\s.t. y^{(i)}(\omega^T x^{(i)} + b) \ge 1</script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Continuous-Time Fourier Series</title>
      <link href="2021/03/22/signal%20and%20system%207/"/>
      <url>2021/03/22/signal%20and%20system%207/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-7-Continuous-Time-Fourier-Series"><a href="#Lecture-7-Continuous-Time-Fourier-Series" class="headerlink" title="Lecture 7 Continuous-Time Fourier Series"></a>Lecture 7 Continuous-Time Fourier Series</h2><p>在一个线性系统中如果输入可被分解为a linear combination of basic input，with each of these basic inputs generating an associated output</p><script type="math/tex; mode=display">x(t) = a_1\phi_1(t) + a_2\phi_2(t) + \cdots \\\phi_k(t) \rightarrow \psi_k(t) \quad and \ system\ is \ linear \\Then:y(t) = a_1\psi_1(t) + a_2\psi_2(t) + \cdots</script><p><strong>对于基本信号$\phi_k(t)$的选择 首先是a broad class of signals could be represented in terms of these basic inputs 其次是 the response to these basic inputs is easy to compute.</strong></p><p><strong>核心的两个性质</strong></p><p>在LTI系统中我们的选择是</p><p>C-T: $\phi_k(t) = \delta(t - k\Delta)$  和 $\psi_k(t) = h(t - k\Delta) \Rightarrow$ Convolution Integral</p><p>D-T: $\phi_k[n] = \delta[n - k]$ 和$\psi_k[n] = h[n - k] \Rightarrow$ Convolution Sum</p><p>新的building blocks是complex exponentials$($ 拥有上述两个性质 $)$ </p><p>$\phi_k(t) = e^{s_kt}$  $s_k$ 是complex， $\phi_k[n]$ = $z_k^n$  $z_k$ 是complex</p><p><strong>Fourier Analysis:</strong></p><p>C-T: $s_k = j\omega_k$      $\phi_k(t) = e^{j\omega_kt}$  </p><p>D-T: $|z_k| = 1$        $\phi_k[n] = e^{j\Omega_kn}$</p><p>这里对于$s_k$ 和 $z_k$ 是复数的特殊情况</p><p><strong>一般性的化 $s_k$ complex $\Rightarrow$ Laplace transforms   $z_k$ complex $\Rightarrow$ z-transforms</strong></p><h3 id="Eigenfunction-property-of-this-particular-set-of-building-blocks"><a href="#Eigenfunction-property-of-this-particular-set-of-building-blocks" class="headerlink" title="Eigenfunction property of this particular set of building blocks"></a>Eigenfunction property of this particular set of building blocks</h3><script type="math/tex; mode=display">e^{j\omega_kt} \rightarrow H(\omega_k)e^{j\omega_kt} \\e^{j\omega_kt} \rightarrow \int_{-\infty}^{+\infty}h(\tau)e^{j\omega_k(t - \tau)}d\tau \\=e^{j\omega_kt}\int_{-\infty}^{+\infty}h(\tau)e^{-j\omega_k\tau}d\tau \\= e^{j\omega_kt}H(\omega_k)</script><p>we put in a complex exponential, we get out a complex exponential of the same frequency, multiplied by a complex constant. <strong>这就是所谓的eigenfunction property</strong></p><p>即输入和输出看起来除了振幅变化之外 其余完全一样，the change amplitude being the eigenvalue</p><p>称$e^{j\omega_kt}$为eigenfunction  称$H(w_k)$为eigenvalue</p><h3 id="Periodic-Signals-Fourier-Series"><a href="#Periodic-Signals-Fourier-Series" class="headerlink" title="Periodic Signals: Fourier Series"></a>Periodic Signals: Fourier Series</h3><script type="math/tex; mode=display">x(t) = x(t + T_0) \quad T_0 \ is \ period \\w_0 = \frac{2\pi}{T_0} = 2\pi f_0</script><p>对于$e^{j\omega_0t}$ 的基本周期就是$T_0$，谐波相关的复指数是(虽然$T_0$也是周期，但是它们的基本周期更短) 如$e^{jk\omega_0t}$ 其周期为$\frac{T_0}{k} = \frac{2\pi}{k\omega_0}$  随着k的变化 这些complex exponentials 称作为harmonically related </p><p>傅里叶级数（一个非常普通的周期函数可以表示成谐波相关的复指数信号的线性组合）</p><script type="math/tex; mode=display">x(t) = \sum_{k = -\infty}^{+\infty}a_ke^{jk\omega_0t} \tag{1}</script><p> 对于这个级数我们有两个问题</p><ul><li>我们如何确定傅里叶级数的系数$a_k$</li><li>how broad a class of signals can be represented this way</li></ul><p>傅里叶级数也有其他的表现形式 trigonometric form:</p><script type="math/tex; mode=display">a_k = A_ke^{j\theta_k} = B_k + jC_k \\e^{jk\omega_0t} = cosk\omega_0t + jsink\omega_0t \\</script><p>然后老师写了下面的式子，至于怎么从$(1)$得到下面的式子 我没弄明白</p><script type="math/tex; mode=display">x(t) = a_0 + 2\sum_{k = 1}^\infty A_kcos(k\omega_0t + \theta_k) \\= a_0 + 2\sum_{k = 1}^\infty [B_kcosk\omega_0t -C_ksink\omega_0t]</script><p>注意到：</p><script type="math/tex; mode=display">\int_{T_0}e^{jm\omega_0t}dt = \begin{cases}T_0& m = 0 \\0& m \ne 0\end{cases} \\\because \int_{T_0}e^{jm\omega_0t}dt = \int_{T_0}cosm\omega_0tdt + j\int_{T_0}sinm\omega_0tdt = 0 + 0j = 0 \ for \ m\ne0</script><p>根据这个性质：</p><script type="math/tex; mode=display">\int_{T_0}x(t)e^{jn\omega_0t}dt = \int_{T_0}e^{-jn\omega_0t}\sum_{k=-\infty}^{+\infty}a_ke^{jk\omega_0t}dt \\= \sum_{k = -\infty}^{+\infty}a_k\int_{T_0}e^{-j(k - n)\omega_0t}dt \\= \begin{cases}T_0& if \ k = n \\0& if \ k \ne n\end{cases} \\\therefore \frac{1}{T_0}\int_{T_0}x(t)e^{-jn\omega_0t} = a_n \quad analysis \ equation</script><p>我们称方程$(1)$ 为synthesis equation</p><p>例子[25:00] antisymmetric periodic square wave</p><p>得到的傅里叶系数$a_0 = 0, a_k = \frac{1}{j\pi k}(1 - (-1)^k)$ 是纯虚数 和 odd sequence 即$a_k = -a_{-k}$</p><p>表示成三角级数: **注意因为$a_k$是纯虚数，乘上$j$之后就变成实数</p><script type="math/tex; mode=display">x(t) = a_0 + \sum_{k = 1}^{+\infty} 2ja_ksink\omega_0t</script><p>例子 [30:40]  symmetric periodic square wave</p><p>[34:00] 展示了级数是如何还原出方波函数的 利用部分和$x_N(t) = \frac{1}{2} + \sum_{k = 1}^N2a_kcosk\omega_0t$ 不断增加N的大小 来看图像是如何的</p><p><strong>the low frequency terms that represent the broad time behavior, and it’s the high frequency terms that are used to build up the sharp transitions in the time domain.</strong></p><p>[43:00]讲的是傅里叶级数收敛的条件</p><p>[47:00] 显示了部分和随N增加，原函数与傅里叶级数部分和的误差的能量值 随N增大的变化，当N是奇数时误差能量不变，N是偶数时误差能量减小</p>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Systems Represented by Differential and Difference Equations</title>
      <link href="2021/03/14/signal%20and%20system%206/"/>
      <url>2021/03/14/signal%20and%20system%206/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-6-System-Represented-by-Differential-and-Difference-Equations"><a href="#Lecture-6-System-Represented-by-Differential-and-Difference-Equations" class="headerlink" title="Lecture 6 System Represented by Differential and Difference Equations"></a>Lecture 6 System Represented by Differential and Difference Equations</h2><p><a href="https://www.bilibili.com/video/BV1xy4y167DD?p=6">视频链接</a> 对应于书上的2.4的内容</p><p>A particularly important set of systems, which are linear and time-invariant, are those that are represented by linear constant-coefficient differential equations in continuous time or linear constant-coefficient difference equations in discrete time</p><h3 id="n阶线性常系数微分方程的定义："><a href="#n阶线性常系数微分方程的定义：" class="headerlink" title="n阶线性常系数微分方程的定义："></a>n阶线性常系数微分方程的定义：</h3><script type="math/tex; mode=display">\sum_{k = 0}^Na_k\frac{d^ky(t)}{dt^k} = \sum_{k = 0}^Mb_k\frac{d^kx(t)}{dt^k}  \tag{1}</script><p> 常系数表示系数都是常数（不随时间变化），被称为线性是因为对应于linear combination of these derivatives, not because it corresponds to a linear system</p><p><strong>这个方程可能会也可能不会对应于线性系统</strong></p><h3 id="n阶线性常系数差分方程的定义："><a href="#n阶线性常系数差分方程的定义：" class="headerlink" title="n阶线性常系数差分方程的定义："></a>n阶线性常系数差分方程的定义：</h3><script type="math/tex; mode=display">\sum_{k=0}^Na_ky[n-k] = \sum_{k=0}^Mb_kx[n-k] \tag{2}</script><h3 id="n阶线性常系数微分方程的解法："><a href="#n阶线性常系数微分方程的解法：" class="headerlink" title="n阶线性常系数微分方程的解法："></a>n阶线性常系数微分方程的解法：</h3><p><strong>方程相对应的Homogeneous Equation定义如下，其解为$y_h(t)$</strong></p><script type="math/tex; mode=display">\sum_{k = 0}^Na_k\frac{d^ky_h(t)}{dt^k} = 0 \tag{3}</script><p>给定输入$x(t)$，如果$y_p(t)$满足方程$(1)$ 那么$y_p(t) + y_h(t)$也会满足方程$(1)$ </p><p>通常称$y_p(t)$为Particular solution，$y_h(t)$为Homogeneous solution</p><p>所以方程$(1)$并非是a unique specification of the system.</p><h3 id="n阶线性齐次常系数微分方程的解法："><a href="#n阶线性齐次常系数微分方程的解法：" class="headerlink" title="n阶线性齐次常系数微分方程的解法："></a>n阶线性齐次常系数微分方程的解法：</h3><p>“guess” solution of the form $y_h(t) = Ae^{st}$ 把这个代入到$(3)$中得</p><script type="math/tex; mode=display">\sum_{k=0}^Na_kAs^ke^{st} = 0 \label{4}\tag{4} \\</script><script type="math/tex; mode=display">\because e^{st} \ne 0, A \ne 0 \quad \therefore \sum_{k=0}^Na_ks^k = 0 \quad N \ roots \ s_i \quad i = 1,\cdots,N \tag{5}</script><script type="math/tex; mode=display">y_h(t) = A_1e^{s_1t} + A_2e^{s_2t} + \cdots + A_Ne^{s_Nt} \tag{6}</script><p>通过方程$(5)$能得到得是N个s得值 但是常数$N$以及$A_1,\cdots,A_N$是不确定的</p><p>我们要解出这些常数 就需要N auxiliary conditions, 例如</p><script type="math/tex; mode=display">y(t), \frac{dy(t)}{dt},\cdots,\frac{d^{N - 1}y(t)}{dt^{N-1}} \ at \ t = t_0</script><p>取决于这些auxiliary conditions，the system may or may not correspond to a linear system</p><ul><li>Linear system $\iff$ auxiliary conditions = 0</li><li>Causal LTI $\iff$ initial rest： if $x(t) = 0$ for $t &lt; t_0$ then $y(t) = 0$ for $t &lt; t_0$</li></ul><p>例：$\frac{dy(t)}{dt} + ay(t) = x(t)$ 其对应的齐次方程为$\frac{dy_h(t)}{dt} + ay_h(t) = 0 \tag{7}$</p><p>猜测解为$y_h(t)  = Ae^{st}$ 代入得 $Ase^{st} + aAe^{st} = 0$ 化简得$s + a = 0$</p><p>得$y_h(t) = Ae^{-at}$</p><p>假设给定特定得输入为$x(t) = ku(t)$ 则$x(1) = k$ 代入微分方程得$\frac{dy(t)}{dt} + ay(t) = ku(t)$</p><p>解得相应的特解为$y_p = \frac{k}{a}[1 - e^{-at}]u(t)$</p><p>对于该微分方程的a family of solutions 为$y(t) = \frac{k}{a}[1 - e^{-at}]u(t) + Ae^{-at}$</p><p>求出A的值要auxiliary conditions，对于initial rest的条件 $\Rightarrow$ Causal, LTI $\Rightarrow$ $y(t) = \frac{k}{a}[1 - e^{-at}]u(t)$</p><p><strong>求冲激响应 这个知识点重要</strong></p><p>对于LTI系统 级联的顺序无关紧要</p><script type="math/tex; mode=display">u(t) \rightarrow \boxed{\frac{d}{dt}} \rightarrow \boxed{h(t)} \rightarrow h(t)</script><p>上面的系统与下面等效</p><script type="math/tex; mode=display">u(t) \rightarrow \boxed{h(t)}  \rightarrow \boxed{\frac{d}{dt}}  \rightarrow h(t)</script><p>上面的中间结果为$\delta(t)$，下面的中间结果为$s(t) = \frac{1}{a}[1 - e^{-at}]u(t)$ 所以求$h(t)$ 就可以直接对$s(t)$求导</p><p>解得$h(t) = \frac{s(t)}{dt} = u(t)\frac{d}{dt}\frac{1}{a}[1 - e^{-at}] + \frac{1}{a}[1 - e^{-at}]\frac{d}{dt}u(t) = e^{-at}u(t) + \frac{1}{a}[1 - e^{-at}]\delta(t)$</p><p>当$t \ne 0$时 $\delta(t)$为0， 得$h(t) = e^{-at}u(t)$  当$t = 0时$ $1 - e^{-at} = 0$ 所以无论$t$取什么值，右边的都为0</p><p>由之前的知识知 如果该系统是稳定的， 那么$a &gt; 0$</p><h3 id="n阶线性常系数差分方程的解法："><a href="#n阶线性常系数差分方程的解法：" class="headerlink" title="n阶线性常系数差分方程的解法："></a>n阶线性常系数差分方程的解法：</h3><p>与连续的情况类似 齐次方程为</p><script type="math/tex; mode=display">\sum_{k = 0}^N a_ky_h[n - k] = 0 \quad (Homogeneous \ Equation) \tag{8}</script><p>给定输入$x[n]$，如果$y_p[n]$满足方程$(8)$ 那么$y_p[n] + y_h[n]$也会满足方程$(1)$ </p><p>通常称$y_p[n]$为Particular solution，$y_h[n]$为Homogeneous solution</p><h3 id="n阶线性齐次常系数微分方程的解法：-1"><a href="#n阶线性齐次常系数微分方程的解法：-1" class="headerlink" title="n阶线性齐次常系数微分方程的解法："></a>n阶线性齐次常系数微分方程的解法：</h3><p>“guess” solution of the form $y[n] = Az^n$ 代入到方程$(8)$得$\sum_{k = 0}^Na_kAz^nz^{-k} = 0$</p><p>同样$\because A \ne 0，z \ne 0$ 我们得到$\sum_{k = 0}^Na_kz^{-k} = 0$ N roots $z_1, z_2, \cdots, z_N$</p><p>最终的解是$y_h[n] = A_1z_1^n + \cdots + A_Nz_N^n$</p><p>我们要解出这些常数 就需要N auxiliary conditions, 例如</p><script type="math/tex; mode=display">y[n_o], y[n_o - 1], y[n_o - 2],\cdots,y[n_o - N + 1]</script><p>取决于这些auxiliary conditions，the system may or may not correspond to a linear system</p><ul><li>Linear system $\iff$ auxiliary conditions = 0</li><li>Causal LTI $\iff$ initial rest： if $x[n] = 0$ for $n &lt; n_o$ then $y[t] = 0$ for $n &lt; n_o$</li></ul><p>微分方程和差分方程很类似， 某些情况下，差分方程更容易处理</p><p>假设系统是causal的：</p><p>对于方程$(2)$的解，$y[n] = \frac{1}{a_o} \{  \sum_{k = 0}^Mb_kx[n - k] - \sum_{k = 1}^Na_ky[n - k] \}$</p><p>如果我们知道$x[n]$和$y[n_o - 1], y[n_o - 2], \cdots, y[n_o - N]$ 那么我可以求得$y[n_o]$继而可以求$y[n_o + 1]$</p><p>例 $y[n] - ay[n - 1] = x[n]$ $\Rightarrow$ $y[n] = x[n] + ay[n - 1] \tag{9}$</p><p>causal LTI $\iff$ Initial Rest  假设输入$x[n] = \delta[n]$ 代入得$h[n] = \delta[n] + ah[n - 1]$</p><p>$h[n] = 0$ 对于$n &lt; 0$ 然后$n = 0$ 代入得$h[0] = 0$ 继续得$h[1] = a, h[2] = a^2$</p><p>如果是这个系统是稳定话 $\iff |a| &lt; 1$</p><p>求解齐次方程的解 $y_h[n]$  令$y_h[n] = Az^n$ 代入得$Az^n - aAz^{n - 1} \Rightarrow 1 - az^{-1} = 0 \Rightarrow a = z$</p><p>$\Rightarrow y_h[n] = Aa^n$  所以对与这个系统$\delta[n] \rightarrow a^nu[n] + Aa^n$</p><p>如果系统是causal的LTI 那么$A = 0$</p><p>[32:04] 将方程$(9)$画成图，这个图是一个反馈系统 [33:49] 画的是方程$(2)$对应的图</p><p>接下来的是将连续的情况画成图</p>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Perceptron Generalized Linear Model</title>
      <link href="2021/03/13/cs229%204/"/>
      <url>2021/03/13/cs229%204/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-4-Perceptron-Generalized-Linear-Model"><a href="#Lecture-4-Perceptron-Generalized-Linear-Model" class="headerlink" title="Lecture 4 Perceptron Generalized Linear Model"></a>Lecture 4 Perceptron Generalized Linear Model</h2><p><a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=5">视频链接</a></p><p><a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes1.pdf">讲义链接</a></p><h3 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h3><p>感知机现实不怎么会用到，学这个主要是出于历史原因</p><p>logistic regression用的函数是sigmoid函数 $g(z) = \frac{1}{1 + e^{-z}}$</p><p>而percetron用的是$g(z) = \begin{cases} 1&amp; z \ge 0 \\0&amp;  z &lt; 0 \end{cases}$</p><p>两者的更新公式都是$\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}$</p><p>其中$y^{(i)} - h_\theta(x^{(i)})$的值只可能是1或者0或者-1</p><p>翻翻李航的书(第二版) P39  $y$的取值是$\{-1,1\}$，更新标准是 如果错误分类，则</p><script type="math/tex; mode=display">\omega \leftarrow \omega + \eta y_i x_i \\b \leftarrow b + \eta y_i</script><p>[9:00] 左右的视频讲的非常清楚 图画的很好</p><p>感知机一般不用于实际的原因之一是这个模型没有概率解释，而且感知机无法对XOR进行分类</p><h3 id="exponential-family"><a href="#exponential-family" class="headerlink" title="exponential family"></a>exponential family</h3><p>这是一类概率分布 probability density function为：</p><script type="math/tex; mode=display">p(y;\eta) = b(y)exp(\eta^TT(y) - a(\eta))</script><p>这个公式里面$y$是数据，$\eta$是natural parameter，$T(y)$是充分统计量，$b(y)$是base measure，$a(\eta)$是log-partition（为了让这个式子积分结果为1）</p><p>关于充分统计量的知识<a href="https://www.zhihu.com/question/41367707/answer/572628701">链接</a>  在数理统计书上的定义是 设$\theta$是总体分布中的参数，$X_1,X_2,\cdots,X_n$是来自此总体的样本，$T = T(X_1, X_2, \cdots,X_n)$是统计量，若在已知$T = t$的条件下，样本的条件分布于$\theta$无关，则称$T$为$\theta$的充分统计量</p><p>Bernoulli(Binary data)  $\phi = $ probability of event</p><p>其PDF为$p(y;\phi) = \phi^y(1-\phi)^{(1 - y)}$ 我们要把这个PDF massage 成exponential family的形式</p><script type="math/tex; mode=display">\begin{aligned}p(y;\phi) &= exp(log(\phi^y(1-\phi)^{(1-y)})) \\&= exp[log(\frac{\phi}{1-\phi})y + log(1 - \phi)]\end{aligned}</script><p>massage: $b(y) = 1, T(y) = y,\eta = log(\frac{\phi}{1 - \phi}) + log(1 - \phi),a(\eta) = -log(1 - \phi)$</p><p>由$\eta = log(\frac{\phi}{1 - \phi})$可得$\phi = \frac{1}{1 + e^{-\eta}}$代入道$a(\eta)$中可得$a(\eta) = log(1 + e^\eta)$</p><p>由上面的massage 可知Bernoulli分布是exponential 分布</p><p>Gaussian（假设方差为1）</p><p>其PDF为$p(y;\mu) = \frac{1}{\sqrt{2\pi}}exp(-\frac{(y - \mu)^2}{2})$把这个PDF massage成 exponential family的形式</p><script type="math/tex; mode=display">p(y;\mu) = \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}exp(\mu y - \frac{1}{2}\mu^2)</script><p>其中$b(y) = \frac{1}{\sqrt{2\pi}}exp{-\frac{y^2}{2}}, T(y) = y, \eta = \mu, a(\eta) = \frac{\mu^2}{2} = \frac{\eta^2}{2}$</p><p><strong>exponential family的性质</strong></p><ul><li>如果我们对exponential family进行MLE w.r.t. $\eta$，那么the optimization problem is concave</li><li>$E[y;\eta] = \frac{\partial}{\partial\eta}a(\eta)$</li><li>$Var[y;\eta] = \frac{\partial^2}{\partial \eta^2}a(\eta)$</li></ul><h3 id="Generalized-Linear-Models"><a href="#Generalized-Linear-Models" class="headerlink" title="Generalized Linear Models"></a>Generalized Linear Models</h3><p><strong>we can build many powerful models by choosing an appropriate family in the exponential family and kind of plugging it onto a linear model</strong></p><p>对于GLM的假设</p><ul><li><p>$y | x;\theta \sim$ Exponential Family$(\eta)$ 对于不同的数据类型可以用不同的模型Real - Gaussian，Binary - Bernoulli，Count - Poisson，$R^+$ - Gamma, exponential，Distribution - Beta，Dirichlet 后面两个一般用于贝叶斯统计学习</p></li><li><p>$\eta = \theta^Tx \ \theta \in \mathrm{R}^n \ x \in \mathrm{R}^n$</p></li><li><p>Test Time: Output为$E[y|x;\theta] \Rightarrow h_\theta(x) = E[y|x;\theta]$</p><p>Train Time: $\max_\theta log p(y^{(i)}, \theta^Tx^{(i)})$ 用gradient ascent</p></li></ul><p><strong>GLMs Training</strong></p><p>Learning Update Rule(所有GLMs都一样) $\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x{(i)}))x_j^{(i)}$</p><p>[52:12] 一些术语 $\eta$ - natural parameter </p><p>$\mu = E[y;\eta] = g(\eta)$ Canonical Response function</p><p>$\eta = g^{-1}(\mu)$ Canonical Link function</p><p>[53:50] 开始讲述了three different kinds of parameterizations we have</p><p>model parameters       natural parameters     canonical parameters</p><p>[58:40] 有人问到如何对于输出来选择分布</p><p>老师的Answer: the choice of what distribution you are going to choose is really dependent on the task that you have. So if  your task is regression where you want to output real valued numbers like price of the house then you choose a distribution over the real numbers like a Gaussian. If your task is classification, where your output is binary 0, or 1, you choose a distribution that models binary data</p><h3 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h3><p>用于多个类别的分类，输出是一个one-hot vector，就是一个只有0和1构成的向量，向量里面每个位置代表着一个类，一个向量里面只有一个1其余的都是0，1所在的位置就表示输出是相应的类</p><p>这部分讲义里面写的挺好</p><p>softmax 输出的是所有类的一个概率分布</p><h4 id="666-one-hot-vector-可以看作是一个概率分布"><a href="#666-one-hot-vector-可以看作是一个概率分布" class="headerlink" title="666 one-hot vector 可以看作是一个概率分布"></a>666 one-hot vector 可以看作是一个概率分布</h4><p><strong>真实标签 也可以看作是一个概率分布，对于标签的类概率是1，其余类概率是0</strong> </p><p>我们希望两个distribution变得相近，the term for that is to minimize the cross entropy between the two distributions</p><h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>这是官网上面关于高斯分布的材料的学习笔记，<a href="http://cs229.stanford.edu/section/gaussians.pdf">链接part1</a>  以及 <a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/more_on_gaussians.pdf">链接part2</a></p><p>对于univariate normal distribution，the coefficient $\frac{1}{\sqrt{2\pi}\sigma}$ is a constant that does not depend  on x; <strong>我们可以把它看作是为了让下面的积分为1 所存在的一个”normalization factor”</strong></p><script type="math/tex; mode=display">\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}exp(-\frac{1}{2\sigma^2}(x - \mu)^2) = 1</script><p>Proposition 1的证明在下面的Appendix A.1</p><script type="math/tex; mode=display">\Sigma = E[(X - \mu)(X - \mu)^T] = E[XX^T] - \mu\mu^T</script><p>Proposition 2证明$\Sigma$是半正定的（这里$\Sigma$的定义是the covariance matrix corresponding to some random vector X）证明中有个等式引起了我的注意：</p><script type="math/tex; mode=display">\sum_i\sum_jx_ix_jz_iz_j = (x^Tz)^2</script><p>x的维数要和z的维数相同</p><p>假如维数为1 则$\sum_i\sum_jx_ix_jz_iz_j = x_1^2z_1^2 = (x^Tz)^2$</p><p>假如维数为2 则$\sum_i\sum_jx_ix_jz_iz_j = x_1^2z_1^2 + 2x_1x_2z_1z_2 + x_2^2z_2^2 = (x1z_1 + x_2z_2)^2 = (x^Tz)^2$</p><p>假如维数为k成立，那么维数为k + 1时</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i = 1}^{k + 1}\sum_{j = 1}^{k + 1}x_ix_jz_iz_j  &= x_{k+1}z_{k+1}\sum_{j= 1}^{k + 1}x_jz_j +\sum_{i = 1}^k\sum_{j = 1}^{k + 1}x_ix_jz_iz_j \\&= x_{k+1}z_{k+1}\sum_{j= 1}^{k + 1}x_jz_j + x_{k + 1}z_{k + 1}\sum_{i = 1}^kx_iz_i + \sum_{i = 1}^k\sum_{j = 1}^kx_ix_jz_iz_j\\&= (x^Tz)_{k \ dimension}^2 + x_{k + 1}^2z_{k + 1}^2 + 2x_{k+1}z_{k+1}\sum_{i = 1}^kx_iz_i \\&= (x^Tz)_{k \ dimension}^2 + 2x_{k + 1}z_{k + 1}(x^Tz)_{k \ dimension} +  x_{k + 1}^2z_{k + 1}^2 \\&= ((x^Tz)_{k \ dimension}^2 + x_{k + 1}z_{k + 1})^2 \\&= (x^Tz)^2_{k+1 \ dimension} \quad \quad  \quad \quad \quad \quad q.e.d.\end{aligned}</script><p><strong>讲义里面的Shape of isocontours写的非常具有启发性</strong></p><p>对于$\Sigma$是对角阵的情况：</p><script type="math/tex; mode=display">c = \frac{1}{2\pi\sigma_1\sigma_2}exp(-\frac{1}{2\sigma_1^2(x_1 - \mu_1)^2} - \frac{1}{2\sigma_2^2}(x_2 - \mu_2)^2) \\log(\frac{1}{2\pi c\sigma_1\sigma_2}) = \frac{1}{2\sigma_1^2}(x_1 - \mu_1)^2 + \frac{1}{2\sigma_2^2}(x_2 - \mu_2)^2 \\1 = \frac{(x_1 - \mu_1)^2}{2\sigma_1^2log(\frac{1}{2\pi c\sigma_1\sigma_2})} + \frac{(x_2 - \mu_2)^2}{2\sigma_2^2log(\frac{1}{2\pi c\sigma_2\sigma_2})}</script><p><strong>等高线是一个axis-aligned ellipse</strong></p><p>对于$\Sigma$不是对角阵的情况：等高线是一个rotated ellipse</p><p>扩展到n维的情况 图形称作为ellipsoids</p><p><strong>Appendix A.2非常棒</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Locally Weighted Linear Regression</title>
      <link href="2021/03/08/cs229%203/"/>
      <url>2021/03/08/cs229%203/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-3-Locally-Weighted-Linear-Regression"><a href="#Lecture-3-Locally-Weighted-Linear-Regression" class="headerlink" title="Lecture 3 Locally Weighted Linear Regression"></a>Lecture 3 Locally Weighted Linear Regression</h3><p>这是根据<a href="https://www.bilibili.com/video/BV1JE411w7Ub?p=2">吴恩达CS229课程视频</a>的笔记</p><p>官方有一个notes，<a href="http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes1.pdf">链接</a></p><p>对于一个比较复杂形状的函数，自己定义函数的形式$y=f(x)$比较困难，这时可以用locally weighted linear regression</p><ul><li><p>”Parametric” learning algorithm:</p><p>Fit fixed set of parameters $\theta_i$ to data</p></li><li><p>“Non-Parameteric” learning algorithm</p><p>Amount of data/parameters you need to keep grows(linearly) with the size of data</p><p>LWR，KNN就是Non-Parameteric的</p></li></ul><p><strong>LWR通常不外推，意思是对于在训练集两端外的预测 效果可能会不好</strong></p><p><strong>老师更倾向对于相对低维的数据集使用LWR(比如n = 2或3，有很多数据的情况)</strong></p><h4 id="Probabilistic-interpretation"><a href="#Probabilistic-interpretation" class="headerlink" title="Probabilistic interpretation"></a>Probabilistic interpretation</h4><p>这个部分是说明cost function的由来，基于一个重要假设是$y^{(i)} = \theta^\mathrm{T}x^{(i)} + \epsilon^{(i)}$ 其中$\epsilon^{(i)}$是独立同分布且服从$\mathcal{N}(0, \sigma^2)$，$\epsilon^{(i)}$代表着随机误差</p><p><strong>注意，这个假设就表示训练集中的每个数据是IID的，如果现实中数据不是IID的，那么用这个方法就不合理，可以去构建一个更复杂的模型</strong></p><p>讲义当中的公式$\mathcal{P}(y^{(i)}|x^{(i)};\theta)$里面$x^{(i)}$和$\theta$中间的分号表示$\theta$是一个参数，如果是逗号，那么说明$\theta$是一个随机变量</p><p><strong>likelihood和probability的区别</strong></p><p>视频里面是这么说的：</p><p>the likelihood of the parameters is exactly the same thing as the probability of the data就是函数形式相同，if you view this thing as a function of the parameters holding the data fixed, then we call that likelihood</p><h4 id="Classification-Problem"><a href="#Classification-Problem" class="headerlink" title="Classification Problem"></a>Classification Problem</h4><p>视频44分钟里面画了一张图，用之前的线性回归来应用于这个数据集效果是不好的，线性回归的方法是回归出一条直线，然后设置一个阈值，通过阈值进行分类(大于阈值的一类，小于阈值的另一类)</p><p>这个方法的缺陷在于，45分钟的图，如果有一个稍微偏远的数据，可能会明显改变直线，因而改变决策边界，效果不好。</p><p>我们想要一个$h_\theta(x) \in [0, 1]$  sigmoid函数 $g(x) = \frac{1}{1 + e^{-z}}$ 代入得 $h_\theta(x) = g(\theta^{\mathrm{T}}x) = \frac{1}{1 + e^{-\theta^{\mathrm{T}}x}}$</p><p><em>这里有一个假设$\mathcal{P}(y = 1 | x ; \theta) = h_\theta(x)$</em> 由于是二分类的 所以$\mathcal{P}(y = 0 | x ; \theta) = 1 - h_\theta(x)$</p><p>将这两个式子写成一个式子 $\because y \in \{0 , 1\}$</p><script type="math/tex; mode=display">\mathcal{P}(y | x ; \theta) = h(x)^y(1-h(x))^{1-y}    \quad 666</script><p>对这个概率分布，we can write down the likelihood of the parameters as:</p><script type="math/tex; mode=display">\begin{aligned}L(\theta) &= p(\vec{y}|X,\theta) \\&= \prod_{i = 1}^n p(y^{(i)}|x^{(i)};\theta) \\&= \prod_{i = 1}^n (h_\theta(x^{(i)}))^{y^{(i)}}(1 - h_\theta(x^{(i)}))^{(1 - y^{(i)})}\end{aligned}</script><p>maximize the log likelihood:</p><script type="math/tex; mode=display">\begin{aligned}\ell(\theta) &= logL(\theta) \\&=  \sum_{i = 1}^n y^{(i)}logh(x^{(i)}) + (1 - y^{(i)})log(1 - h(x^{(i)}))\end{aligned}</script><p><strong>要注意的是</strong> 训练的时候是用梯度上升 而不是梯度下降！</p><p><strong>选择$h_\theta(x)$的原因之一是保证likelihood function只有一个全局最大值（concave function)</strong></p><p>对于logistics regression的常见问题</p><p><a href="https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c">为什么logistics regression不采用MSE作为Loss Function</a></p><p>总结一下就是<strong>MSE doesn’t strongly penalize misclassifications</strong>和<strong>MSE loss function for logistic regression is non-convex</strong></p><h4 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h4><p>这个方法和gradient descent的区别是 这个方法一次更新的跨度更大(视频66: 12)，视频中举例 如果用1000次gradient descent得到好的$\theta$，那么用Newton’s method 经过10次迭代就能得到好的$\theta$，不过每次迭代的代价会比较昂贵</p><p>牛顿方法旨在解决的问题是， 我们有一个函数$f$，我们想要找到一个$\theta$ 使得$f(\theta) = 0$</p><p>将牛顿方法应用到机器学习中就是用牛顿方法来找导数为0的点</p><p>视频69分钟开始 演示牛顿方法的过程，更新算法如下：</p><script type="math/tex; mode=display">\theta^{(t + 1)} := \theta^{(t)} - \frac{f(\theta^{(t)})}{f^\prime(\theta^{(t)})}</script><p>当$\theta$是一个向量的时候，$\theta \in \mathcal{R}^{n + 1}$</p><script type="math/tex; mode=display">\theta^{(t + 1)} := \theta^{(t)} - H^{-1}\nabla_\theta\mathcal{l}(\theta)</script><p>其中H是Hessian matrix $H_{ij} = \frac{\partial^2\mathcal{l}(\theta)}{\partial\theta_i\partial\theta_j}$， 这个算法的耗时地方在于对hessian matrix求逆耗时，如果拥有的参数数量10个或50个 老师推荐有这个方法</p><p>Newton’s method enjoys a property called quadratic convergence(意思是如果某一次迭代后$\theta$的误差是0.01error 再经过一次迭代之后 误差变为0.0001error) —&gt; 我没懂他说的0.01error是什么意思，emmm… 反正就是很快的</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs229 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convolution</title>
      <link href="2021/03/08/signal%20and%20system%204/"/>
      <url>2021/03/08/signal%20and%20system%204/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-4-Convolution"><a href="#Lecture-4-Convolution" class="headerlink" title="Lecture 4 Convolution"></a>Lecture 4 Convolution</h3><p><strong>这个笔记是根据</strong><a href="https://www.bilibili.com/video/BV1xy4y167DD?p=4">奥本海姆信号与系统的视频课来写的</a></p><p>如何利用time invariant 和linear属性—&gt;将一个信号分解为一组基本信号</p><p>对于基本信号的选择，目的是为了能够轻松地产生输出信号</p><ul><li>delayed impulses  &lt;—-&gt;  Convolution</li><li>complex exponential &lt;—-&gt;  Fourier analysis</li></ul><h4 id="将离散信号表示为冲激信号的线性组合"><a href="#将离散信号表示为冲激信号的线性组合" class="headerlink" title="将离散信号表示为冲激信号的线性组合"></a>将离散信号表示为冲激信号的线性组合</h4><script type="math/tex; mode=display">x[n] = \begin{cases}c& n = 0 \\0& n \neq 0\end{cases}\\其中c \neq 0</script><p>对于这个信号 可以表示为$x[0] \times \delta[n]$</p><p>同样可得$x[1] = x[1] \times \delta[n - 1]$还有$x[-1] = x[-1] \times \delta[n + 1]$</p><p>对于任意的一个信号$x[n]$ 就可以分解为$x[n] = \sum_{k=-\infty}^{+\infty}x[k]\delta[n-k]$</p><p><strong>这样表示的作用是什么？</strong></p><p>如果我们是在线性系统中，那么响应是线性组合的形式（假如$\delta[n-k]$的响应是$h_k[n]$），那么</p><script type="math/tex; mode=display">y[n] = \sum_{k=-\infty}^{+\infty}x[k]h_k[n]</script><p>如果系统是time-invariant 那么$h_k[n] = h_0[n - k]$ 将$h_0[n]$记为$h[n]$ 即为unit impulse</p><p>对于LTI系统                                       $y[n] = \sum_{k=-\infty}^{+\infty}x[k]h[n-k]$</p><h4 id="将连续信号表示为冲激信号的线性组合"><a href="#将连续信号表示为冲激信号的线性组合" class="headerlink" title="将连续信号表示为冲激信号的线性组合"></a>将连续信号表示为冲激信号的线性组合</h4><p>首先将连续时间信号分解为连续任意狭窄的矩形，当这些矩形的宽度变为0，会越近似于原信号，<strong>要注意的是 当每个矩形变得越来越窄就越来越多地与冲激信号对应</strong></p><script type="math/tex; mode=display">x(t) = \begin{cases}x(0) \times \delta_{\Delta}(t)\times\Delta& 0 \le t \le \Delta \\0& otherwise\end{cases}</script><script type="math/tex; mode=display">x(t) \approx x(0)\delta_{\Delta}(t)\Delta + x(\Delta)\delta_{\Delta}(t - \Delta)\Delta + x(-\Delta)\delta_{\Delta}(t + \Delta)\Delta + ... \\x(t) \approx \sum_{k=-\infty}^{+\infty}x(k\Delta)\delta_{\Delta}(t - k\Delta)\Delta \\x(t) = \lim_{\Delta \to 0}\sum_{k=-\infty}^{+\infty}x(k\Delta)\delta_{\Delta}(t - k\Delta)\Delta \\= \int_{-\infty}^{+\infty}x(\tau)\delta(t - \tau)d\tau  \quad sifting \ integral</script><p>对于线性系统输出为：</p><script type="math/tex; mode=display">y(t) = \lim_{\Delta \to 0}\sum_{k = -\infty}^{+\infty}x(k\Delta)h_{k\Delta}(t)\Delta \\=\int_{-\infty}^{\infty}x(\tau)h_{\tau}(t)d\tau</script><p>同样 如果系统是time invariant $h_{k\Delta} = h_0(t - k\Delta)$ 和 $h_{\tau}(t) = h_0(t - \tau)$ 可得：</p><script type="math/tex; mode=display">y(t) = \int_{-\infty}^{+\infty}x(\tau)h(t - \tau)d\tau</script><p>由上述可知 <strong>如果我们知道了对应于t=0或者n=0的冲激响应，那么通过卷积我们能得到任意输入的输出</strong></p><p><strong>我们将*符号表示卷积</strong></p><script type="math/tex; mode=display">y[n] = \sum_{k = -\infty}^{+\infty}x[k]h[n - k] = x[n] * h[n] \\y(t) = \int_{-\infty}^{+\infty}x(\tau)h(t - \tau)d\tau = x(t) * h(t)</script><p>从23分钟后就是介绍离散的例子 求$x[n] = u[n]$和$h[n] = \alpha^nu[n]$的响应，将$x[n] 变为截取的u[n]$</p><p>29分钟后是介绍连续的例子 $x(t) = u(t)$ 和 $h(t) = e^{-at}u(t)$ 同样也有$u(t)$的截取形式</p><p>视频里面的动画来显示卷积的过程挺不错</p><p>47分钟开始分析了一个连续时间卷积的例子</p><script type="math/tex; mode=display">y(t) = \int_{-\infty}^{\infty}x(\tau)h(t - \tau)d\tau \\= \int_{-\infty}^{\infty}u(\tau)e^{-a(t - \tau)}u(t - \tau)d\tau</script><p>考虑区间$t &lt; 0$， 因为$u(\tau)$在$\tau$小于0时值为0 当$\tau$大于等于0时$u(t - \tau)$为0 所以积分的值为0</p><p>考虑区间$t &gt; 0$ <strong>注意视频中$t = 0$的情况被忽略了，我觉得是因为重点不在$t = 0$</strong></p><p>当$\tau &lt; 0$时$u(\tau)$为0， 当$\tau &gt; t$时$u(t - \tau)$为0，也就是在区间$(-\infty, 0)$ 和$(t, +\infty)$积分的值为0</p><script type="math/tex; mode=display">\therefore \int_{-\infty}^{\infty}u(\tau)e^{-a(t - \tau)}u(t - \tau)d\tau \\= \int_{0}^{t}u(\tau)e^{-a(t - \tau)}u(t - \tau)d\tau  \\ = \int_{0}^{t}e^{-a(t - \tau)}d\tau \\= \frac{1}{a}[e^{at} - 1] \\\therefore y(t) = \begin{cases}0& t < 0 \\\frac{1}{a}{e^{at} - 1}& t > 0\end{cases}</script>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Signals and Systems Part II</title>
      <link href="2021/03/08/signal%20and%20system%203/"/>
      <url>2021/03/08/signal%20and%20system%203/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-3-Signals-and-Systems-Part-II"><a href="#Lecture-3-Signals-and-Systems-Part-II" class="headerlink" title="Lecture 3 Signals and Systems: Part II"></a>Lecture 3 Signals and Systems: Part II</h3><p>这是根据公开课视频写的笔记<a href="https://www.bilibili.com/video/BV1xy4y167DD?p=3">链接</a></p><h3 id="几个重要的函数"><a href="#几个重要的函数" class="headerlink" title="几个重要的函数"></a>几个重要的函数</h3><h5 id="离散的unit-step函数"><a href="#离散的unit-step函数" class="headerlink" title="离散的unit step函数"></a>离散的unit step函数</h5><script type="math/tex; mode=display">\begin{equation}u[n] = \begin{cases}1& n \ge 0 \\0& n < 0\end{cases}\end{equation}</script><h5 id="离散的unit-impulse-函数"><a href="#离散的unit-impulse-函数" class="headerlink" title="离散的unit impulse 函数"></a>离散的unit impulse 函数</h5><script type="math/tex; mode=display">\begin{equation}\delta[n] = \begin{cases}1& n = 0 \\0& otherwise\end{cases}\end{equation}</script><p>两者之间的关系是</p><script type="math/tex; mode=display">\delta[n] = u[n] - u[n - 1] \quad (first \quad difference) \\ u[n] = \sum_{m = -\infty}^{n}\delta[m] \quad (running \quad sum)</script><p>unit step sequence can be thought of as a succession of unit impulses one following another</p><p><strong>这个知识点重要</strong></p><script type="math/tex; mode=display">u[n] = \delta[n] + \delta[n - 1] + \delta[n - 2] + ... \\u[n] = \sum_{k = 0}^{\infty}\delta[n - k]</script><h5 id="连续的unit-step函数"><a href="#连续的unit-step函数" class="headerlink" title="连续的unit step函数"></a>连续的unit step函数</h5><script type="math/tex; mode=display">\begin{equation}u(t) = \begin{cases}1& t > 0 \\0& t < 0\end{cases}\end{equation}</script><p>关于unit step函数在t = 0的定义有多种。t = 0处的定义不是重点，重点是这个函数的关键在于t = 0时 这个函数是不连续的</p><p>unit step可以看作是一个approximation unit step函数取极限得到</p><script type="math/tex; mode=display">\begin{equation}u_{\Delta}(t) = \begin{cases}1& t > \Delta \\\frac{1}{\Delta}& 0 \le t \le \Delta \\0& t < 0\end{cases}\end{equation} \\u(t) = \lim_{\Delta \to 0}u_{\Delta}(t)</script><h5 id="连续的unit-impulse函数"><a href="#连续的unit-impulse函数" class="headerlink" title="连续的unit impulse函数"></a>连续的unit impulse函数</h5><p>类比unit step和unit impulse之间的关系 来定义连续的unit impulse函数</p><p>unit impulse 就是 unit step的导数，由approximation unit step函数的定义来得到</p><script type="math/tex; mode=display">\delta(t) = \frac{du(t)}{dt} \\\delta_{\Delta}(t) = \frac{du_{\Delta}(t)}{dt} \\\begin{equation}\delta_{\Delta}(t) = \begin{cases}\frac{1}{\Delta}& 0 \le t \le \Delta \\0& otherwise\end{cases}\end{equation} \\no \ matter \ what \ the \ value \ of \ \Delta \ is\ , \ the \ area \ is \ always \ equal \ to \ 1 \\\delta(t) = \delta_{\Delta}(t) \quad as \quad \Delta \to 0</script><p>连续时间下，unit step函数和unit impulse函数之间的关系</p><script type="math/tex; mode=display">\delta = \frac{du(t)}{dt} \\u(t) = \int_{-\infty}^{t}\delta(\tau)d\tau</script><h3 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h3><p><strong>定义：a transformation from an input signal to an output signal </strong></p><ul><li>cascade system</li><li>parallel system</li><li>feedback system</li></ul><p>对于LTI系统，the overall system transformation is independet of the order in which the systems are cascaded</p><h5 id="系统属性"><a href="#系统属性" class="headerlink" title="系统属性"></a>系统属性</h5><ul><li><p>memoryless:  $y(t_0) \quad depends \ only \ on \quad x(t_0)$</p><p>例如 $y(t) = x^2(t)$  和 $y[n] = x^2[n]$ 是memoryless</p><p>​        $y(t) = \int_{-\infty}^{t}x^2(\tau)d\tau$ 和$y[n] = x[n - 1] \quad unit \ delay$  不是memoryless</p></li><li><p>invertibility(可逆性)： given the output, there is only one input</p></li><li><p>Causality: its response at any time only depends on values of the input prior to that time</p><p>例如$y[n] = \frac{1}{3}(x[n - 1] + x[n] + x[n + 1]) $ 不是causality</p><p>​        $y[n] = \frac{1}{3}(x[n - 2] + x[n - 1] + x[n])$ 是causality</p></li><li><p>Stability： for every bounded input the output is bounded</p><p>​    feedback system的一个重要的应用是stablize unstable systems</p><p>​    例如$y(t) = \int_{-\infty}^{t}u(\tau)d\tau$  输出$y(t)$ 是ramp function， ramp is unbounded because if you try to establish any bound on it,  you can always go out far enough in time</p></li><li><p>Time invariance: 系统 if $x(t) -&gt; y(t)$ then  $x(t - t_0) -&gt; y(t - t_0)$</p><p>例如 $y(t) = sint \times x(t)$ 不是time invariance 因为当输入是$x(t-t_0)$时 输出时$sint \times x(t-t_0)$</p></li><li><p>Linearity: </p></li></ul><script type="math/tex; mode=display">if \quad x_1(t) -> y_1(t) \quad and \quad x_2(t) -> y_2(t) \\then \quad ax_t(t) + bx_2(t) -> ay_1(t) + by_2(t)</script>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Properties of Linear Time-invariant Systems</title>
      <link href="2021/03/08/signal%20and%20system%205/"/>
      <url>2021/03/08/signal%20and%20system%205/</url>
      
        <content type="html"><![CDATA[<h3 id="Lecture-5-Properties-of-Linear-Time-invariant-Systems"><a href="#Lecture-5-Properties-of-Linear-Time-invariant-Systems" class="headerlink" title="Lecture 5 Properties of Linear Time-invariant Systems"></a>Lecture 5 Properties of Linear Time-invariant Systems</h3><p>这是根据公开课视频写的笔记<a href="https://www.bilibili.com/video/BV1xy4y167DD?p=5">链接</a></p><h5 id="Commutative"><a href="#Commutative" class="headerlink" title="Commutative"></a>Commutative</h5><script type="math/tex; mode=display">x[n] * h[n] = h[n] * x[n] \\x(t) * h(t) = h(t) * x(t)</script><h5 id="Associative"><a href="#Associative" class="headerlink" title="Associative"></a>Associative</h5><script type="math/tex; mode=display">x * \{h_1 * h_2\} = \{x * h_1 \} * h_2</script><h5 id="Distributive"><a href="#Distributive" class="headerlink" title="Distributive"></a>Distributive</h5><script type="math/tex; mode=display">x * \{ h_1 + h_2 \} = x * h_1 + x * h_2</script><p>视频9分40秒的例子 解释了在LTI系统中commutative性质的作用</p><p>We can interchange the role of input and impulse response, and from an output point of view, the output doesn’t care.</p><p>视频11分钟40秒的例子 解释了在LTI系统中Associative性质的作用</p><p><strong>如果我们有两个级联的LTI系统，任意方式的级联结果是一样的</strong></p><p>对于非LTI系统的级联 比如—&gt;平方根—&gt;加倍—&gt; 和 —&gt;加倍—&gt;平方根—&gt; 是不一样的</p><p>视频15分11秒的例子 解释了在LTI系统中Distributive性质的作用</p><p><strong>总结一下 这块讲的是从卷积的性质来看LTI系统互联时的性质</strong></p><h4 id="LTI系统中，其他系统属性可以关联具有系统的特定属性冲激响应"><a href="#LTI系统中，其他系统属性可以关联具有系统的特定属性冲激响应" class="headerlink" title="LTI系统中，其他系统属性可以关联具有系统的特定属性冲激响应"></a>LTI系统中，其他系统属性可以关联具有系统的特定属性冲激响应</h4><ul><li><p>Memoryless</p><p>$h(t - \tau)$ nonzero only at $\tau = t$  which implys $h(t) = K\delta(t)$ 和 $h[n] = K\delta[n]$</p><p>视频19分22秒有图 由于我们想让冲激响应contribute something after we multiply and go through an integral, and what that says is the only thing that it can be and meet all those conditions is a scaled impulse</p></li><li><p>Invertibility</p><p>视频21分钟有图 $y = x <em> (h </em> h^{-1}) = x$ 我觉得视频上的图有问题，$h$和$h^{-1}$之间应该是卷积符号</p></li><li><p>Stability</p><script type="math/tex; mode=display">\sum_{-\infty}^{+\infty}|h[k]| < \infty \\\int_{-\infty}^{+\infty}|h(\tau)|d\tau < \infty</script></li><li><p>Causality</p><p>引入概念 zero input response of a linear system(无论是否是time invariant 结论都成立)</p><p>if you put nothing into it you get nothing out of it</p><p>意思是如果$x(t) = 0$对于任意t成立， 则$y(t) = 0$对于任意t成立；如果$x[n] = 0$对于任意t成立， 则$y[n] = 0$对于任意t成立 原因是对于线性系统 如果$x(t) -&gt;y(t)$ 那么$ax(t) -&gt;ay(t)$ 令$a$为0 就可以得到这个情况</p><p><strong>causality这个性质说明的是系统无法预测输入（视频27分45秒）</strong></p><p>对于线性系统</p><p>if $x(t) = 0$ 对任意$t &lt; t_0$成立，那么$y(t) = 0$对任意$t &lt; t_0$成立， 称作initial rest</p><p>对于LTI系统</p><p>causality意味着$h(t) = 0$对任意$t&lt;0$成立，$h[n] = 0$对于任意$n &lt; 0$成立 因为相应的输入是$\delta(t)$和$\delta[n]$这两个在$t&lt;0$的时候 值均为0</p><p>视频33分8秒 示意了 如果$h(t) = 0$对任意$t &lt; 0$成立 则系统是causality的推导</p></li></ul><p>34分钟 举了一个例子</p><p>Accumulator: $y[n] = \sum_{k = -\infty}^nx[k]$， Accumulator是一个LTI系统中$h[n] = u[n]$的例子，下面分析</p><p>$\because h[n] \neq k\delta[n]$   我们可知系统是memory的</p><p>$\because h[n] = 0$对于任意n &lt; 0成立   我们可知系统是causal的</p><p>$\because \sum_{n = -\infty}^{+\infty}|h[n]| = \infty$ 我们可知系统是not stable的</p><p>38分钟关于Accumulator的求逆 没有看懂</p><p>40分钟的例子 视频里面写着 initial rest $\Rightarrow$ LTI 有点没懂</p><p>视频最后的部分讲的是</p><p>$\delta (t) * \delta (t) = \delta (t) $ 和</p><p> $ \delta[n] *\delta[n] = \delta[n]$</p><p>后者可以画图推理得到，但是前者比较tricky</p><p>因为考虑到$\delta(t) = \lim_{\Delta \to 0}\delta_{\Delta}(t)$ 如果对$\delta_{\Delta}(t) * \delta_{\Delta}(t)$得到的是一个$r_{\Delta}(t)$</p><script type="math/tex; mode=display">\begin{equation}r_{\Delta}(t) = \begin{cases}\frac{1}{\Delta^2}t& 0 < t < \Delta \\\frac{1}{\Delta} - (t - \Delta) * \frac{1}{\Delta^2}& \Delta < t < 2\Delta \\0& otherwise\end{cases}\end{equation}</script><p>那么$\delta(t) = \lim_{\Delta \to 0}r_{\Delta}(t)$</p><p>再考虑对于$\delta(t)$求导，会发现比较麻烦，老师说正确的解决方法是through a set of mathematics referred to as generalized functions</p><p><strong>当我们讨论一个函数时，我们讨论的是what the value of the function is at any instant of time，当面对冲激函数的时候就出现麻烦，那么就从另一种定义operational  definition来对待函数，即not related to what the impulse is, but to what the impulse does under the operation of convolution</strong></p><p><strong>SO WHAT IS IMPULSE?</strong></p><p>An impulse in something which under convolution retains the function !   666</p><p>意思就是impulse这个函数的定义是通过卷积来定义的，对于任意的函数，如果有一个函数和这个任意的函数卷积出来的结果不变，那么这个函数就是impulse函数</p><script type="math/tex; mode=display">x(t) * \delta(t) = x(t) \\\therefore \frac{d\delta(t)}{dt} = u_1(t) \\x(t) * u_1(t) = \frac{dx(t)}{dt} \\u_2(t) = u_1(t) * u_1(t) \\x(t) * u_2(t) = \frac{d^2x(t)}{dt}u_k(t) = u_1(t) * u_1(t) * ... \quad k \ times \\x(t) * u_k(t) = \frac{d^kx(t)}{dt} \\u_0(t) = \delta(t)</script><p>同理 对积分器引入$u_{-m}(t)$的定义 mth running integral</p><script type="math/tex; mode=display">u_k(t) * u_p(t) = u_{k + p}(t)</script>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号与系统 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何建站</title>
      <link href="2021/02/18/%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%99/"/>
      <url>2021/02/18/%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<h4 id="如何建站"><a href="#如何建站" class="headerlink" title="如何建站"></a>如何建站</h4><p>本站是在github上面建的静态网站，用的是hexo模板(这个资料很多)</p><p>主题是<a href="https://github.com/blinkfox/hexo-theme-matery">闪烁之狐</a></p><p>对于mathjax渲染问题，可能需要参考一下这篇博客<a href="https://www.cnblogs.com/Ai-heng/p/7282110.html">链接</a></p><p>对于发生spawn failed的错误 可以参考一下这篇博客<a href="https://1187100546.github.io/2019/11/24/spawn-failed/">链接</a></p><p>有问题欢迎留言</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
